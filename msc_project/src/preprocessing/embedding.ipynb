{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/msc_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = \"/lyceum/jhk1c21/msc_project/data/graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(data_home, 'filtered', 'filtered_nodes.pkl'))\n",
    "df = pd.read_csv(os.path.join(data_home, 'filtered', 'filtered_nodes.csv'))\n",
    "df = pd.read_csv(os.path.join(data_home, 'full', 'nodes_full.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = list(df['title'])\n",
    "keywords_list = list(df['keywords'])\n",
    "abstract_list = list(df['abstract'])\n",
    "fos_list = list(df['fos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fast_model = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(model, keywords_list):\n",
    "    embeddings = [ np.array(list(map(model.get_word_vector, keywords))) for keywords in keywords_list ]\n",
    "    return embeddings\n",
    "    \n",
    "def word_list_embedding(model, keywords_list):\n",
    "    embeddings = word_embedding(model, keywords_list)\n",
    "    # print(len(embeddings))\n",
    "    # return np.array([ np.mean(keywords_embedding, axis=0) for keywords_embedding in embeddings])\n",
    "    return [ np.mean(keywords_embedding, axis=0) for keywords_embedding in embeddings]\n",
    "    \n",
    "def sentence_embedding(model, sentence_list):\n",
    "    return np.array([ model.get_sentence_vector(' '.join(sentence.split())) for sentence in sentence_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_embedding_list = word_list_embedding(fast_model, keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148061, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_embedding_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embedding_list = sentence_embedding(fast_model, title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00433523 -0.02681895  0.03109596  0.03126718  0.01572027  0.03395665\n",
      "  0.07647985 -0.03281466  0.03433651  0.00113465]\n",
      "[ 0.01835213 -0.01114322  0.03089921 -0.00523332  0.032729    0.03502502\n",
      "  0.03554188 -0.02476176  0.01622009  0.02455734]\n"
     ]
    }
   ],
   "source": [
    "print(title_embedding_list[0][:10])\n",
    "print(title_embedding_list[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_from_abstract = []\n",
    "threshold = 0.3\n",
    "keywords_limit = 2\n",
    "bert_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is done\n",
      "1000 is done\n",
      "2000 is done\n",
      "3000 is done\n",
      "4000 is done\n",
      "5000 is done\n",
      "6000 is done\n",
      "7000 is done\n",
      "8000 is done\n",
      "9000 is done\n",
      "10000 is done\n",
      "11000 is done\n",
      "12000 is done\n",
      "13000 is done\n",
      "14000 is done\n",
      "15000 is done\n",
      "16000 is done\n",
      "17000 is done\n",
      "18000 is done\n",
      "19000 is done\n",
      "20000 is done\n",
      "21000 is done\n",
      "22000 is done\n",
      "23000 is done\n",
      "24000 is done\n",
      "25000 is done\n",
      "26000 is done\n",
      "27000 is done\n",
      "28000 is done\n",
      "29000 is done\n",
      "30000 is done\n",
      "31000 is done\n",
      "32000 is done\n",
      "33000 is done\n",
      "34000 is done\n",
      "35000 is done\n",
      "36000 is done\n",
      "37000 is done\n",
      "38000 is done\n",
      "39000 is done\n",
      "40000 is done\n",
      "41000 is done\n",
      "42000 is done\n",
      "43000 is done\n",
      "44000 is done\n",
      "45000 is done\n",
      "46000 is done\n",
      "47000 is done\n",
      "48000 is done\n",
      "49000 is done\n",
      "50000 is done\n",
      "51000 is done\n",
      "52000 is done\n",
      "53000 is done\n",
      "54000 is done\n",
      "55000 is done\n",
      "56000 is done\n",
      "57000 is done\n",
      "58000 is done\n",
      "59000 is done\n",
      "60000 is done\n",
      "61000 is done\n",
      "62000 is done\n",
      "63000 is done\n",
      "64000 is done\n",
      "65000 is done\n",
      "66000 is done\n",
      "67000 is done\n",
      "68000 is done\n",
      "69000 is done\n",
      "70000 is done\n",
      "71000 is done\n",
      "72000 is done\n",
      "73000 is done\n",
      "74000 is done\n",
      "75000 is done\n",
      "76000 is done\n",
      "77000 is done\n",
      "78000 is done\n",
      "79000 is done\n",
      "80000 is done\n",
      "81000 is done\n",
      "82000 is done\n",
      "83000 is done\n",
      "84000 is done\n",
      "85000 is done\n",
      "86000 is done\n",
      "87000 is done\n",
      "88000 is done\n",
      "89000 is done\n",
      "90000 is done\n",
      "91000 is done\n",
      "92000 is done\n",
      "93000 is done\n",
      "94000 is done\n",
      "95000 is done\n",
      "96000 is done\n",
      "97000 is done\n",
      "98000 is done\n",
      "99000 is done\n",
      "100000 is done\n",
      "101000 is done\n",
      "102000 is done\n",
      "103000 is done\n",
      "104000 is done\n",
      "105000 is done\n",
      "106000 is done\n",
      "107000 is done\n",
      "108000 is done\n",
      "109000 is done\n",
      "110000 is done\n",
      "111000 is done\n",
      "112000 is done\n",
      "113000 is done\n",
      "114000 is done\n",
      "115000 is done\n",
      "116000 is done\n",
      "117000 is done\n",
      "118000 is done\n",
      "119000 is done\n",
      "120000 is done\n",
      "121000 is done\n",
      "122000 is done\n",
      "123000 is done\n",
      "124000 is done\n",
      "125000 is done\n",
      "126000 is done\n",
      "127000 is done\n",
      "128000 is done\n",
      "129000 is done\n",
      "130000 is done\n",
      "131000 is done\n",
      "132000 is done\n",
      "133000 is done\n",
      "134000 is done\n",
      "135000 is done\n",
      "136000 is done\n",
      "137000 is done\n",
      "138000 is done\n",
      "139000 is done\n",
      "140000 is done\n",
      "141000 is done\n",
      "142000 is done\n",
      "143000 is done\n",
      "144000 is done\n",
      "145000 is done\n",
      "146000 is done\n",
      "147000 is done\n",
      "148000 is done\n"
     ]
    }
   ],
   "source": [
    "for idx, abstract in enumerate(abstract_list):\n",
    "    keywords = bert_model.extract_keywords(abstract, top_n=20, use_mmr=True)\n",
    "    \n",
    "    filtered_keywords = [ keyword[0] for keyword in keywords if keyword[1] > threshold ]\n",
    "    if len(filtered_keywords) == 0:\n",
    "        filtered_keywords = [ keyword[0] for keyword in keywords[:keywords_limit] ]\n",
    "    \n",
    "    keywords_from_abstract.append(filtered_keywords)\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"{idx} is done => shape: {filtered_keywords.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_embedding_list = word_list_embedding(fast_model, keywords_from_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607 469 += , . = = ( ) < • ∫ = • ∕ = ∕ = < × = = ( ) = + ∫ = • ∫ = . .. .. .. .. .. .. .. .. .. .. .. .. .. .... .. .. .. .. .. .. .. .. . .. . .. . .. .. .. . ... .. .. .. .. .. .. .. .... .... .. .. .. . . .. .. .. .. .. .. .... .... ... ... ... . ...... .................... .... .... .... .... .. .. .. . . .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .\n",
      "8212 3 N/A\n",
      "9424 2   \n",
      "20851 1 :\n",
      "24129 2   \n",
      "24705 20 'Where's' in a name?\n",
      "35417 3 N/A\n",
      "42362 2   \n",
      "55084 3 N/A\n",
      "67338 2   \n",
      "83948 2   \n",
      "91579 2   \n",
      "93543 2   \n",
      "103099 3 N/A\n",
      "104488 2   \n",
      "104530 2   \n",
      "109805 674 .@~.~• @.~@. ~ ~@#@@@#. @@@@@@@@. .•°.... .... @° .'. .... @. .° .... @. .. .° .@ .... .. .@ .... .. °@ .... °° °@ .... ~ .... ..@ ................ .. @ .... ..@ ................ .. @ .... • .@ ................ .. @ .... • .@ ................ .. @ .... ..~ ................ ~... @... ~... ~... • .@ ........ • ~ ; ..,. .... . .............. • ..... @ ........ @@.. .@ .@ .@ .@ .@ .@ .@ .@ .@ • .@. ° °@° • . • °@° • • .@• • • .@@.. .@ ......... @. .@@@@@@@@@@@.. .@ ......... @@.. .@. . .@@.. .@. . .@... .@. . .@.. .@ .... @.. .@. . .@.. .@. . .@.. .@. . .@... .@. . .@@.. ..... @ .......... @@.. ..@@@@@@@@@@@@@@@@.. ..• ~ .. ..,.--- ... ...... .* ........ ..~... ::~;::::\n",
      "111396 2   \n",
      "114952 2   \n",
      "120792 2   \n",
      "121561 5 None.\n",
      "145706 1 .\n"
     ]
    }
   ],
   "source": [
    "to_eleminate_list = []\n",
    "original_shape = abstract_embedding_list[0].shape\n",
    "for idx, embedding in enumerate(abstract_embedding_list):\n",
    "    if original_shape != embedding.shape:\n",
    "        to_eleminate_list.append(idx)\n",
    "        print(idx, len(abstract_list[idx]), abstract_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_embedding = []\n",
    "title_embedding = []\n",
    "abstract_embedding = []\n",
    "\n",
    "filtered_fos = []\n",
    "filtered_id = []\n",
    "start = 0\n",
    "\n",
    "for end in to_eleminate_list:\n",
    "    keywords_embedding.extend(keywords_embedding_list[start:end])\n",
    "    title_embedding.extend(title_embedding_list[start:end])\n",
    "    abstract_embedding.extend(abstract_embedding_list[start:end])\n",
    "    \n",
    "    filtered_fos.extend(list(df['fos'].iloc[:][start:end]))\n",
    "    filtered_id.extend(list(df['_id'].iloc[:][start:end]))\n",
    "    start = end + 1\n",
    "    \n",
    "keywords_embedding.extend(keywords_embedding_list[start:])\n",
    "title_embedding.extend(title_embedding_list[start:])\n",
    "abstract_embedding.extend(abstract_embedding_list[start:])\n",
    "\n",
    "filtered_fos.extend(list(df['fos'].iloc[:][start:]))\n",
    "filtered_id.extend(list(df['_id'].iloc[:][start:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_embedding = np.array(keywords_embedding)\n",
    "title_embedding = np.array(title_embedding)\n",
    "abstract_embedding = np.array(abstract_embedding)\n",
    "\n",
    "filtered_id = np.array(filtered_id)\n",
    "filtered_fos = np.array(filtered_fos, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/title.npy', title_embedding)\n",
    "np.save('../data/keywords.npy', keywords_embedding)\n",
    "np.save('../data/abstract.npy', abstract_embedding)\n",
    "np.save('../data/id.npy', filtered_id)\n",
    "np.save('../data/fos.npy', filtered_fos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Objective programming', 'Fire protection', 'Computer science', 'Fuzzy logic', 'Operations research', 'Fire risk', 'Genetic algorithm', 'Decision maker']),\n",
       "       list(['Integer', 'Weight function', 'Edge cover', 'Vertex (graph theory)', 'Graph theory', 'Approximation algorithm', 'Discrete mathematics', 'Dynamic programming', 'Combinatorics', 'Exponential function', 'Vertex (geometry)', 'Algorithm', 'Neighbourhood (graph theory)', 'Minimum weight', 'Vertex cover', 'Mathematics']),\n",
       "       list(['CDNA Arrays', 'Normalization (statistics)', 'Pattern recognition', 'Biology', 'Population Heterogeneity', 'High density', 'Comparative genomic hybridization', 'Artificial intelligence', 'Mega-', 'Genetics', 'DNA microarray', 'Gene expression microarray']),\n",
       "       ...,\n",
       "       list(['XML Encryption', 'XML framework', 'Computer science', 'XML validation', 'Document Structure Description', 'XML namespace', 'Theoretical computer science', 'RELAX NG', 'XML schema', 'XML Signature']),\n",
       "       list(['Probability mass function', 'VC dimension', 'Neuroscience', 'Empirical risk minimization', 'Probability measure', 'Algorithm', 'Uniform convergence', 'Psychology', 'Structural risk minimization', 'Set estimation', 'Estimator']),\n",
       "       list(['Geometric mechanics', 'Open air', 'Terrain', 'Dynamical systems theory', 'Future interest', 'Human–computer interaction', 'Artificial intelligence', 'Robot', 'Robotics', 'Physics', 'Computation'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to load the npy file, particularly fos.npy\n",
    "with open('../data/fos.npy', 'rb') as f:\n",
    "    a = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/computerandgyein/Library/Mobile Documents/com~apple~CloudDocs/jihwan/UK/Southampton/University/MSc Research /code/msc_project/preprocessing/embedding.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/computerandgyein/Library/Mobile%20Documents/com~apple~CloudDocs/jihwan/UK/Southampton/University/MSc%20Research%20/code/msc_project/preprocessing/embedding.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m similarity\n",
      "\u001b[0;31mNameError\u001b[0m: name 'similarity' is not defined"
     ]
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
