{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9296a3-3072-4259-bdf5-30138b290c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lyceum/jhk1c21/.conda/envs/msc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-01 15:50:22.947135: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-01 15:51:13.890701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25e0d6-6315-459b-9147-1be46f85623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = \"/lyceum/jhk1c21/msc_project/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86111ef-895b-42c2-95c5-47db1025e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(data_home + \"graph/nodes_full.pkl\")\n",
    "df = pd.read_csv(os.path.join(data_home, \"graph\", \"full\", \"nodes_full.csv\"), index_col='_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2649a69-4439-4eaa-95dc-efe34dc899d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_list = list(df['title'])\n",
    "# keywords_list = list(df['keywords'])\n",
    "abstract_list = df['abstract']\n",
    "# fos_list = list(df['fos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b55765a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'53e99784b7602d9701f3f411': 'The eXtensible Markup Language 驴 XML 驴 is not only a language for communication between humans and the web, it is also a language for communication between programs. Rather than passing parameters, programs can pass documents from one to another, containing not only pure data, but control information as well. Even legacy programs written in ancient languages such as COBOL and PL/I can be adapted by means ofinterface reengineering to process and to generate XML documents.',\n",
       " '53e99784b7602d9701f3f5fe': 'Resource allocation for multi-tier web applications in virtualization environments is one of the most important problems in autonomous computing. On one hand, the more resources that are provisioned to a multitier web application, the easier it is to meet service level objectives (SLO). On the other hand, the virtual machine which hosts the multi-tier web application needs to be consolidated as much as possible in order to maintain high resource utilization. This paper presents an adaptive resource controller which consists of a feedback utilization controller and an auto-regressive and moving average model (ARMA)-based model estimator. It can meet application-level quality of service (QoS) goals while achieving high resource utilization. To evaluate the proposed controllers, simulations are performed on a testbed simulating a virtual data center using Xen virtual machines. Experimental results indicate that the controllers can improve CPU utilization and make the best tradeoff between resource utilization and performance for multi-tier web applications.',\n",
       " '53e99784b7602d9701f3e15d': 'As process variations become a significant problem in deep sub-micron technology, a shift from deterministic static timing analysis to statistical static timing analysis for high-performance circuit designs could reduce the excessive conservatism that is built into current timing design methods. We address the timing yield problem for sequential circuits and propose a statistical approach to handle it. We consider the spatial and path reconvergence correlations between path delays, set-up time and hold time constraints, and clock skew due to process variations. We propose a method to get the timing yield based on the delay distributions of register-to-register paths in the circuit On average, the timing yield results obtained by our approach have average errors of less than 1.0% in comparison with Monte Carlo simulation. Experimental results show that shortest path variations and clock skew due to process variations have considerable impact on circuit timing, which could bias the timing yield results. In addition, the correlation between longest and shortest path delays is not significant.',\n",
       " '53e99784b7602d9701f3f95d': 'Mobile online analytical processing (mOLAP) encompasses all necessary technologies for information systems that enable OLAP data access to users carrying a mobile device. This paper presents FCLOS, a complete client–server architecture explicitly designed for mOLAP. FCLOS founds on intelligent scheduling and compressed transmissions in order to become a query efficient, self-adaptive and scalable mOLAP information system. Scheduling exploits derivability between data cubes in order to group related queries and eventually reduce the necessary transmissions (broadcasts). Compression is achieved by the m-Dwarf, a novel, compressed data cube physical structure, which has no loss of semantic information and is explicitly designed for mobile applications. The superiority of FCLOS against state of the art systems is shown both experimentally and analytically.',\n",
       " '53e99785b7602d9701f442cb': 'The emergence of India as a global player in software development, IT, and call centre operations is one side of an information revolution that has also begun to impact on governance and development at a domestic level in areas such as e-governance, e-commerce and e-health. The state, private and civil sectors have invested in numerous initiatives throughout the length and breadth of India aimed at extending the benefits of the information revolution to rural and remote areas. These range from Reliance Infocom’s roll out of low-cost mobile cellular phones, to numerous civil society based initiatives aimed at establishing affordable access to information and knowledge. The state continues to invest in ICTs for development – from its support for Village Public Telephones (VPTs) to its enabling the computerisation of land records such as the Bhoomi project in Karnataka. The state’s recognition of the role played by private and civil society sectors in development marks a major and distinct change in attitude from one characterised by ‘tolerance’ at best for these sectors and belief in the self-sufficiency of a ‘dirigiste’ economy, to pragmatic accomodations with these sectors. This change is to some extent a reflection of post-SAP policies adopted by the state, best illustrated by its steady withdrawal of support from its welfare agenda.',\n",
       " '53e9978ab7602d9701f4bc6d': 'The health care system in Sweden and many other countries is facing increasing costs. The major reason is the changing age distribution of the population with more elderly people in need of support. At the same time, health care systems are often very labor and staff intensive. In this paper, we focus on a staff planning problem arising in Sweden where people receive home care from the local authorities. The objective is to develop visiting schedules for care providers that incorporate some restrictions and soft objectives. Each visit has a particular task to be performed, for example: cleaning, washing, personal hygiene and/or nursing activities. Each staff member has skills and each client should, if possible, be visited by the same contact person. The operational situation is continuously changing and planning is done each day. We describe the development of a decision support system Laps Care to aid the planners. The system consists of a number of components including information data bases, maps, optimization routines, and report possibilities. We formulate the problem using a set partitioning model and, for a solution method, we make use of a repeated matching algorithm. The system is currently in operation at a number of home care organizations. We report on the practical impact of the system in the health care organization which was involved in the development. The savings are considerably in terms of saved planning time and in the quality of the routes, as well as the measured quality for the clients. Numerical experiments of the system are presented.',\n",
       " '53e9978db7602d9701f4ccea': 'Common inductive learning strategies offer tools for knowledge acquisition, but possess some inherent limitations due to the use of fixed bias during the learning process. To overcome the limitations of such base-learning approaches, a research trend explores the potentialities of meta-learning, which is oriented to the development of mechanisms based on a dynamical search of bias. This may lead to an improvement of the base-learner performance on specific learning tasks, by profiting of the accumulated past experience. In this paper, we present a meta-learning framework called Mindful (Meta INDuctive neuro-FUzzy Learning) which is founded on the integration of connectionist paradigms and fuzzy knowledge management. Due to its peculiar organisation, Mindful can be exploited on different levels of application, being able to accumulate learning experience in cross-task contexts. This specific knowledge is gathered during the meta-learning activity and it is exploited to suggest parametrisation for future base-learning tasks. The evaluation of the Mindful system is detailed through an ensemble of experimental sessions involving both synthetic domains and real-world data.',\n",
       " '53e9978db7602d9701f4d1aa': 'Mobile ad hoc networks (MANETs) are gaining importance as a promising technology for flexible, proximity-based, mobile communication. However, the inherent dynamics of MANETs imposes strong limitations on the design of distributed applications. They need to be able to adapt to changing conditions quickly and organize themselves in terms of component placement and communication habits. In this paper, we present MESHMdl, a middleware that provides a high level of awareness and decoupling for application components to make them more flexible and adaptable. We focus on the Event Space as the central communication medium of MESHMdl. The Event Space offers a simple, unified communication interface for inter-agent communication as well as for communication with the middleware and resource access. Furthermore, it serves as a means for flexibly extending a MESHMdl\\xa0daemon. We investigate the performance of the Event Space on different mobile devices and show that it is superior to comparable systems.',\n",
       " '53e9978db7602d9701f50739': 'The objective of this study is to present a novel tool for predictive modelling of urban growth. The proposed tool, named iCity – Irregular City, extends the traditional formalization of cellular automata (CA) to include an irregular spatial structure, asynchronous urban growth, and a high spatio-temporal resolution to aid in spatial decision making for urban planning. The iCity software tool was developed as an embedded model within a common desktop geographic information system (GIS) with a user-friendly interface to control modelling operations for urban land-use change. This approach allows the model developer to focus on implementing model logic rather than developing an entire stand-alone modelling application. It also provides the model user with a familiar environment in which to run the model to simulate urban growth.',\n",
       " '53e9978db7602d9701f517bf': 'Many decision support tools have been developed over the last 20 years and, in general, they support what Simon termed substantive rationality. However, such tools are rarely suited to helping people tackle wicked problems, for which a form of procedural rationality is better suited. Procedurally rational approaches have appeared in both management science and computer science, examples being the soft OR approach of cognitive mapping and the design rationale based on IBIS. These approaches are reviewed and the development of Wisdom, a procedurally rational decision support process and accompanying tool, is discussed and evaluated.',\n",
       " '53e99792b7602d9701f54b95': 'This paper introduces SwissAnalyst, a complete Data Mining environment powered by Weka. SwissAnalyst was developed as an intuitive process layer, which offers all necessary features to place it on par, in terms of functionality, with most major commercial Data Mining software packages. As GNU GPL software, SwissAnalyst offers a license-free platform to develop both proofs of concept for potential business users and complex process-driven solutions for researchers. ',\n",
       " '53e99792b7602d9701f58152': 'On behalf of the IRPS 2010 Management Committee and the IRPS Board of Directors, it is my pleasure to present the 48th edition of the International Reliability Physics technical proceedings. Within its pages are the manuscripts detailing the technical presentations and posters that are the heart of the 2010 symposium. This volume would not be possible without the efforts of many individuals, especially the authors whose work was selected for its outstanding technical quality.',\n",
       " '53e99792b7602d9701f5af27': 'A promising traffic flow forecasting model based on Multivariate Adaptive Regression Splines (MARS) is developed in this paper. First, the historical traffic flow data is obtained from the loop detectors installed on the road network of Beijing. Then, part of the data is selected for training the MARS model while the rest is used to test the method. The results based on MARS method are compared with those of other methods such as the Neural Networks. The proposed MARS method is proved to have a considerable accuracy. Moreover, the model constructed with MARS can be described with analytical functions, which helps a lot in the further research on traffic flow forecasting.',\n",
       " '53e99792b7602d9701f5af35': 'This paper describes an approach to the feature location problem for distributed systems, that is, to the problem of locating which code components are important in providing a particular feature for an end user. A feature is located by observing system execution and noting time intervals in which it is active. Traces of execution in intervals with and without the feature are compared. Earlier experience has shown that this analysis is difficult because distributed systems often exhibit stochastic behavior and because time intervals are hard to identify with precision. To get around these difficulties, the paper proposes a definition of time interval based on the causality analysis introduced by Lamport and others. A strict causal interval may be defined, but it must often be extended to capture latent events and to represent the inherent imprecision in time measurement. This extension is modeled using a weighting function which may be customized to the specific circumstances of each study. The end result of the analysis is a component relevance index, denoted p\"c, which can be used to measure the relevance of a software component to a particular feature. Software engineers may focus their analysis efforts on the top components as ranked according to p\"c. Two case studies are presented. The first study demonstrates the feasibility of p\"c by applying our method to a well-defined distributed system. The second study demonstrates the versatility of p\"c by applying our method to message logs obtained from a large military system. Both studies indicate that the suggested approach could be an effective guide for a software engineer who is maintaining or enhancing a distributed system.',\n",
       " '53e99792b7602d9701f5b06f': 'This paper describes an approach to representing cases as nested graph-structures, i.e., as hierarchically, spatially, temporally and causally interconnected nodes (case nodes), which may be themselves recursively described by other sets of interconnected nodes. Each case node represents a case piece (sub-case). An adjacency matrix may represent these nested graph-structured cases. Within our approach, new cases are constructed using an iterative context-guided retrieval of case nodes from multiple cases. In order to illustrate the expressiveness of this case representation approach, we discuss its application to the diagnosis and therapeutics of neurological diseases, to architectural design and to storytelling. Some issues that come out of this approach, like its contribution to the representation of cases of CBR and to integrate ordinary and creative reasoning, are discussed. ',\n",
       " '53e99792b7602d9701f5b074': 'Yalut is a novel user-centric hybrid content sharing overlay for social networking. Yalut enables the users to retain control over their own data and preserve their privacy, whilst still using the popular centralized services. In this demonstration, we show the feasibility of Yalut by integrating the service with the popular social networking apps on Android devices, Mac and Windows desktop platforms. We show that it is possible to provide the benefits of distributed content sharing on top of the existing centralized services with minimal changes to the content sharing process.',\n",
       " '53e99792b7602d9701f5b0ed': 'Grid is a new solution to computationally and data intensive computing problems. Since the distributed knowledge discovery process is both data and computational intensive, the Grid is a natural platform for deploying a high performance data mining service. In order to improve the performance of data mining applications, an effective method is task parallelization. Existing mechanisms of data mining parallelization are based on NOW or SMP, it is necessary to develop new parallel mechanism for grid feature. In this paper, we present a framework for high performance DDM applications in Computational Grid environments called Data Mining Grid, with the function for decomposing data mining application into subtasks and then combine those subtasks to form directed acyclic graph. This kind of parallel mechanism decomposes application according to the actual computation power of each node in dynamic Grid environment.',\n",
       " '53e99792b7602d9701f5b10e': \"Because digital images can be modified with relative ease, considerable effort has been spent developing image forensic algorithms capable of tracing an image's processing history. In contrast to this, relatively little consideration has been given to anti-forensic operations designed to mislead forensic techniques. In this paper, we propose an anti-forensic technique capable of removing artifacts indicative of wavelet-based image compression from an image. Our technique operates by adding anti-forensic dither to a previously compressed image's wavelet coefficients so that the anti-forensically modified wavelet coefficient distribution matches a model of the coefficient distribution before compression. Simulation results show that our algorithm is capable of fooling current forensic image compression detection algorithms 100% of the time.\",\n",
       " '53e99792b7602d9701f5b0c6': \"Although different kinds of probabilistic π-calculus have been introduced and found their place in quantitative verification and evaluation,their behavioural equivalences still lack a deep investigation.We propose a simple probabilistic extension of the π-calculus,π p,which is inspired by Herescu and Palamidessi's probabilistic asynchronous π-calculus.An early semantics of our π p is presented.We generalise several classic behavioural equivalences to probabilistic versions,obtaining the probabilistic(strong) barbed equivalence and probabilistic bisimulation for π p.Then we prove that the coincidence between the barbed equivalence and bisimilarity in the π-calculus is preserved in the probabilistic setting.\",\n",
       " '53e99792b7602d9701f5b140': 'The article describes a proposed framework for representation and presentation of evolving information. As an example, we use administrative information about an educational module. Evolution is expressed by a set of scenarios and is used for adaptive presentation of information represented as hypermedia. Representation of meta-information is based on the XML (eXtensible Markup Language). We experimented with the proposed approach and implemented a software prototype, which enables the time view and the user view of the educational module information. All presented data are stored in the database. Each element has associated constraints at the level of presentation, intended group of readers and presentation time',\n",
       " '53e99792b7602d9701f5b119': 'Location of fire stations is an important factor in its fire protection capability. This paper aims to determine the optimal location of fire station facilities. The proposed method is the combination of a fuzzy multi-objective programming and a genetic algorithm. The original fuzzy multiple objectives are appropriately converted to a single unified ‘min–max’ goal, which makes it easy to apply a genetic algorithm for the problem solving. Compared with the existing methods of fire station location our approach has three distinguish features: (1) considering fuzzy nature of a decision maker (DM) in the location optimization model; (2) fully considering the demands for the facilities from the areas with various fire risk categories; (3) being more understandable and practical to DM. The case study was based on the data collected from the Derbyshire fire and rescue service and used to illustrate the application of the method for the optimization of fire station locations.',\n",
       " '53e99792b7602d9701f5b191': \"Human-Sensibility Ergonomics (HSE) is to apprehend human sensitivity features by measuring human senses and developing index tables related to psychology and physiology. One of the main purposes of HSE may be developing human-centered goods, environment and relevant technologies for an improved life quality. In order to achieve the goal, a test bed or a simulator can be a useful tool in controlling and monitoring a physical environment at will. This paper deals with requirements, design concepts, and specifications of the computing environment for the HSE, which is a part of HSE Technology Development Program sponsored by Korean Ministry of Science and Technology. The integrated computing system is composed of real-time and non-real-time environments. The non-real-time development environment comprises several PC's with Windows NT and their graphical user interfaces coded in Microsoft's Visual C++. Each PC independently controls and monitors a thermal or a light or an audio or a video environment. Each of software and database, developed in the non-real-time environment, is directly ported to the real-time environment through a local-area network. Then the real-time computing system, based on the cPCI bus, controls the integrated HSE environment and collects necessary information. The cPCI computing system is composed of a Pentium CPU board and dedicated I/O boards, whose quantities are determined with expandability considered. The integrated computing environment of the HSE simulator guarantees real-time capability, stability and expandability of the hardware, and to maximize portability, compatibility, maintainability of its software.\",\n",
       " '53e99792b7602d9701f5b223': 'We present the systematic analysis of all dangerous N-2 contingencies observed in medium size model of Polish power grid with about 2600 power lines. Each of the dangerous contingencies is composed of two initially tripped lines and one or more lines that overloaded as the result. There are 443 distinct contingencies that do not lead to immediate islanding of the grid. In the scope of the work we analyze the statistics of individual line participation in those contingencies and show that some lines have anomalously high rate of participation in the contingencies. Next, we show that about third of all the contingencies can be associated with the sub grids that are connected to the rest of the grid via small set of power line chains. The contingencies arise when cutting some of those chains results in overload of the others. Simple reduction of power grid corresponding to aggregation of chain components significantly reduces the total number of distinct contingencies. The rest of the contingencies are closely related to a set of almost dangerous N-1 contingencies that result in heavy loading of particular lines. Tripping many different additional lines on top of these N-1 contingencies results in an overload of one or more lines. We conclude our work by characterization of the joint distributions of power flows through the initiating and overloaded lines and statistical analysis of topological distance between the initially tripped and overloaded lines.',\n",
       " '53e99792b7602d9701f5b19a': 'The functional autoregressive process has become a useful tool in the analysis of functional time series data. It is defined by the equation X\"n\"+\"1=@JX\"n+@e\"n\"+\"1, in which the observations X\"n and errors @e\"n are curves, and @J is an operator. To ensure meaningful inference and prediction based on this model, it is important to verify that the operator @J does not change with time. We propose a method for testing the constancy of @J against a change-point alternative which uses the functional principal component analysis. The test statistic is constructed to have a well-known asymptotic distribution, but the asymptotic justification of the procedure is very delicate. We develop a new truncation approach which together with Mensov\\'s inequality can be used in other problems of functional time series analysis. The estimation of the principal components introduces asymptotically non-negligible terms, which however cancel because of the special form of our test statistic (CUSUM type). The test is implemented using the R package fda, and its finite sample performance is examined by application to credit card transaction data.',\n",
       " '53e99792b7602d9701f5b1ba': 'Simulated evolution on a computer can provide a means for generating appropriate tactics in real-time combat scenarios. Individual unit or higher level organizations, such as tanks and platoons, can use evolutionary computation to adapt to the current and projected situations. In this article, we briefly review current knowledge in evolutionary algorithms and offer an example of applying these techniques to generate adaptive behavior in a platoon-level engagement of tanks in which the mission of one platoon is changed on-the-fly.',\n",
       " '53e99792b7602d9701f5b29b': \"Recently, social network privacy becomes a hot issue in the field of privacy. We are concerned about the path nodes in the social network. With the knowledge of the two endpoints of a path, the adversary can attack the privacy of the nodes on this path. In this paper, we define the adversary's background knowledge, propose the anonymity model, and propose PN- Anonymity algorithm to adjust paths. Experimental results show that our algorithms can achieve the path nodes anonymous, and information loss can be well controlled to ensure the availability of information.\",\n",
       " '53e99792b7602d9701f5b2a1': 'We investigate the fixed parameter complexity of one of the most popular problems in combinatorial optimization, WEIGHTED VERTEX COVER. Given a graph G = (V, E), a weight function ω: V → R+, and k ∈ R+, WEIGHETD VERTEX COVER (WVC for short) asks for a subset C of vertices in V of weight at most k such that every edge of G has at least one endpoint in C. WVC and its variants have all been shown to be NP-complete. We show that, when restricting the range of ω to positive integers, the so-called INTEGER-WVC can be solved as fast as unweighted VERTEX COVER. Our main result is that if the range of ω is restricted to positive reals ≥ 1, then so-called REAL-WVC can be solved in time O(1.3954k + k|V|). If we modify the problem in such a way that k is not the weight of the vertex cover we are looking for, but the number of vertices in a minimum weight vertex cover, then the same running time can be obtained. If the weights are arbitrary (referred to by GENERAL-WVC), however, the problem is not fixed parameter tractable unless P = NP.',\n",
       " '53e99792b7602d9701f5b2b3': 'As Massively Multiplayer Online Games enjoy a huge popularity and are played by tens of thousands of players simultaneously, an efficient software architecture is needed to cope with the dynamically changing loads at the server side. In this paper we discuss a novel way to support this kind of application by dividing the virtual world into several parts, called microcells. Every server is assigned a number of microcells and by dynamically redeploying these microcells when the load in a region of the world suddenly increases, the platform is able to adapt to changing load distributions. The software architecture for this system is described and we also provide some evaluation results that indicate the performance of our platform.',\n",
       " '53e99792b7602d9701f5b2bc': \"Soft-state is a well established approach to designing robust network protocols and applications. However it is unclear how to apply soft-state approach to protocols that must maintain a large amount of state information in a scalable way. For example the Border Gateway Protocol (BGP) is used to maintain the global routing tables at core Internet routers, and the table size is typically above 180,000 entries and continues to grow over time. In this paper, we propose a novel approach, Persistent Detection and Recovery (PDR), to enable large-state protocols and applications to maintain state consistency using a soft-state approach. PDR uses state compression and receiver participation mechanisms to avoid per-state refresh overhead. We evaluate PDR's effectiveness and scalability by applying its mechanisms to maintain the consistency of BGP routing tables between routers. Our results show that the proposed PDR mechanisms are effective and efficient in detecting and correcting route insertion, modification, and removal errors. Moreover, they eliminate the need for routers to exchange full routing tables after a session reset, thus enabling routers to recover quickly from transient session failures.\",\n",
       " '53e99792b7602d9701f5b2c7': 'In two-channel competitive genomic hybridization microarray experiments, the ratio of the two fluorescent signal intensities at each spot on the microarray is commonly used to infer the relative amounts of the test and reference sample DNA levels. This ratio may be influenced by systematic measurement effects from non-biological sources that can introduce biases in the estimated ratios. These biases should be removed before drawing conclusions about the relative levels of DNA. The performance of existing gene expression microarray normalization strategies has not been evaluated for removing systematic biases encountered in array-based comparative genomic hybridization (CGH), which aims to detect single copy gains and losses typically in samples with heterogeneous cell populations resulting in only slight shifts in signal ratios. The purpose of this work is to establish a framework for correcting the systematic sources of variation in high density CGH array images, while maintaining the true biological variations.After an investigation of the systematic variations in the data from two array CGH platforms, SMRT (Sub Mega base Resolution Tiling) BAC arrays and cDNA arrays of Pollack et al., we have developed a stepwise normalization framework integrating novel and existing normalization methods in order to reduce intensity, spatial, plate and background biases. We used stringent measures to quantify the performance of this stepwise normalization using data derived from 5 sets of experiments representing self-self hybridizations, replicated experiments, detection of single copy changes, array CGH experiments which mimic cell population heterogeneity, and array CGH experiments simulating different levels of gene amplifications and deletions. Our results demonstrate that the three-step normalization procedure provides significant improvement in the sensitivity of detection of single copy changes compared to conventional single step normalization approaches in both SMRT BAC array and cDNA array platforms.The proposed stepwise normalization framework preserves the minute copy number changes while removing the observed systematic biases.',\n",
       " '53e99792b7602d9701f5b2d3': 'This paper presents a novel IA (instrumentation amplifier) design for implantable biomedical devices and systems with a 140-dB CMRR (common-mode rejection ratio). The proposed IA is composed of 3 stages, including a preamplifier, a 2nd-order BPF (band-pass filer), and a DC-level shifter and output buffer stage. A low-noise gm-C amplifier is used in the preamplifier stage so as to reduce the coupled thermal noise which might overwhelm the weak neural signals. The BPF is designed based on an OTA (operational transconductance amplifier) with dual current switches aiming at the low power as well as low noise demands. A source follower is employed to carry out the DC-level shifter and the output buffer, which provides an output signal adequate to drive the following stage, which is usually an ADC (analog to digital converter). Detailed analysis of the proposed circuitry is derived to solidify the proposed architecture. The proposed design is implemented using TSMC 0.35 mum 2P4M CMOS process. The results of post-layout simulations verify the performance of our design. The CMRR is better than 140 dB, and, most important of all, the input noise (RMS) is merely 23.28 dB at all PVT (process, supply voltage, temperature) corners',\n",
       " '53e99792b7602d9701f5b35d': 'We consider wireless networks that can be modeled by multiple access channels in which all the terminals are equipped with multiple antennas. The propagation model used to account for the effects of transmit and receive antenna correlations is the unitary-invariant-unitary model, which is one of the most general models available in the literature. In this context, we introduce and analyze two resource allocation games. In both games, the mobile stations selfishly choose their power allocation policies in order to maximize their individual uplink transmission rates; in particular they can ignore some specified centralized policies. In the first game considered, the base station implements successive interference cancellation (SIC) and each mobile station chooses his best space-time power allocation scheme; here, a coordination mechanism is used to indicate to the users the order in which the receiver applies SIC. In the second framework, the base station is assumed to implement single-user decoding. For these two games a thorough analysis of the Nash equilibrium is provided: the existence and uniqueness issues are addressed; the corresponding power allocation policies are determined by exploiting random matrix theory; the sum-rate efficiency of the equilibrium is studied analytically in the low and high signal-to-noise ratio regimes and by simulations in more typical scenarios. Simulations show that, in particular, the sum-rate efficiency is high for the type of systems investigated and the performance loss due to the use of the proposed suboptimum coordination mechanism is very small.',\n",
       " '53e99792b7602d9701f5b394': 'We propose the saliency driven nonlinear diffusion filtering as a boost for object recognition. Taking saliency image as mask for magnitudes of gradients, nonlinear diffusion filtering treats foreground and background selectively. It preserves foreground information while filters out background information as much as possible. In salient area, semantically important structures are well preserved, while in non-salient area, cluttered structures are inhibited and smoothed into plain regions. Object recognition is conducted utilizing Bag-of-Words model, which can implicitly emphasize important foreground features for the reason of selective filtering. Experiments show that recognition accuracies using filtered images are generally higher than those using initial images, and are comparable with state-of-the-art. Consequently, we draw a safe conclusion that saliency driven nonlinear diffusion filtering undoubtedly help improve recognition performance, as long as saliency images are appropriate.',\n",
       " '53e99792b7602d9701f5b3d4': \"IRESite is an exhaustive, manually annotated nonredundant relational database focused on the IRES elements (Internal Ribosome Entry Site) and containing information not available in the primary public databases. IRES elements were originally found in eukaryotic viruses hijacking initiation of translation of their host. Later on, they were also discovered in 5'-untranslated regions of some eukaryotic mRNA molecules. Currently, IRESite presents up to 92 biologically relevant aspects of every experiment, e. g. the nature of an IRES element, its functionality/defectivity, origin, size, sequence, structure, its relative position with respect to surrounding protein coding regions, positive/negative controls used in the experiment, the reporter genes used to monitor IRES activity, the measured reporter protein yields/activities, and references to original publications as well as cross-references to other databases, and also comments from submitters and our curators. Furthermore, the site presents the known similarities to rRNA sequences as well as RNA-protein interactions. Special care is given to the annotation of promoter-like regions. The annotated data in IRESite are bound to mostly complete, full-length mRNA, and whenever possible, accompanied by original plasmid vector sequences. New data can be submitted through the publicly available web-based interface at http://www.iresite.org and are curated by a team of lab-experienced biologists.\",\n",
       " '53e99792b7602d9701f5b3e5': 'Augmented Reality is the technology that overlays virtual image and information generated by computer on real scene, the technology combines the virtual object in a real world. This paper puts forward a registration method of multi-marker in augmented system. A paleontology magic book is designed and realized. The book is special for virtual education and can be interactive in real time. At the end of the paper, the result is showed. In this study, the prosperous future of augmented reality applying in education can be seen.',\n",
       " '53e99792b7602d9701f5b414': 'An analytic inverse method is presented for the theoretical design of 3-D transverse gradient coils. Existing gradient coil design methods require the basic geometry of the coil to be predetermined before optimization. Typically, coil windings are constrained to lie on cylindrical, planar, spherical, or conical surfaces. In this paper, a fully 3-D region in the solution space is explored and the p...',\n",
       " '53e99792b7602d9701f5b41b': \"We present the first constant-round non-malleable commitment scheme and the first constant-round non-malleable zero-knowledge argument system, as defined by Dolev, Dwork and Naor. Previous constructions either used a non-constant number of rounds, or were onlysecure under stronger setup assumptions. An example of such an assumption is the shared random string model where we assume all parties have access to a reference string that was chosen uniformly at random by a trusted dealer.We obtain these results by defining an adequate notion of non-malleable coin-tossing, and presenting a constant-round protocol that satisfies it. This protocol allows us to transform protocols that are non-malleable in (a modified notion of) the shared random string model into protocols that are non-malleable in the plain model (without any trusted dealer or setup assumptions). Observing that known constructions of a non-interactive non-malleable zero-knowledge argument systems in the shared random string model (De Santis et. al., 2001) are in fact non-malleable in the modified model, and combining them with our coin-tossing protocol we obtain the results mentioned above.The techniques we use are different from those used in previous constructions of non-malleable protocols. In particular our protocol uses diagonalization and a non-black-box proof of security (in a sense similar to Barak's zero-knowledge argument).\",\n",
       " '53e99792b7602d9701f5b420': \"Social Bookmarking Systems SBS have been widely adopted in the last years, and thus they have had a significant impact on the way that online content is accessed, read and rated. Until recently, the decision on what content to display in a publisher's web pages was made by one or at most few authorities. In contrast, modern SBS-based applications permit their users to submit their preferred content, to comment on and to rate the content of other users and establish social relations with each other. In that way, the vision of the social media is realized, i.e. the online users collectively decide upon the interestingness of the available bookmarked content. This article attempts to provide insights into the dynamics emerging from the process of content rating by the user community. To this end, the article proposes a framework for the study of the statistical properties of an SBS, the evolution of bookmarked content popularity and user activity in time, as well as the impact of online social networks on the content consumption behavior of individuals. The proposed analysis framework is applied to a large dataset collected from digg, a popular social media application.\",\n",
       " '53e99792b7602d9701f5b42b': 'Hierarchy is one of the most conspicuous features of numerous natural, technological and social systems. The underlying structures are typically complex and their most relevant organizational principle is the ordering of the ties among the units they are made of according to a network displaying hierarchical features. In spite of the abundant presence of hierarchy no quantitative theoretical interpretation of the origins of a multi-level, knowledge-based social network exists. Here we introduce an approach which is capable of reproducing the emergence of a multi-levelled network structure based on the plausible assumption that the individuals (representing the nodes of the network) can make the right estimate about the state of their changing environment to a varying degree. Our model accounts for a fundamental feature of knowledge-based organizations: the less capable individuals tend to follow those who are better at solving the problems they all face. We find that relatively simple rules lead to hierarchical self-organization and the specific structures we obtain possess the two, perhaps most important features of complex systems: a simultaneous presence of adaptability and stability. In addition, the performance (success score) of the emerging networks is significantly higher than the average expected score of the individuals without letting them copy the decisions of the others. The results of our calculations are in agreement with a related experiment and can be useful from the point of designing the optimal conditions for constructing a given complex social structure as well as understanding the hierarchical organization of such biological structures of major importance as the regulatory pathways or the dynamics of neural networks.',\n",
       " '53e99792b7602d9701f5b448': 'This paper studies the problem of determining the position of beacon nodes in Local Positioning Systems (LPSs), for which there are no inter-beacon distance measurements available and neither the mobile node nor any of the stationary nodes have positioning or odometry information. The common solution is implemented using a mobile node capable of measuring its distance to the stationary beacon nodes within a sensing radius. Many authors have implemented heuristic methods based on optimization algorithms to solve the problem. However, such methods require a good initial estimation of the node positions in order to find the correct solution. In this paper we present a new method to calculate the inter-beacon distances, and hence the beacons positions, based in the linearization of the trilateration equations into a closed-form solution which does not require any approximate initial estimation. The simulations and field evaluations show a good estimation of the beacon node positions.',\n",
       " '53e99792b7602d9701f5b452': 'A weighing matrix of weight k is a square matrix M with entries 0, 卤 1 such that MM T = kI n . We study the case that M is a circulant and k = 22t for some positive integer t. New structural results are obtained. Based on these results, we make a complete computer search for all circulant weighing matrices of order 16.',\n",
       " '53e99796b7602d9701f5b9d1': ' We investigate the fusion of audio and video a posterioriphonetic probabilities in a hybrid ANN/HMM audio-visualspeech recognition system. Three basic conditions to thefusion process are stated and implemented in a linear anda geometric weighting scheme. These conditions are theassumption of conditional independence of the audio andvideo data and the contribution of only one of the two pathswhen the SNR is very high or very low, respectively. In thecase of the geometric weighting a new... ',\n",
       " '53e99796b7602d9701f5be8e': \"PLoS ONE, a peer-reviewed Open Access academic journal published by the Pub- lic Library of Science, was founded in 2006 with the intent of reevaluating many of the aspects of the scholarly journal. As a result, PLoS ONE has taken elements of the traditional publishing model for scholarly journals and separated them into those functions that are most effectively carried out before publication (for exam- ple, peer review in order to evaluate whether the article deserves to join the scien- tific literature) and those that can most effectively be carried out after publication (for example, how impactful the article was once it joined the literature). With this basic premise in place, and using the online tools that are now available, the jour- nal has grown to the extent that in 2009 it will become one of the largest journals in the world (by publication volume). This article overviews the development of the journal to date - how it differs from most other journals and how it engages with its core audiences. In March 2009, the journal (along with other PLoS titles) began a program to place 'article-level metrics' on each publication, and this article out- lines how this has been achieved, as well as plans for further development. In con- clusion, this article looks forward to the future developments of this transforma- tional journal.\",\n",
       " '53e99796b7602d9701f5be93': 'We address the problem of finding a set of contour curves in an image. We consider the problem of perceptual grouping and contour completion, where the data is a set of points in the image. A new method to find complete curves from a set of contours or edge points is presented. Our approach is based on a previous work on finding contours as minimal paths between two end points using the fast marching algorithm (L. D Cohen and R. Kimmel, International Journal of Computer Vision, Vol. 24, No. 1, pp. 57–78, 1997). Given a set of key points, we find the pairs of points that have to be linked and the paths that join them. We use the saddle points of the minimal action map. The paths are obtained by backpropagation from the saddle points to both points of each pair.In a second part, we propose a scheme that does not need key points for initialization. A set of key points is automatically selected from a larger set of admissible points. At the same time, saddle points between pairs of key points are extracted. Next, paths are drawn on the image and give the minimal paths between selected pairs of points. The set of minimal paths completes the initial set of contours and allows to close them. We illustrate the capability of our approach to close contours with examples on various images of sets of edge points of shapes with missing contours.',\n",
       " '53e99796b7602d9701f5bc01': 'The objective of the paper is to construct a backbone node with self organization, topology control and reconfiguration capabilities. The key issues in wireless networks are maintain a topology with minimum degree, self organization during link or node failure and reconstruction ability when the backbone changes the position. Existing research works concentrate on any one of the issues by a backbone, but nodes in wireless are battery operated. To solve the all issues separately more power is required. To overcome the existing issues we propose a localized approach namely STAB-WIN, which will solve all the issues without affecting the entire system performance using local updates. This research work focuses on multiservice ability of a node to meet the design goals of next generation networks. Our approach is witnessed by the simulation results on analyzing the parameters like scalability which includes backbone size, routing overhead, control transfer and QoS parameters.',\n",
       " '53e99796b7602d9701f5ba7b': 'Network theory has been used for modeling biological data as well as social networks, transportation logistics, business transcripts, and many other types of data sets. Identifying important features/parts of these networks for a multitude of applications is becoming increasingly significant as the need for big data analysis techniques grows. When analyzing a network of protein-protein interactions (PPIs), identifying nodes of significant importance can direct the user toward biologically relevant network features. In this work, we propose that a node of structural importance in a network model can correspond to a biologically vital or significant property. This relationship between topological and biological importance can be seen in/between structurally defined nodes, such as hub nodes and driver nodes, within a network and within clusters. This work proposes data mining approaches for identification and examination of relationships between hub and driver nodes within human, yeast, rat, and mouse PPI networks. Relationships with other types of significant nodes, with direct neighbors, and with the rest of the network were analyzed to determine if the model can be characterized biologically by its structural makeup. We performed numerous tests on structure with a data-driven mentality, looking for properties that were potentially significant on a network level and then comparing those properties to biological significance. Our results showed that identifying and cross-referencing different types of topologically significant nodes can exemplify properties such as transcription factor enrichment, lethality, clustering, and Gene Ontology (GO) enrichment. Mining the biological networks, we discovered a key relationship between network properties and how sparse/dense a network is-a property we described as \"sparseness\". Overall, structurally important nodes were found to have significant biological relevance.',\n",
       " '53e99796b7602d9701f5bc29': 'In recent years social media have become indispensable tools for information dissemination, operating in tandem with traditional media outlets such as newspapers, and it has become critical to understand the interaction between the new and old sources of news. Although social media as well as traditional media have attracted attention from several research communities, most of the prior work has been limited to a single medium. In addition temporal analysis of these sources can provide an understanding of how information spreads and evolves. Modeling temporal dynamics while considering multiple sources is a challenging research problem. In this paper we address the problem of modeling text streams from two news sources - Twitter and Yahoo! News. Our analysis addresses both their individual properties (including temporal dynamics) and their inter-relationships. This work extends standard topic models by allowing each text stream to have both local topics and shared topics. For temporal modeling we associate each topic with a time-dependent function that characterizes its popularity over time. By integrating the two models, we effectively model the temporal dynamics of multiple correlated text streams in a unified framework. We evaluate our model on a large-scale dataset, consisting of text streams from both Twitter and news feeds from Yahoo! News. Besides overcoming the limitations of existing models, we show that our work achieves better perplexity on unseen data and identifies more coherent topics. We also provide analysis of finding real-world events from the topics obtained by our model.',\n",
       " '53e99796b7602d9701f5bf7d': 'The lack of accurate yet open to public simulation infrastructure has puzzled researchers in the memcomputing area for sometime. In this paper, we propose for the first time a full tool chain called MSim that supports the cycle-accurate microarchitecture level simulation for memcomputing studies. With MSim, the performance gains of utilizing memcomputing for arbitrary applications on user configurable computer system architectures can be evaluated in high accuracy. In addition, MSim provides flexible interfaces with pervasive object-oriented design, which makes it well-suited as a good base platform for researchers to explore new memcomputing technologies.',\n",
       " '53e99796b7602d9701f5bfcb': \"In this paper, we propose a system for helping users to judge the credibility of Web search results and to search for credible Web pages. Conventional Web search engines present only titles, snippets, and URLs for users, which give few clues to judge the credibility of Web search results. Moreover, ranking algorithms of the conventional Web search engines are often based on relevance and popularity of Web pages. Towards credibility-oriented Web search, our proposed system provides users with the following three functions: (1) calculation and visualization of several scores of Web search results on the main credibility aspects, (2) prediction of user's credibility judgment model through user's credibility feedback for Web search results, and (3) re-ranking of Web search results based on user's predicted credibility model. Experimental results suggest that our system enables users - in particular, users with knowledge about search topics - to find credible Web pages from a list of Web search results more efficiently than conventional Web search interfaces.\",\n",
       " '53e99796b7602d9701f5c0d7': \"One drawback of Hierarchical Task Network (HTN) planning is the difficulty of providing complete domain knowledge, i.e., a complete and correct set of HTN methods for every task. To provide a principled way to overcome this difficulty, we define a simple formalism that extends classical planning to include problem decomposition using methods, and a planning algorithm based on this formalism. In our formalism, the methods specify ways to achieve goals (rather than tasks as in conventional HTN planning), and goals may be achieved even when no methods are available. Our planning algorithm, GoDeL (Goal Decomposition with Landmarks), is sound and complete irrespective of whether the domain knowledge (i.e., the set of methods given to the planner) is complete. By comparing GoDeL's performance with varying amounts of domain knowledge across three benchmark planning domains, we show experimentally that (1) GoDeL works correctly with partial planning knowledge, (2) GoDeL's performance improves as more planning knowledge is given, and (3) when given full domain knowledge, GoDeL matches the performance of a state-of-the-art hierarchical planner.\",\n",
       " '53e99796b7602d9701f5c106': \"We present PKIP, an adaptable learning assistant tool for managing question items in item banks. PKIP is not only able to automatically assist educational users to categorize the question items into predefined categories by their contents but also to correctly retrieve the items by specifying the category and/or the difficulty level. PKIP adapts the ''categorization learning model'' to improve the system's categorization performance using the incoming question items. PKIP tool has an advantage over the traditional document categorization methods in that it can correctly categorize the question item which lacks keywords since it adopts the feature selection technique and support vector machine approach to item bank text categorization. In our initial experimentation, PKIP was designed and implemented to manage the Thai high primary mathematics question items. PKIP was tested and evaluated in terms of both system accuracy and user satisfaction. The evaluation result shows that the system accuracy is acceptable and PKIP satisfies the need of the users.\",\n",
       " '53e99796b7602d9701f5c118': 'In this paper, the steganographic method proposed by Chang and Tseng [1] is reviewed and the problem of Chang-Tseng scheme during extraction process is described. Two rectified schemes are proposed to overcome the problem so that the embedded secret data can be recovered correctly from the stego-image.',\n",
       " '53e99796b7602d9701f5c140': 'When we are watching videos, there exist spatiotemporal gaps between where we look and what we focus on, which result from temporally delayed responses and anticipation in eye movements. We focus on the underlying structures of those gaps and propose a novel method to predict points of gaze from video data. In the proposed methods, we model the spatiotemporal patterns of salient regions that tend to be focused on and statistically learn which types of the patterns strongly appear around the points of gaze with respect to each type of eye movements. It allows us to exploit the structures of gaps affected by eye movements and salient motions for the gaze-point prediction. The effectiveness of the proposed method is confirmed with several public datasets.',\n",
       " '53e99796b7602d9701f5c157': 'This paper describes a new out-of-core multi-resolution data structure for real-time visualization, interactive editing and externally efficient processing of large point clouds. We describe an editing system that makes use of the novel data structure to provide interactive editing and preprocessing tools for large scanner data sets. Using the new data structure, we provide a complete tool chain for 3D scanner data processing, from data preprocessing and filtering to manual touch-up and real-time visualization. In particular, we describe an out-of-core outlier removal and bilateral geometry filtering algorithm, a toolset for interactive selection, painting, transformation, and filtering of huge out-of-core point-cloud data sets and a real-time rendering algorithm, which all use the same data structure as storage backend. The interactive tools work in real-time for small model modifications. For large scale editing operations, we employ a two-resolution approach where editing is planned in real-time and executed in an externally efficient offline computation afterwards. We evaluate our implementation on example data sets of sizes up to 63GB, demonstrating that the proposed technique can be used effectively in real-world applications.',\n",
       " '53e99796b7602d9701f5c159': 'Currently, the Functional Magnetic Resonance Imaging (fMRI), Positron Emission Tomography (PET), and Electroencephalography (EEG) recordings are the major techniques of neuroimaging. The EEG with its highest temporal resolution is still a crucial measurement for localization of activities arising from the electrical behaviour of the brain. A scalp topographic map for an EEG may be a superposition of several simpler subtopographic maps, each resulting from an individual electrical source located at a certain depth. Furthermore, this source may have a temporal characteristic as an oscillation or a rhythm that extends in a certain time window which has been a basis of assumption for the time-frequency analysis methods. A method for the spatio-temporal wavelet decomposition of multichannel EEG data is proposed which facilitates the localization of electrical sources separate and/or overlapping on a continuum of time, frequency and space domains. The subtopographic maps asociated with each of these individual components are then used in the MUSIC source localization algorithm. The validations are performed on simulated EEG data. Spatio-temporal wavelet decomposition as a preprocessing method improves the source localization by simplifying the topographic data formed by the superposition of EEG generators, having possible combinations of temporal, frequency and/or spatial overlappings. Spatio-temporal analysis of EEG will help enhance the accuracy of dipole source reconstruction in neuroimaging.',\n",
       " '53e99796b7602d9701f5c15d': 'This paper introduces ArcE, a GIS tool for modelling actual evapotranspiration (EA) from an undefined number of meteorological stations. From daily data of precipitation and temperature, ArcE uses ArcObjects as the programming language to incorporate equations and hydrological boundary conditions, in order to calculate EA at monthly and yearly time steps. Because weather data are often missing, ArcE is programmed to use non-global models such as Hargreaves for potential evapotranspiration (EP) and Budyko for EA. In arid regions, where results from global and non-global models are expected to deviate, ArcE allows for the segregation of low-divergent areas suitable for interpolating EA from those that should be excluded for mapping the variable. In the semiarid Almanzora River basin, a heterogeneous region with contrasting climate in SE Spain, divergence in lowlands with a higher aridity index was about 15% with respect to an accurate estimate of EA from the Penman–Monteith equation. Evaluating EA is a first step for mapping the non-evaporative fraction of precipitation as the difference in P and EA.',\n",
       " '53e99796b7602d9701f5c177': 'In wireless sensor network, sensory readings are often noisy due to the imprecision of measuring hardware and the disturbance of deployment environment, so it is often inaccurate if we use individual sensor readings to answer queries. In this paper, we consider a useful application of sensor network: maximum average value region query. This query returns the region with the maximum average value among all possible regions in the network, where the region is a fix-sized circle pre-defined by users. Using the average value of a region to answer the query, noises between sensors will be neutralized with each other, which will make the results more reliable. However, because of the huge amount of possible regions in the network, it is costly to process the query exactly. Therefore, we propose a sampling-based algorithm AMAVR to deal with the problem approximately. AMAVR uses a background value to prune the useless regions which cannot be the result. A further optimization strategy is also given to handle the situation that, background value based filter does not work when some individual sensor nodes have higher values than their neighbors. By using both of the two techniques, the scale of the sampling population can be effectively reduced, that is, we cost less energy to get a satisfying result. At last, the conducted simulations demonstrate the energy efficiency of the proposed methods in our paper.',\n",
       " '53e99796b7602d9701f5c202': 'In this work we address the rate adaptation problem of a cognitive radio (CR) link in time-variant fading channels. Every time the primary users (PU) liberate the channel, the secondary user (SU) selects a transmission rate (from a finite number of available rates) and begins the transmission of fixed sized packets until a licensed user reclaims the channel back. After each transmission episode the number of successfully transmitted packets is used by the SU to update its optimal rate selection ahead of the next episode. The problem is formulated as an n-armed bandit problem and it is solved by means of a Monte Carlo control algorithm.',\n",
       " '53e99796b7602d9701f5c193': \"Power distribution networks that include distributed generation need new control strategies based on distributed and hierarchical structures. For these systems, a replicator dynamics strategy for dynamic resource allocation in the dispatch of distributed generators in a microgrid is presented. This approach uses the characteristics of a defined subsystem in order to offer a simple algorithm with optimal and feasible solutions even in cases where other methods do not satisfy the problem's constraints. To compare the performance of the replicator dynamics strategy and to analyze the optimality of the obtained solutions, a market multiagent-based scheme is adapted. The results are implemented in a simulation model for different scenarios to show the applicability of the proposed strategy.\",\n",
       " '53e99796b7602d9701f5c20d': 'We survey recent results on the computational complexity of mixed shop scheduling problems. In a mixed shop, some jobs have fixed machine orders (as in the job shop), while the operations of the other jobs may be processed in arbitrary order (as in the open shop). The main attention is devoted to establishing the boundary between polynomially solvable and NP-hard problems. When the number of operations per job is unlimited, we focus on problems with a lived number of jobs. (C) 2000 Elsevier Science B.V. All rights reserved.',\n",
       " '53e99796b7602d9701f5c214': 'We introduce the framework for storing and comparing compound objects. The implemented system is based on the RDBMS model, which - unlike other approaches in this area - enables to access the most detailed data about considered objects. It also contains ROLAP cubes designed for specific object classes and appropriately abstracted modules that compute object similarities, referred as comparators. In this paper, we focus on the case study related to images. We show specific examples of fuzzy logic comparators, together with their corresponding SQL statements executed at the level of pixels. We examine several open source database engines by means of their capabilities of storing and querying large amounts of such represented image data. We conclude that the performance of some of them is comparable to standard techniques of image storage and processing, with far better flexibility in defining new similarity criteria and analyzing larger image collections.',\n",
       " '53e99796b7602d9701f5c218': 'In this paper, we propose a secure semi-fragile watermarking technique based on integer wavelet transform with a choice of two watermarks to be embedded. A self-recovering algorithm is employed, that hides the image digest into some wavelet subbands for detecting possible illicit object manipulation undergone in the image. The semi-fragility makes the scheme tolerant against JPEG lossy compression with the quality factor as low as 70%, and locates the tampered area accurately. In addition, the system ensures more security because the embedded watermarks are protected with private keys. The computational complexity is reduced by using parameterized integer wavelet transform. Experimental results show that the proposed scheme guarantees safety of a watermark, recovery of image and localization of tampered area.',\n",
       " '53e99796b7602d9701f5c1a2': 'Existing medical vocabularies lack rich terms to describe findings that are generated by modem molecular diagnostic procedures. Most bioinformatics resources were designed primarily to support the needs of the research community. We describe the development of a curated resource, the Clinical Bioinformatics Ontology (CBO), a semantic network appropriate for describing clinically significant genomics concepts. The CBO includes concepts appropriate for both molecular diagnostics and cytogenetics. A standardized methodology based on consistent application of RefSeq information is applied to the curation of the CBO in order to provide a reproducible and reliable tool. Challenges related to this curation process are discussed in this paper. At the time of submission the CBO included 4,069 concepts, associated by 8,463 relationships.',\n",
       " '53e99796b7602d9701f5c226': \"Since Wolfram proposed to use cellular automata as pseudorandom sequence generators, many cryptographic applications using cellular automata have been introduced. One of the recent one is Mukherjee, Ganguly, and Chaudhuri's message authentication scheme using a special class of cellular automata called Single Attractor Cellular Automata (SACA). In this paper, we show that their scheme is vulnerable to a chosen-message attack, i.e., the secret key can be recovered by an attacker using only several chosen message-MAC pairs. The weakness of the scheme results from the regularity of SACA.\",\n",
       " '53e99796b7602d9701f5c1bf': 'We introduce PSCEL, a new language for developing autonomic software components capable of adapting their behaviour to react to external stimuli and environment changes. The application logic generating the computational behaviour of systems components is defined in a procedural style, by the programming constructs, while the adaptation logic is defined in a declarative style, by the policing constructs. The interplay between these two kinds of constructs permits to dynamically produce and enforce adaptation actions. To show PSCEL practical applicability and effectiveness, we employ it in a Cloud Computing case study.',\n",
       " '53e99796b7602d9701f5c256': 'Along with the rapid growth of heterogeneous cloud services and network technologies, more mobile devices use cloud storage services to enlarge the capacity and share data in our daily lives. We commonly use cloud storage service clients in a straight forward fashion, since we may easily obtain most client-side software from many services providers. However, when more devices and users participate in heterogeneous services, the difficulty increases to manage these services efficiently and conveniently. In this paper we design and implement a novel cloud-oriented file service, Wukong, which provides a user-friendly and highly-available facilitative data access method for mobile devices in cloud settings. By using the innovative storage abstraction layer and a set of optimization strategies, Wukong supports heterogeneous services with a relatively high performance. By evaluating a prototype in a systematic way on the aspects of the supporting interface, system performance, and the system resource cost, we find that this easily operable file service has a high usability and extensibility. It costs about 50 to 150 lines of code to implement a new backend service supporting plugin. Wukong achieves an acceptable throughput of 179.11 KB/s in an ADSL environment and 80.68 KB/s under a country EVDO 3G network with negligible overhead.',\n",
       " '53e99796b7602d9701f5c24a': 'Nowadays, the accuracy of speech processing systems is strongly affected by acoustic noise. This is a serious obstacle regarding the demands of modern applications. Therefore, these systems often need a noise reduction algorithm working in combination with a precise voice activity detector (VAD). The computation needed to achieve denoising and speech detection must not exceed the limitations imposed by real time speech processing systems. This paper presents a novel VAD for improving speech detection robustness in noisy environments and the performance of speech recognition systems in real time applications. The algorithm is based on a Multivariate Complex Gaussian (MCG) observation model and defines an optimal likelihood ratio test (LRT) involving multiple and correlated observations (MCO) based on a jointly Gaussian probability distribution (jGpdf) and a symmetric covariance matrix. The complete derivation of the jGpdf-LRT for the general case of a symmetric covariance matrix is shown in terms of the Cholesky decomposition which allows to efficiently compute the VAD decision rule. An extensive analysis of the proposed methodology for a low dimensional observation model demonstrates: (i) the improved robustness of the proposed approach by means of a clear reduction of the classification error as the number of observations is increased, and (ii) the trade-off between the number of observations and the detection performance. The proposed strategy is also compared to different VAD methods including the G.729, AMR and AFE standards, as well as other recently reported algorithms showing a sustained advantage in speech/non-speech detection accuracy and speech recognition performance using the AURORA databases.',\n",
       " '53e99796b7602d9701f5c287': 'Watermarking is a process that embeds one signal into another signal to provide a hidden identity. It is widely used in multimedia signal processing for copyright protection purpose. Motivated by the observation that in some applications it is highly desirable for the physical layer itself to have some user identifying features instead of obtaining such information from the media access control (MAC) layer, we extend the watermarking idea and propose a method to give a convolutionally or turbo coded system a hidden identity. The proposed algorithm embeds a periodic watermark sequence into the convolutionally coded data sequence in the transmitter and detects the watermark using the channel error information supplied by the forward error control (FEC) module in the receiver. It does not consume any extra transmission bandwidth by properly designing the watermark sequence using the properties of convolutional codes. The algorithm has several potential applications in wireless and satellite communications. It is easy to implement and fits generic modems',\n",
       " '53e99796b7602d9701f5c48d': 'As a generalization of the precise and pessimistic diagnosis strategies of system-level diagnosis of multicomputers, the t/k diagnosis strategy can significantly improve the self-diagnosing capability of a system at the expense of no more than k fault-free processors (nodes) being mistakenly diagnosed as faulty. In the case k⩾2, to our knowledge, there is no known t/k diagnosis algorithm for general diagnosable system or for any specific system. Hypercube is a popular topology for interconnecting processors of multicomputers. It is known that an n-dimensional cube is (4n−9)/3-diagnosable. This paper addresses the (4n−9)/3 diagnosis of n-dimensional cube. By exploring the relationship between a largest connected component of the 0-test subgraph of a faulty hypercube and the distribution of the faulty nodes over the network, the fault diagnosis of an n-dimensional cube can be reduced to those of two constituent (n−1)-dimensional cubes. On this basis, a diagnosis algorithm is presented. Given that there are no more than 4n−9 faulty nodes, this algorithm can isolate all faulty nodes to within a set in which at most three nodes are fault-free. The proposed algorithm can operate in O(Nlog2N) time, where N=2n is the total number of nodes of the hypercube. The work of this paper provides insight into developing efficient t/k diagnosis algorithms for larger k value and for other types of interconnection networks.',\n",
       " '53e99796b7602d9701f5c55e': \"For many English learners, vocabulary learning is viewed as a burden. Digital game-based learning carries numerous potential to draw learners' attention and help them learn information effectively. Many game-based learning systems claim to foster the learning process. However, not all the games are suitable for vocabulary learning. Our aim is to provide the selection criteria for teachers when they apply game-based vocabulary learning for teaching. In this study, we firstly collected the criteria on evaluating game-based vocabulary learning from the previous research. Sixty-six criteria are chosen and divided into four dimensions (teaching, game, society, and technology) and fourteen categories. Next, some criteria are selected from the sixty-six ones through English teachers. Finally the selected criteria are ranked and given the weight calculated by ROC (Rank Order Centroid) according to students' preferences.\",\n",
       " '53e99796b7602d9701f5c563': \"An analysis of the polarimetric scattering properties of oil-covered waters is conducted using the classic Poincaré ellipticity parameter chi (χ) from the Stokes parameters of hybrid polarized mode Synthetic Aperture Radar (SAR). For natural oil seeps, χ has a change in signs, comparing oil-covered waters with the 'clean' ocean surface. The χ sign reversal is basic to sign difference oil spill detection methods. However, a problem is that oil spills related to the Deep Water Horizon (DWH) disaster did not exhibit a reversal in χ signs, when 'clean' ocean surfaces are compared to the area contaminated by crude oil. Therefore, the scattering mechanism related to the oil seeps is different from that of the DWH oil spill; the former is dominated with even bounce scattering and the latter is dominated by Bragg scattering, similar to that of the clean oil-free ocean surface. © 2013 IEEE.\",\n",
       " '53e99796b7602d9701f5c571': \"In this letter, we propose a new spatio-spectral Laplacian support vector machine (SS-LapSVM) for semi-supervised hyperspectral image classification. The clustering assumption on spectral vectors is used to formulate a manifold regularizer, and neighborhood spatial constraints of hyperspectral images are designed to construct a spatial regularizer. Moreover, a non-iterative optimization procedure is presented to solve this dual-regularized SVM, which makes rapid classification possible. By combining spatial and spectral information together, SS-LapSVM can avoid the speckle-like misclassification of hyperspectral images in the original Lap-SVM. The performance of SS-LapSVM is evaluated on AVIRIS image data taken over Indiana's Indian Pine, and the results show that it can achieve accurate and rapid classification with a small number of labeled data, and outperform state-of-the-art semi-supervised approaches.\",\n",
       " '53e99796b7602d9701f5c598': 'When distributing digital content over a broadcast channel it\\'s often necessary to revoke users whose access privileges have expired, thus preventing them from recovering the content. This works well when users make a conscious decision to leave the system or have misbehaved, but numerous cases exist in which the revocation is in error and users are consequently left with the often onerous burden of getting reinstated. We introduce a gradual form of revocation that we call service degradation that enables the content distributor to provide \"cues\" to the user in the form of degraded system performance. The cues alert the user to their impending revocation and allow them to take the necessary action to remain in the system. Our protocols build on techniques for broadcast encryption and spam-fighting to provide the appropriate form of service for this previously ignored class of users.',\n",
       " '53e99796b7602d9701f61877': 'Clustering results often critically depend on density and similarity, and its complexity often changes along with the augment of sample dimensionality. In this paper, we refer to classical shared nearest neighbor clustering algorithm (SNN), and provide a high-dimensional shared nearest neighbor clustering algorithm (DSNN). This DSNN is evaluated using a freeway traffic data set, and experiment results show that DSNN settles many disadvantages in SNN algorithm, such as outliers, statistic, core points, computation complexity etc, also attains better clustering results on multi-dimensional data set than SNN algorithm.',\n",
       " '53e9979bb7602d9701f650d6': 'Project plans fail to reflect the real work situation if they represent infrequently updated, high level objectives. We propose a tangible interface that allows team members to update and share their progress in real time through peripheral task management.',\n",
       " '53e9979bb7602d9701f697e4': 'Researchers in operational research have used fuzzy c-means clustering ubiquitously in a wide range of areas. It appears that Internet portals can also benefit from this powerful methodology. This paper tests the applicability of fuzzy c-means clustering to Internet portals and shows that there exist meaningful clusters among the potential users of an Internet portal that offers business-related information. The results suggest that Internet portals can use fuzzy c-means clustering to determine homogenous groups of people and offer specific information to each group.',\n",
       " '53e9979eb7602d9701f6b38a': 'In this paper, we investigate second order Cohen–Grossberg neural networks with transmission delays and an unsupervised Hebbian-type learning behavior. By using Laypunov–Krasovskii functional, some new sufficient conditions are established for the existence and global exponential p-stability of a unique equilibrium without strict conditions imposed on self regulation functions. The obtained sufficient conditions are easy to verify and our results improve the previously known results. Finally, computer simulations are performed to illustrate exponential p-stability of equilibrium and learning dynamic behavior of neurons.',\n",
       " '53e9979eb7602d9701f6c8c4': 'Transmission electron microscopy (TEM) is an important modality for the analysis of cellular structures in neurobiology. The computational analysis of neurons entail their segmentation and reconstruction from TEM images. This problem is complicated by the heavily textured nature of cellular TEM images and typically low signal-to-noise ratios. In this paper, we propose a new partial differential equation for enhancing the contrast and continuity of cell membranes in TEM images.',\n",
       " '53e9979eb7602d9701f6ee9c': 'The range of books reviewed is wide, covering theory and applications in operations research, statistics, management science, econometrics, mathematics, computers, and information systems (no software is reviewed). In addition, we include books in other fields that emphasize technical applications. Publishers who wish to have their books reviewed should send them to Professor Benjamin Lev. We list the books received; not all books received can be reviewed because space is limited. Those who would like to review books are urged to send me their names, addresses, and specific areas of expertise. We commission all reviews and do not accept unsolicited book reviews. Readers are encouraged to suggest books that might be reviewed or to ask publishers to send me copies of such books. The authors or editors of the books reviewed in this issue are Stephen Biddle, Michael J. Brusco, Stephanie Stahl, David G. Luenberger, Christoph Schwindt, and John Wang.',\n",
       " '53e997a2b7602d9701f72efc': 'Increasing complexity of large scale distributed systems is becoming unmanageable because of the manual style adopted for the management today. The manual management techniques are time-consuming, un-secure and more prone to errors. A new paradigm for self-management is pervading over the old manual system to begin the next generation of computing. In this paper we have, proposed and implemented the automated multiagent based approach to self-heal the grid system from faults. This approach automatically monitor and handle the faults occurred in grid environment while executing the job.',\n",
       " '53e997a2b7602d9701f74b58': 'In this work, decision feedback (DF) detection algorithms based on multiple processing branches for multi-input multi-output (MIMO) spatial multiplexing systems are proposed. The proposed detector employs multiple cancellation branches with receive filters that are obtained from a common matrix inverse and achieves a performance close to the maximum likelihood detector (MLD). Constrained minimum mean-squared error (MMSE) receive filters designed with constraints on the shape and magnitude of the feedback filters for the multi-branch MMSE DF (MB-MMSE-DF) receivers are presented. An adaptive implementation of the proposed MB-MMSE-DF detector is developed along with a recursive least squares-type algorithm for estimating the parameters of the receive filters when the channel is time-varying. A soft-output version of the MB-MMSE-DF detector is also proposed as a component of an iterative detection and decoding receiver structure. A computational complexity analysis shows that the MB-MMSE-DF detector does not require a significant additional complexity over the conventional MMSE-DF detector, whereas a diversity analysis discusses the diversity order achieved by the MB-MMSE-DF detector. Simulation results show that the MB-MMSE-DF detector achieves a performance superior to existing suboptimal detectors and close to the MLD, while requiring significantly lower complexity.',\n",
       " '53e997a2b7602d9701f77b85': 'This paper introduces Cárnico-ICSPEA2, a metaheuristic co-evolutionary navigator designed by its end-user as an aid for the analysis and multi-objective optimisation of a beef cattle enterprise running on temperate pastures and fodder crops in Chalco, Mexico State, in the central plateau of Mexico. By combining simulation routines and a multi-objective evolutionary algorithm with a deterministic and stochastic framework, the software imitates the evolutionary behaviour of the system of interest, helping the farm manager to ‘navigate’ through his system’s dynamic phase space. The ultimate goal was to enhance the manager’s decision-making process and co-evolutionary skills, through an increased understanding of his system and the discovery of new, improved heuristics. This paper describes the numerical simulation and optimisation resulting from the application of Cárnico-ICSPEA2 to solve a specific multi-objective optimisation problem, along with implications for the management of the system of interest.',\n",
       " '53e997a6b7602d9701f7b09a': 'A Mobile Ad hoc NETwork (MANET) is a group of mobile nodes that form a multihop wireless network. The topology of the network can change randomly due to unpredictable mobility of nodes and propagation characteristics. Previously, it was assumed that the nodes in the network were assigned IP addresses a priori. This may not be feasible as nodes can enter and leave the network dynamically. A dynamic IP address assignment protocol like DHCP requires centralized servers that may not be present in MANETs. Hence, we propose a distributed protocol for dynamic IP address assignment to nodes in MANETs. The proposed solution guarantees unique IP address assignment under a variety of network conditions including message losses, network partitioning and merging. Simulation results show that the protocol incurs low latency and communication overhead for an IP address assignment.',\n",
       " '53e997a6b7602d9701f7af85': \"Many fault localization methods have been proposed in the literature. These methods take in a set of program execution profiles and output a list of suspicious program elements. The list of program elements ranked by their suspiciousness is then presented to developers for manual inspection. Currently, the suspicious elements are ranked in a batch process where developers' inspection efforts are rarely utilized for ranking. The inaccuracy and static nature of existing fault localization methods prompt us to incorporate user feedback to improve the accuracy of the existing methods. In this paper, we propose an interactive fault localization framework that leverages simple user feedback. Our framework only needs users to label the statements examined as faulty or clean, which does not require additional effort than conventional non-interactive methods. After users label suspicious program elements as faulty or clean, our framework incorporates such information and re-orders the rest of the suspicious program elements, aiming to expose truly faulty elements earlier. We have integrated our solution with three well-known fault localization methods: Ochiai, Tarantula, and Jaccard. The evaluation on five Unix programs and the Siemens test suite shows that our solution achieves significant improvements on fault localization accuracy.\",\n",
       " '53e997a6b7602d9701f7b0eb': \"As we know, the performance of the elliptic curve cryptosystem (ECC) deeply depends on the computation of scalar multiplication. Thus, how to speed up the computation of the elliptic curve scalar multiplication is a significant issue. In 1994, Lim and Lee proposed a more flexible precomputation method used in wireless networks environments for speeding up the computation of exponentiation. This method can be also used for speeding up the scalar multiplication of elliptic curves. We call it LLECC method. However, the less storage is equipped with the computing devices, the less efficient it is. For this reason, we propose a more efficient algorithm than LLECC's in this paper. First, we modify LLECC method to reduce the storage of precomputed values, and then propose an efficient algorithm based on the nonadjacent form (NAF) representation and multidoubling method. Furthermore, the proposed algorithm can be also used for speeding up the multi-point multiplication of elliptic curves. According to the simulation results, the proposed algorithm can reduce 11% and 21% in the aspect of the computational complexity and storage cost, respectively, in an elliptic curve of size 160-bit over finite fields with characteristic greater than 3, as compared with LLECC's. Finally, we implement the elliptic curve digital signature algorithm-like (ECDSA-like) system in the personal digital assistant (PDA) using the proposed algorithms to improve the scalar multiplication.\",\n",
       " '53e997a6b7602d9701f7c682': 'This paper investigates the use of tangible user interfaces (TUIs) for controlling multimedia systems. It presents a preliminary stucty where a tangible device called ARemote is used to choose TV channels in a relatively large list by performing 30 gestures. Three dzflerent interaction strategies, that either relies on crossing or scrolling, have been developed and tested in a controlled experiment.',\n",
       " '53e997a6b7602d9701f7d5aa': 'A k-product uncapacitated facility location problem can be described as follows. There is a set of demand points where clients are located and a set of potential sites where facilities of unlimited capacities can be set up. There are k different kinds of products. Each client needs to be supplied with k kinds of products by a set of k different facilities and each facility can be set up to supply only a distinct product with a non-negative fixed cost determined by the product it intends to supply. There is a non-negative cost of shipping goods between each pair of locations. These costs are assumed to be symmetric and satisfy the triangle inequality. The problem is to select a set of facilities to be set up and their designated products and to find an assignment for each client to a set of k facilities so that the sum of the setup costs and the shipping costs is minimized. In this paper, an approximation algorithm within a factor of 2k+1 of the optimum cost is presented. Assuming that fixed setup costs are zero, we give a 2k-1 approximation algorithm for the problem. In addition we show that for the case k=2, the problem is NP-complete when the cost structure is general and there is a 2-approximation algorithm when the costs are symmetric and satisfy the triangle inequality. The algorithm is shown to produce an optimal solution if the 2-product uncapacitated facility location problem with no fixed costs happens to fall on a tree graph.',\n",
       " '53e997a6b7602d9701f7f1a4': 'This paper deals with the steady state behaviour of an M/G/1 retrial queue with an additional second phase of optional service subject to breakdowns occurring randomly at any instant while serving the customers. This model generalizes both the classical M/G/1 retrial queue subject to random breakdown as well as M/G/1 queue with second optional service and server breakdowns. We carry out an extensive analysis of this model.',\n",
       " '53e997a6b7602d9701f7f1a5': 'A novel optimal Finite Word Length (FWL) controller design is proposed in the framework of μ theory. A computationally tractable close-loop stability measure with FWL implementation considerations of the controller is derived based on the μ theory, and the optimal FWL controller realizations are obtained by solving the resulting optimal FWL realization problem using linear matrix inequality techniques.',\n",
       " '53e997aab7602d9701f8050e': 'In this paper, a δ-shock maintenance model for a deteriorating system is studied. Assume that shocks arrive according to a renewal process, the interarrival time of shocks has a Weibull distribution or gamma distribution. Whenever an interarrival time of shocks is less than a threshold, the system fails. Assume further the system is deteriorating so that the successive threshold values are geometrically nondecreasing, and the consecutive repair times after failure form an increasing geometric process. A replacement policy N is adopted by which the system will be replaced by an identical new one at the time following the Nth failure. Then the long-run average cost per unit time is evaluated. Afterwards, an optimal policy N* for minimizing the long-run average cost per unit time could be determined numerically.',\n",
       " '53e997aab7602d9701f8167b': 'A β-expectation tolerance interval procedure is derived from the concept of generalized pivotal quantity, which has been frequently used to obtain confidence intervals in situations where standard procedures do not lead to useful solutions. The proposed procedure can be applied to general balanced mixed linear models. Some practical examples are given to illustrate the proposed procedure. In addition, detailed simulation studies are conducted to evaluate its performance, showing that it can be recommended for use in practical applications.',\n",
       " '53e997aab7602d9701f82c7e': 'It is shown that the Bruhat partial order on permutations is equivalent to a certain natural partial order induced by TP2 matrices and that a positive matrix is TP2 if (and only if) it satisfies certain inequalities induced by Bruhat order. (c) 2010 Elsevier B.V. All rights reserved.',\n",
       " '53e997abb7602d9701f85d7b': 'Unified Parallel C (UPC) is a Partitioned Global Address Space (PGAS) language that exhibits high performance and portability on a broad class of shared and distributed memory parallel architectures. This paper describes the design and implementation of a parallel numerical library for UPC built on top of the sequential BLAS routines. The developed library exploits the particularities of the PGAS paradigm, taking into account data locality in order to guarantee a good performance. The library was experimentally validated, demonstrating scalability and efficiency.',\n",
       " '53e997abb7602d9701f85f26': 'Let K be an unbounded convex polyhedral subset of R n represented by a system of linear constraints, and let K Δ be the convex hull of the set of extreme points of K . We show that the combinatorial-facial structure of K does not uniquely determine the combinatorial-facial structure of K Δ . We prove that the problem of checking whether two given extreme points of K are nonadjacent on K Δ , is NP-complete in the strong sense. We show that the problem of deriving a linear constraint representation of K Δ , leads to the question of checking whether the dimension of K Δ is the same as that of K , and we prove that resolving this question is hard because it needs the solution of some NP-complete problems. Finally we provide a formula for the dimension of K Δ , under a nondegeneracy assumption.',\n",
       " '53e997abb7602d9701f86129': 'Information systems fail for a number of reasons. Several failure reasons include communication, complexity, organization, technology, and leadership. Failure can be outlined in four major categories: technical shortcomings, project management shortcomings, organizational issues, and the continuing information explosion. Change management is the process of assisting individuals and organizations in passing from an old way of doing things to a new way of doing things. Change management starts early in a technical process, as the need for making major changes starts at the conceptual level. This paper briefly covers the people side of implementing new information systems, and describes resistance to change and various strategies to manage technological change.',\n",
       " '53e997aeb7602d9701f887b9': 'Saadani et al. [N.E.H. Saadani, P. Baptiste, M. Moalla, The simple F2∥Cmax with forbidden tasks in first or last position: A problem more complex that it seems, European Journal of Operational Research 161 (2005) 21–31] studied the classical n-job flow shop scheduling problem F2∥Cmax with an additional constraint that some jobs cannot be placed in the first or last position. There exists an optimal job sequence for this problem, in which at most one job in the first or last position is deferred from its position in Johnson’s [S.M. Johnson, Optimal two- and three-stage production schedules with setup times included, Naval Research Logistics Quarterly 1 (1954) 61–68] permutation. The problem was solved in O(n2) time by enumerating all candidate job sequences. We suggest a simple O(n) algorithm for this problem provided that Johnson’s permutation is given. Since Johnson’s permutation can be obtained in O(nlogn) time, the problem in Saadani et al. (2005) can be solved in O(nlogn) time as well.',\n",
       " '53e997aeb7602d9701f89a32': 'Most simulation models for data communication networks encompass hundreds of parameters that can each take on millions of values. Such models are difficult to understand, parameterize and investigate. This paper explains how to model a modern data communication network concisely, using only 20 parameters. Further, the paper demonstrates how this concise model supports efficient design of simulation experiments. The model has been implemented as a sequential simulation called MesoNet, which uses Simulation Language with Extensibility (SLX). The paper discusses model resource requirements and the performance of SLX. The model and principles delineated in this paper have been used to investigate parameter spaces for large (hundreds of thousands of simultaneously active flows), fast (hundreds of Giga-bits/second) simulated networks under a variety of congestion control algorithms.',\n",
       " '53e997aeb7602d9701f89ed5': 'The face inversion effect has been used as a basis for claims about the specialization of face-related perceptual and neural processes. One of these claims is that the fusiform face area FFA is the site of face-specific feature-based and/or configural/holistic processes that are responsible for producing the face inversion effect. However, the studies on which these claims were based almost exclusively used stimulus manipulations of whole faces. Here, we tested inversion effects using single, discrete features and combinations of multiple discrete features, in addition to whole faces, using both behavioral and fMRI measurements. In agreement with previous studies, we found behavioral inversion effects with whole faces and no inversion effects with a single eye stimulus or the two eyes in combination. However, we also found behavioral inversion effects with feature combination stimuli that included features in the top and bottom halves eyes-mouth and eyes-nose-mouth. Activation in the FFA showed an inversion effect for the whole-face stimulus only, which did not match the behavioral pattern. Instead, a pattern of activation consistent with the behavior was found in the bilateral inferior frontal gyrus, which is a component of the extended face-preferring network. The results appear inconsistent with claims that the FFA is the site of face-specific feature-based and/or configural/holistic processes that are responsible for producing the face inversion effect. They are more consistent with claims that the FFA shows a stimulus preference for whole upright faces.',\n",
       " '53e997aeb7602d9701f8caec': 'Diverse structural and functional brain alterations have been identified in both schizophrenia and bipolar disorder, but with variable replicability, significant overlap and often in limited number of subjects. In this paper, we aimed to clarify differences between bipolar disorder and schizophrenia by combining fMRI (collected during an auditory oddball task) and diffusion tensor imaging (DTI) data. We proposed a fusion method, “multimodal CCA+ joint ICA”, which increases flexibility in statistical assumptions beyond existing approaches and can achieve higher estimation accuracy. The data collected from 164 participants (62 healthy controls, 54 schizophrenia and 48 bipolar) were extracted into “features” (contrast maps for fMRI and fractional anisotropy (FA) for DTI) and analyzed in multiple facets to investigate the group differences for each pair-wised groups and each modality. Specifically, both patient groups shared significant dysfunction in dorsolateral prefrontal cortex and thalamus, as well as reduced white matter (WM) integrity in anterior thalamic radiation and uncinate fasciculus. Schizophrenia and bipolar subjects were separated by functional differences in medial frontal and visual cortex, as well as WM tracts associated with occipital and frontal lobes. Both patients and controls showed similar spatial distributions in motor and parietal regions, but exhibited significant variations in temporal lobe. Furthermore, there were different group trends for age effects on loading parameters in motor cortex and multiple WM regions, suggesting that brain dysfunction and WM disruptions occurred in identified regions for both disorders. Most importantly, we can visualize an underlying function–structure network by evaluating the joint components with strong links between DTI and fMRI. Our findings suggest that although the two patient groups showed several distinct brain patterns from each other and healthy controls, they also shared common abnormalities in prefrontal thalamic WM integrity and in frontal brain mechanisms.',\n",
       " '53e997b2b7602d9701f94f64': \"The purpose of this paper is to share the experience gained in, and the efforts made toward, introducing and implementing a new course in the challenging and important area of geophysical signal processing at the Electrical Engineering (EE) Department, King Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia. The new course, titled “Geo-Signal Processing,” was offered both at the graduate level and as a special topics course to undergraduates. The course was developed in collaboration with the Earth Sciences Department at KFUPM. This paper contributes new information because it stresses the multidisciplinary aspects of digital signal processing (DSP) technologies when applied to estimating the Earth's layered structure on the basis of seismic data. Unlike many Earth sciences seismic data processing courses, this Geo-Signal Processing course also emphasizes that the perspective taken by those working in DSP is different from that taken by geophysicists. The course presents DSP with particular emphasis on seismic data signals and the artifacts accompanying them while covering the principles and algorithms needed for processing seismic signals in both deterministic and statistical fashion. Topics include, but are not limited to, basic seismic theory, acquisition of seismic data, analysis of seismic signals and noise, deterministic filtering of seismic data, and statistical processing of seismic data.\",\n",
       " '53e997b2b7602d9701f94f82': \"A main challenge in today's embedded system design is to find the perfect balance between performance and power consumption. This paper presents a run-time resource management framework for embedded heterogeneous multi-core platforms. It allows dynamic adaptation to changing application context and transparent optimization of the platform resource usage following a distributed and hierarchical approach. A Global Resource Manager (GRM) is running in parallel with the central manager of the application on the host processor of the platform. Each IP core of the platform can execute its own Local Resource Manager (LRM), and the GRM conforms to practices of each LRM. The operating points managed by the GRM are identified in a design-space exploration phase as a set of Pareto-optimal configurations of the application and their impacts with regards to the quality of experience, performance and energy consumption. The GRM has already been integrated in a POSIX version of an audio-driven video surveillance application in order to maximize its QoE parameters with respect to the battery duration and the energy budget of the platform, used to analyze the GRM efficiency.\",\n",
       " '53e997b5b7602d9701f97587': 'This paper describes a novel interface named the K-explorer that provides the user with a multiple access channel to information of the real-world objects by using a smart phone (K-tai in Japanese) that people carry at almost all times. The K-explorer system presents interfaces to obtain information of objects in the real-world by a smartphone so that an information space is overlaid and mixed to augment the real-space where people work everyday. The system implements two interfaces, the remote interface and the direct interface, with the goals of constant connectivity, extensibility and context sensitivity. The information of remote objects can be obtained with the remote interface based on a map, images and keywords. The direct interface is embodied by introducing a K-station that augments limited connectivity of a K-tai to the real-world. Three types of K-station, the Tray-, Desk-Light- and Bag-type implementations were built to demonstrate specific functions. Consequently the augmentation of information of the real-world objects was realized as retrieval, linking, registration, tracing a history and position, and providing plan related information of objects. A user evaluation suggested the effectiveness of the system.',\n",
       " '53e997b5b7602d9701f99968': '. \\xa0\\xa0In this paper we study primal-dual path-following algorithms for the second-order cone programming (SOCP) based on a family\\n of directions that is a natural extension of the Monteiro-Zhang (MZ) family for semidefinite programming. We show that the\\n polynomial iteration-complexity bounds of two well-known algorithms for linear programming, namely the short-step path-following\\n algorithm of Kojima et al. and Monteiro and Adler, and the predictor-corrector algorithm of Mizuno et al., carry over to the\\n context of SOCP, that is they have an O(\\u2009logε-1) iteration-complexity to reduce the duality gap by a factor of ε, where n is the number of second-order cones. Since the MZ-type family studied in this paper includes an analogue of the Alizadeh,\\n Haeberly and Overton pure Newton direction, we establish for the first time the polynomial convergence of primal-dual algorithms\\n for SOCP based on this search direction.',\n",
       " '53e997bab7602d9701fa09cb': 'We present a new algorithm for simulating large-scale crowds at interactive rates based on the Principle of Least Effort. Our approach uses an optimization method to compute a biomechanically energy-efficient, collision-free trajectory that minimizes the amount of effort for each heterogeneous agent in a large crowd. Moreover, the algorithm can automatically generate many emergent phenomena such as lane formation, crowd compression, edge and wake effects ant others. We compare the results from our simulations to data collected from prior studies in pedestrian and crowd dynamics, and provide visual comparisons with real-world video. In practice, our approach can interactively simulate large crowds with thousands of agents on a desktop PC and naturally generates a diverse set of emergent behaviors.',\n",
       " '53e997bab7602d9701fa0de6': 'In this paper, we consider a heterogeneous WSN consists of sensor nodes having limited computation and communication capabilities. For our scheme, sensor nodes are deployed in groups called clusters composed of cluster head and cluster members. Therefore, there is a possibility that the nodes in one cluster are not in communication range of neighboring cluster. As we know in WSN, the message has to be transmitted to long distances which increase the communication overhead. For this reason, a gateway node is required, so that two neighboring cluster heads can communicate with each other. We propose a scheme to select a gateway node by identifying the boundary points through convex hull approach. After identifying the gateway node, we choose the protocol for key exchange between the two gateway nodes. It will minimize the communication and computation overhead.',\n",
       " '53e997bab7602d9701fa0de7': \"This paper examines some characteristics of the 'British School' of information science. Three main forces driving the development of the new subject in Britain are identified: the documentation movement; special libraries; and the need for better treatment of scientific and technical information. Five characteristics which, taken together, distinguish the early British approach to information science from those adopted elsewhere are identified: its subject-based nature; its broad approach to information and information science; its status as an academic subject with a strong professional remit; its involvement with, but distinction from, information technology; and its involvement with memory institutions. Lessons are drawn for the future development of the information sciences.\",\n",
       " '53e997bab7602d9701fa2187': 'Cliquewidth and NLC-width are two closely related parameters that measure the complexity of graphs. Both clique- and NLC-width are defined to be the minimum number of labels required to create a labelled graph by certain terms of operations. Many hard problems on graphs become solvable in polynomial-time if the inputs are restricted to graphs of bounded clique- or NLC-width. Cliquewidth and NLC-width differ at most by a factor of two. The relative counterparts of these parameters are defined to be the minimum number of labels necessary to create a graph while the tree-structure of the term is fixed. We show that Relative Cliquewidth and Relative NLC-width differ significantly in computational complexity. While the former problem is NP-complete the latter is solvable in polynomial time. The relative NLC-width can be computed in O(n^3) time, which also yields an exact algorithm for computing the NLC-width in time O(3^nn). Additionally, our technique enables a combinatorial characterisation of NLC-width that avoids the usual operations on labelled graphs.',\n",
       " '53e997bab7602d9701fa2b44': 'Increasingly, for extensibility and performance, special purpose application code is being integrated with database system code. Such application code has direct access to database system buffers, and as a result, the danger of data being corrupted due to inadvertent application writes is increased. Previously proposed hardware techniques to protect from corruption require system calls, and their performance depends on details of the hardware architecture. We investigate an alternative approach which uses codewords associated with regions of data to detect corruption and to prevent corrupted data from being used by subsequent transactions. We develop several such techniques which vary in the level of protection, space overhead, performance, and impact on concurrency. These techniques are implemented in the Dalí main-memory storage manager, and the performance impact of each on normal processing is evaluated. Novel techniques are developed to recover when a transaction has read corrupted data caused by a bad write and gone on to write other data in the database. These techniques use limited and relatively low-cost logging of transaction reads to trace the corruption and may also prove useful when resolving problems caused by incorrect data entry and other logical errors.',\n",
       " '53e997bdb7602d9701fa70a1': 'We show that if a flow network has k input/output terminals (for the traditional maximum-flow problem, k =2), its external flow pattern (the possible values of flow into and out of the terminals) has two characterizations of size independent of the total number of vertices: a set of 2 k +1 inequalities in k variables representing flow values at the terminals, and a mimicking network with at most 2 2 k vertices and the same external flow pattern as the original network. For the case in which the underlying graph has bounded treewidth, we present sequential and parallel algorithms that can compute these characterizations as well as a flow consistent with any desired feasible external flow (including a maximum flow between two given terminals). For constant k , the sequential algorithm runs in O ( n ) time on n -vertex networks, and the parallel algorithm runs in O (log n ) time on an EREW PRAM with O ( n /log n ) processors if an explicit tree decomposition of the network of size O ( n ) is given; if not, known algorithms can compute such a tree decomposition in O ((log n ) 2 ) time using O ( n /(log n ) 2 ) processors. References REFERENCES 1 R.K. Ahuja T.L. Magnanti J.B. Orlin Network Flows: Theory, Algorithms, and Applications 1993 Prentice–Hall Englewood Cliffs 2 S.R. Arikati S. Chaudhuri C.D. Zaroliagis All-pairs min-cut in sparse networks Proc. 15th Conf. on Foundations of Software Technology and Theoretical Computer Science Lecture Notes in Comput. Sci. 1026 1995 Springer-Verlag Berlin/Heidelberg p. 363–376 3 S. Arnborg J. Lagergren D. Seese Easy problems for tree-decomposable graphs J. Algorithms 12 1991 308 340 4 H.L. Bodlaender Dynamic programming on graphs with bounded treewidth Proc. 15th International Coll. on Automata, Languages and Programming Lecture Notes in Comput. Sci. 317 1988 Springer-Verlag Berlin/Heidelberg p. 105–118 5 H.L. Bodlaender NC-algorithms for graphs with small treewidth Proc. 14th International Workshop on Graph-Theoretic Concepts in Computer Science Lecture Notes in Comput. Sci. 344 1989 Springer-Verlag Berlin/Heidelberg p. 1–10 6 H.L. Bodlaender A linear-time algorithm for finding tree-decompositions of small treewidth SIAM J. Comput. 25 1996 1305 1317 7 H. L. Bodlaender, T. Hagerup, Parallel algorithms with optimal speedup for bounded treewidth, SIAM J. Comput, in 8 M. Chrobak, K. Diks, Network flows in outerplanar graphs, 1987 9 K.L. Clarkson Linear programming in O n d 2 ) time Inform. Process. Lett. 22 1986 21 24 10 R. Cole U. Vishkin Deterministic coin tossing with applications to optimal parallel list ranking Inform. and Control 70 1986 32 53 11 S. Cook C. Dwork R. Reischuk Upper and lower time bounds for parallel random access machines without simultaneous writes SIAM J. Comput. 15 1986 87 97 12 M.E. Dyer On a multidimensional search technique and its application to the Euclidean one-centre problem SIAM J. Comput. 15 1986 725 738 13 L.R. Ford Jr. D.R. Fulkerson Flows in Networks 1962 Princeton Univ. Press Princeton 14 D. Gale A theorem on flows in networks Pacific J. Math. 7 1957 1073 1082 15 A.V. Goldberg Processor-efficient implementation of a maximum flow algorithm Inform. Process. Lett. 38 1991 179 185 16 L.M. Goldschlager R.A. Shaw J. Staples The maximum flow problem is log space complete for P Theoret. Comput. Sci. 21 1982 105 111 17 A.J. Hoffman Some recent applications of the theory of linear inequalities to extremal combinatorial analysis Combinatorial Analysis Amer. Math. Soc. 1960 Providence RI p. 113–127 18 J. JáJá An Introduction to Parallel Algorithms 1992 Addison–Wesley Reading 19 D.B. Johnson Parallel algorithms for minimum cuts and maximum flows in planar networks J. Assoc. Comput. Mach. 34 1987 950 967 20 R.M. Karp E. Upfal A. Wigderson Constructing a perfect matching is in random NC Combinatorica 6 1986 35 48 21 N. Robertson P.D. Seymour Graph Minors. II. Algorithmic aspects of tree-width J. Algorithms 7 1986 309 322 22 Y. Shiloach U. Vishkin An O n 2 log n J. Algorithms 3 1982 128 146 23 J. van Leeuwen Graph algorithms J. van Leeuwen Handbook of Theoretical Computer Science, Vol. A: Algorithms and Complexity 1990 Elsevier Science Amsterdam 525 631 24 K. Weihe Maximum ( s t V V J. Comput. System Sci. 55 1997 454 475',\n",
       " '53e997c2b7602d9701fae9e0': 'Research data centers are often composed of thousands of diverse computer systems used for ongoing research, development, software regression and hardware compatibility testing. The usage patterns of many of these systems result in periodic non-use and extended periods of idleness. Users routinely fail to ensure that idle machines are powered down prior to overnight or extended absence periods. The annual amount of wasted energy in the HP Bangalore development data center is estimated at 14400 MWh resulting in over 8600 tons of CO2 emissions per year. In this paper, we propose Idle Machine Power Savings (IMPS), which seeks to address potential power cost savings and minimize environmental impact. IMPS consists of a low overhead, highly scalable data acquisition framework enabling the development of algorithms (an artificial neural network is used in the initial prototype) for automatic \"extended idle\" notifications and optional automatic shutdown of unused computers in data centers. This paper describes our approach, the framework, a prototype implementation and provides preliminary results. The results show an enormous potential for energy savings that translate directly into financial savings and lowered greenhouse gas emissions.',\n",
       " '53e997c6b7602d9701fb6c83': 'This paper addresses robust H∞ fuzzy static output feedback control problem for T-S fuzzy systems with time-varying norm-bounded uncertainties. Sufficient conditions for synthesis of a fuzzy static output feedback controller for T-S fuzzy systems are derived in terms of a set of linear matrix inequalities (LMIs). In comparison with the existing literatures, the proposed approach not only simplifies the design procedure but also achieves a better H∞ performance. Three drawbacks existing in the previous papers such as coordinate transformation, same output matrices and BMI problem have been eliminated. The effectiveness of the proposed design method is demonstrated by an example for the control of a truck–trailer system.',\n",
       " '53e997c6b7602d9701fb6ce9': 'In this paper robust H∞ control for Takagi–Sugeno general uncertain fuzzy systems where uncertainties come into all the system matrices is considered. The main result is to establish equivalent relationships between quadratic stability and quadratic stability with H∞ disturbance attenuation γ of general uncertain fuzzy systems and H∞ control for fuzzy systems without uncertainty. These relationships imply that quadratically stabilizing controllers and quadratically stabilizing controllers with H∞ disturbance attenuation γ for general uncertain fuzzy systems can be obtained by designing H∞ controllers for fuzzy systems without uncertainties. We first give sufficient conditions for the H∞ norm being less than a given number. We then consider a general H∞ problem with output feedback controllers, and give a design method of H∞ controllers and sufficient conditions which guarantee the required H∞ performance of the closed-loop system. This design method can be applied to quadratic stabilizing controllers and quadratic stabilizing controllers with H∞ disturbance attenuation γ for general uncertain fuzzy systems. Next we analyze quadratic stability and quadratic stability with H∞ disturbance attenuation γ of general uncertain fuzzy systems and establish equivalent relationships to H∞ control of fuzzy systems without uncertainties. Based on these relationships, we design robust controllers for uncertain fuzzy systems. Finally, examples are given to illustrate the theory.',\n",
       " '53e997c6b7602d9701fb706b': 'Most methods for synthesizing panoramas assume that the scene is static. A few methods have been proposed for synthesizing stereo or motion panoramas, but there has been little attempt to synthesize panoramas that have both stereo and motion. One faces several challenges in synthesizing stereo motion panoramas, for example, to ensure temporal synchronization between left and right views in each frame, to avoid spatial distortion of moving objects, and to continuously loop the video in time. We have recently developed a stereo motion panorama method that tries to address some of these challenges. The method blends space-time regions of a video XYT volume, such that the blending regions are distinct and translate over time. This article presents a perception experiment that evaluates certain aspects of the method, namely how well observers can detect such blending regions. We measure detection time thresholds for different blending widths and for different scenes, and for monoscopic versus stereoscopic videos. Our results suggest that blending may be more effective in image regions that do not contain coherent moving objects that can be tracked over time. For example, we found moving water and partly transparent smoke were more effectively blended than swaying branches. We also found that performance in the task was roughly the same for mono versus stereo videos.',\n",
       " '53e997c6b7602d9701fb92c3': \"This preliminary study is intended to obtain the background of basic ICT skill required for teachers to develop the Augmented Reality (AR) application using available AR authoring tool. The AR technology enables the virtual world to be in real world concurrently. These features have been identified to be able to accommodate the learning experience in order to promote learning. The samples are comprised of nine AR application developers and forty four teachers. Interview schedule, questionnaire and demonstration are used to collect data. The findings revealed that teachers have low technical skills in order to develop the AR application using available authoring tool according to the requirement listed by the AR application developer. This indicates that there is a need to develop the AR authoring tool which is easy to use in the perspective of teachers. It shows that it is difficult for them to develop their own AR application because of the technical skill required, although they are very interested in the technology and agreed that the technology can help to enhance the students' cognitive process.\",\n",
       " '53e997cbb7602d9701fbb299': 'The Poisson regression model is frequently used to analyze count data. Pseudo R-squared measures for Poisson regression models have recently been proposed and bias adjustments recommended in the presence of small samples and/or a large number of covariates. In practice, however, data are often over- or sometimes even underdispersed as compared to the standard Poisson model. The definition of Poisson R-squared measures can be applied in these situations as well, albeit with bias adjustments accordingly adapted. These adjustments are motivated by arguments of quasi-likelihood theory. Properties of unadjusted and adjusted R-squared measures are studied by simulation under standard Poisson; over- and underdispersed Poisson regression models and their use is exemplified and discussed with popcorn data.',\n",
       " '53e997cbb7602d9701fbc521': 'A dynamic broadcast encryption (DBE) is a broadcast encryption (BE) scheme where a new user can join the system anytime without modifying preexisting user decryption keys. In this paper, we propose a non-interactive dynamic identity-based broadcast encryption (DIBBE) scheme that is fully secure without random oracles. The PKG does not need to execute any interactive operation with the user during the lifetime of the system. The ciphertext is of constant size, and the public key size is linear in the maximal number of receivers for one encryption. This is the first non-interactive DIBBE scheme which is fully secure without random oracles, and it is collusion resistant for arbitrarily large collusion of users.',\n",
       " '53e997cbb7602d9701fbc532': 'We describe how simulation using production code can help in achieving higher quality software while reducing the effort associated with embedded application development. The recommended steps in developing embedded safety-critical software are outlined, and the issues of proper verification of the simulation models are discussed in detail. The feasibility of the approach is shown on a case study of a brake-by-wire application development.',\n",
       " '53e997cbb7602d9701fbc567': 'In mobile environments, packet transmission services suffer from packet losses due to e.g. inadequate received signal quality, or forced by protocols in the signaling domain of the infrastructure of a mobile communications network. To reduce the occurrence of packet losses and, hence, to improve the quality of packet transmission services, such as e.g. the Short Message Service (SMS) deployed in GSM (Global System for Mobile Communications) based mobile communications networks, a quantitative analysis of the Quality of Service (QoS) in the signaling domain is mandatory. For this reason, appropriate QoS parameters are needed. In this communication, the authors propose such QoS parameters and apply them to the SMS in GSM networks. Furthermore, a system framework for QoS monitoring, alerting and reconfiguring an SMS Center is presented. It operates near real-time and, therefore, helps to maintain a high QoS level. Selected monitoring results gathered during real world network operation are presented.',\n",
       " '53e997cbb7602d9701fbc59f': 'These days the sharing of videos is very popular in social networks. Many of these social media websites such as Flickr, Facebook and YouTube allows the user to manually label their uploaded videos with textual information. However, the manually labelling for a large set of social media is still boring and error-prone. For this reason we present a algorithm for categorisation of videos in social media platforms without decoding them. The paper shows a data-driven approach which makes use of global and local features from the compressed domain and achieves a mean average precision of 0.2498 on the Blip10k dataset. In comparison with existing retrieval approaches at the MediaEval Tagging Task 2012 we will show the effectiveness and high accuracy relative to the state-of-the art solutions.',\n",
       " '53e997cbb7602d9701fbc5fa': 'Massive datasets are becoming commonplace in a wide range of domains, and mining them is recognized as a challenging problem with great potential value. Motivated by this challenge, much effort has been concentrated on developing scalable versions of machine learning algorithms. An often overlooked issue is that large datasets are rarely labeled with the outputs that we wish to learn to predict, due to the human labor required. We make the key observation that analysts can often use queries to define labels for cases, which leads to the problem of learning to predict such query-produced labels. Of course, if a dataset is available in its entirety, we can simply run the query again to compute labels. The interesting scenarios are those where, after the predictive model is trained, new data is gathered at significant incremental cost and, perhaps, over time. The challenge is to accurately predict the query-labels for the projected completion of new datasets, based only on certain cost-effective subsets, which we call bellwethers.',\n",
       " '53e997cbb7602d9701fbd2e6': 'In many combinatorial situations there is a notion of independence of a set of points. Maximal independent sets can be easily constructed by a greedy algorithm, and it is of interest to determine, for example, if they all have the same size or the same parity. Both of these questions may be formulated by weighting the points with elements of an abelian group, and asking whether all maximal independent sets have equal weight. If a set is independent precisely when its elements are pairwise independent, a graph can be used as a model. The question then becomes whether a graph, with its vertices weighted by elements of an abelian group, is well-covered, i.e., has all maximal independent sets of vertices with equal weight. This problem is known to be co-NP-complete in general. We show that whether a graph is well-covered or not depends on its local structure. Based on this, we develop an algorithm to recognize well-covered graphs. For graphs with n vertices and maximum degree $\\\\Delta$, it runs in linear time if $\\\\Delta$ is bounded by a constant, and in polynomial time if $\\\\Delta = O(\\\\root 3 \\\\of {\\\\log n})$. We mention various applications to areas including hypergraph matchings and radius k independent sets. We extend our results to the problem of determining whether a graph has a weighting which makes it well-covered.',\n",
       " '53e997cbb7602d9701fbd305': 'In 1999, after developing and installing over 170 revenue management (RM) systems for more than 70 airlines, PROS Revenue Management, Inc. had the opportunity to develop RM systems for three companies in nonairline industries. PROS research and design department designed the opportunity analysis study (OAS), a mix of OR/MS, consulting, and software development practices to determine the applicability of RM in new business situations. PROS executed OASs with the three companies. In all three cases, the OAS supported the value of RM and led to contracts for implementation of RM systems.',\n",
       " '53e997cbb7602d9701fbd30d': 'Most graph-based semi-supervised learning methods model the structure of a dataset as a single k-NN graph. Although graph construction is an important task, many existing graph-based methods build a graph from a dataset directly and naively. While the resulting k-NN graph provides relatively a good representation of the dataset,it generally produces inappropriate shortcuts on cluster boundaries. In this paper, we propose a novel approach for modeling and combining multiple graphs with different edge weights to avoid such undesirable behavior. Using the combination of those graphs, we can systematically reduce the effect of noise in conceptually similar fashion to an ensemble approach. Experimental results demonstrate that our approach improves classification accuracy on both benchmark and artificial datasets.',\n",
       " '53e997cbb7602d9701fbd34b': 'In this article, we present a novel algorithm to discover multirelational structures from social media streams. A media item such as a photograph exists as part of a meaningful interrelationship among several attributes, including time, visual content, users, and actions. Discovery of such relational structures enables us to understand the semantics of human activity and has applications in content organization, recommendation algorithms, and exploratory social network analysis. We are proposing a novel nonnegative matrix factorization framework to characterize relational structures of group photo streams. The factorization incorporates image content features and contextual information. The idea is to consider a cluster as having similar relational patterns; each cluster consists of photos relating to similar content or context. Relations represent different aspects of the photo stream data, including visual content, associated tags, photo owners, and post times. The extracted structures minimize the mutual information of the predicted joint distribution. We also introduce a relational modularity function to determine the structure cost penalty, and hence determine the number of clusters. Extensive experiments on a large Flickr dataset suggest that our approach is able to extract meaningful relational patterns from group photo streams. We evaluate the utility of the discovered structures through a tag prediction task and through a user study. Our results show that our method based on relational structures, outperforms baseline methods, including feature and tag frequency based techniques, by 35&percnt;--420&percnt;. We have conducted a qualitative user study to evaluate the benefits of our framework in exploring group photo streams. The study indicates that users found the extracted clustering results clearly represent major themes in a group; the clustering results not only reflect how users describe the group data but often lead the users to discover the evolution of the group activity.',\n",
       " '53e997cbb7602d9701fbd352': '\\n The approach presented in this paper focus on detecting data dependences induced by heap-directed pointers on loops that access\\n dynamic data structures. Knowledge about the shape of the data structure accessible from a heap-directed pointer, provides\\n critical information for disambiguating heap accesses originating from it. Our approach is based on a previously developed\\n shape analysis that maintains topological information of the connections among the different nodes (memory locations) in the\\n data structure. Basically, the novelty is that our approach carries out abstract interpretation of the statements being analyzed,\\n and let us annotate the memory locations reached by each statement with read/write information. This information will be later\\n used in order to find dependences in a very accurate dependence test which we introduce in this paper.\\n \\n ',\n",
       " '53e997cbb7602d9701fbd35a': '- The aim of this paper is to propose a novel Voice On Demand (VoD)\\narchitecture and implementation of an efficient load sharing algorithm to\\nachieve Quality of Service (QoS). This scheme reduces the transmission cost\\nfrom the Centralized Multimedia Sever (CMS) to Proxy Servers (PS) by sharing\\nthe videos among the proxy servers of the Local Proxy Servers Group [LPSG] and\\namong the neighboring LPSGs, which are interconnected in a ring fashion. This\\nresults in very low request rejection ratio, reduction in transmission time and\\ncost, reduction of load on the CMS and high QoS for the users. Simulation\\nresults indicate acceptable initial startup latency, reduced transmission cost\\nand time, load sharing among the proxy servers, among the LPSGs and between the\\nCMS and the PS.',\n",
       " '53e997cbb7602d9701fbd35c': \"As sensor networking technologies continue to develop, the notion of adding large-scale mobility into sensor networks is becoming feasible by crowd-sourcing data collection to personal mobile devices. However, tasking such networks at fine granularity becomes problematic because the sensors are heterogeneous and owned by users instead of network operators. In this paper, we present Zoom, a multi-resolution tasking framework for crowdsourced geo-spatial sensor networks. Zoom allows users to define arbitrary sensor groupings over heterogeneous, unstructured and mobile networks and assign different sensing tasks to each group. The key idea is the separation of the task information ( what task a particular sensor should perform ) from the task implementation ( code ). Zoom consists of (i) a map, an overlay on top of a geographic region, to represent both the sensor groups and the task information, and (ii) adaptive encoding of the map at multiple resolutions and region-of-interest cropping for resource-constrained devices, allowing sensors to zoom in quickly to a specific region to determine their task. Simulation of a realistic traffic application over an area of 1 sq. km with a task map of size 1.5 KB shows that more than 90% of nodes are tasked correctly. Zoom also outperforms Logical Neighborhoods, the state-of-the-art tasking protocol in task information size for similar tasks. Its encoded map size is always less than 50% of Logical Neighborhood's predicate size.\",\n",
       " '53e997cbb7602d9701fbd372': 'Service Oriented Architecture (SOA) is a compelling topic in Service-Oriented Computing (SOC) paradigm nowadays, as many requirements come from inter- and intra- enterprise service composition. However, as one of the most significant principles of service orientation, service autonomy, has not been addressed systematically. In this paper, we propose a feasible solution for service autonomy through analyzing its intrinsic characteristics. Firstly, from the service lifecycle management point of view, a three layer architecture of service autonomy is designed, based on which a service agent is built to provide core autonomous service functionalities, including automatic service discovery, proactive service monitoring, decentralized service orchestration, and just-in-time information sharing. Second, XMPP (eXtensible Messaging and Presence Protocol) is employed to construct a lightweight fabric of agents. Finally, three typical use cases of web service composition are used to validate the rationality and feasibility of the proposed solution for service autonomy.',\n",
       " '53e997cbb7602d9701fbd380': \"The paper proposes an equilibrium solution concept for dynamic games where players can communicate with one another, but cannot make contractual agreements. In such games, unlike the static problems without contracting possibilities, the cooperation between players is possible due to the fact that the realization of negotiated agreements can be enforced by suitably-defined strategies. The definition presented combines dynamic programming, the theory of bargaining and the notion of enforceable agreements to produce a class of cooperative solutions defined in the form of memory Nash equilibria satisfying the principle of optimality along the equilibrium trajectory. The choice of a particular solution in this class depends on players' expected actions in case of disagreement, and on an adopted negotiation scheme formalized in the form of a bargaining model. Possible formulations of disagreement policies and bargaining models are discussed in some detail.\",\n",
       " '53e997cbb7602d9701fbd3a1': 'We survey recent and not so recent results related to the computation of exact and approximate distances, and corresponding shortest, or almost shortest, paths in graphs. We consider many different settings and models and try to identify some remaining open problems.',\n",
       " '53e997cbb7602d9701fbd59a': 'Content adaptation, for delivering personalized information based on terminal capabilities, user preferences and access network topology, is still challenging although many standardization works have been done. These works lead to several formats for specifying user session information. This is based on content adaptation by service providers insofar as they know about terminal and access network capabilities. This paper presents a new solution based on a programmable network that inserts dynamically user session information into client/server exchanges. Different software modules associated to different formats can be deployed dynamically into the network on behalf of the service providers. The programmable node ensures transparency from the client point of view even in the case of TCP exchanges. The performance results show that our solution behaves better than a solution based on a proxy when no insertion is needed. On the other hand, the additional delay of this dynamic insertion is largely less than the average time for transmitting and processing requests.',\n",
       " '53e997cbb7602d9701fbd3a8': 'We propose a new class of descriptors which exhibits the ability to yield meaningful structural descriptions of objects. These descriptors are constructed from two types of image primitives: quadrangles and ellipses. The primitives are extracted from an image based on human cognitive psychology and model local parts of objects. Experiments reveal that these primitives densely cover objects in images. In this regard, structural information of an object can be comprehensively described by these primitives. It is found that a combination of simple spatial relationships between primitives plus a small set of geometrical attributes provide rich and accurate local structural descriptions of objects. Category level object detection of four-legged animals, bicycles, and cars images is demonstrated under scaling, moderate viewpoint variations, and background clutter. Promising results are achieved.',\n",
       " '53e997cbb7602d9701fbd3d4': \"In this paper, we propose a method for the dimensionality reduction (DR) of spectral-spatial features in hyperspectral images (HSIs), under the umbrella of multilinear algebra, i.e., the algebra of tensors. The proposed approach is a tensor extension of conventional supervised manifold-learning-based DR. In particular, we define a tensor organization scheme for representing a pixel's spectral-spatial feature and develop tensor discriminative locality alignment (TDLA) for removing redundant information for subsequent classification. The optimal solution of TDLA is obtained by alternately optimizing each mode of the input tensors. The methods are tested on three public real HSI data sets collected by hyperspectral digital imagery collection experiment, reflective optics system imaging spectrometer, and airborne visible/infrared imaging spectrometer. The classification results show significant improvements in classification accuracies while using a small number of features.\",\n",
       " '53e997cbb7602d9701fbd5f8': 'This paper introduces an automatic color design method that is driven by an importance function of the objects within a volumetric dataset. Our method allows the user to intuitively modify the object classification and the importance distribution function in the 2D rendered image. It automatically computes the transfer function, especially the color distribution, to convey the importance of the objects. In our approach, the importance of an object is represented as the attentiveness of a color. In addition, we preserve the color harmony in the rendered image in order to provide a visually pleasing result. In this paper, we propose a set of computational measurements to compute the color attentiveness and color harmony. Our color assignment algorithm supports arbitrary-dimensional transfer functions and obtains interactive frame rates. Our method involves three color spaces, namely Coloroid system, CIE LChuv, and Adobe RGB color space. It calculates the color attentiveness in CIE LChuv space, and the color harmony in Coloroid system. It, then, assigns the transfer function in a dual space of Adobe RGB space and renders the resulting image in Adobe RGB space. We conducted a detailed user study, which proves that our method successfully conveys the importance distributions. Our contribution in this paper is not only our importance driven approach, but also our computational measurements and our color assignment algorithm. © 2012 Wiley Periodicals, Inc.',\n",
       " '53e997cbb7602d9701fbd60e': 'We outline problems with the interpretation of accuracy in the presence of bias, arguing that the issue is a particularly pressing concern for RTE evaluation. Furthermore, we argue that average precision scores are unsuitable for RTE, and should not be reported. We advocate mutual information as a new evaluation measure that should be reported in addition to accuracy and confidence-weighted score.',\n",
       " '53e997cbb7602d9701fbd400': 'A search process implies an exploration of new, unvisited states. This quest to find something new tends to emphasize the\\n processes of change. However, heuristic search is different from random search because features of previous solutions are\\n preserved — even if the preservation of these features is a passive decision. A new parallel simulated annealing procedure\\n is developed that makes some active decisions on which solution features should be preserved. The improved performance of\\n this modified procedure helps demonstrate the beneficial role of common components in heuristic search.\\n ',\n",
       " '53e997cbb7602d9701fbd45b': \"We introduce a non-interactive RSA time-lock puzzle scheme whose level of difficulty can be arbitrarily chosen by artificially enlarging the public exponent. Solving a puzzle for a message m means for Bob to encrypt m with Alice's public puzzle key by repeated modular squaring. The number of squarings to perform determines the puzzle complexity. This puzzle is non-parallelizable. Thus, the solution time cannot be shortened significantly by employing many machines and it varies only slightly across modern CPUs. Alice can quickly verify the puzzle solution by decrypting the ciphertext with a regular private key operation. Our main contribution is an offline submission protocol which enables an author being currently offline to commit to his document before the deadline by continuously solving an RSA puzzle based on that document. When regaining Internet connectivity, he submits his document along with the puzzle solution which is a proof for the timely completion of the document. We have implemented a platform-independent tool performing all parts of our offline submission protocol: puzzle benchmark, issuing a time-lock RSA certificate, solving a puzzle and finally verifying the solution for a submitted document. Two other applications we propose for RSA time-lock puzzles are trial certificates from a well-known CA and a CEO disclosing the signing private key to his deputy.\",\n",
       " '53e997cbb7602d9701fbd6ca': 'Medical Entity Recognition is a crucial step towards efficient medical texts analysis. In this paper we present and compare three methods based on domain-knowledge and machine-learning techniques. We study two research directions through these approaches: (i) a first direction where noun phrases are extracted in a first step with a chunker before the final classification step and (ii) a second direction where machine learning techniques are used to identify simultaneously entities boundaries and categories. Each of the presented approaches is tested on a standard corpus of clinical texts. The obtained results show that the hybrid approach based on both machine learning and domain knowledge obtains the best performance.',\n",
       " '53e997cbb7602d9701fbd15a': 'This note is concerned with the stability analysis and controller design for linear systems involving a network of sensors and actuators, which are triggered in groups by random events. These events are modeled by two independent Markov chains. A novel stability criterion is obtained by considering transmission delays in the measurement and control signals. Based on the stability criterion, the controller gain is designed. A numerical example is given to show the effectiveness of the proposed method.',\n",
       " '53e997cbb7602d9701fbd6d6': 'The tremendous evolution in networking, communication and mobility creates greater security assurance demand than can be provided by simple security measures, such as requiring passwords to gain access to a system. Biometric technologies are being used increasingly as an effective means. The combination of biometrics and cryptography is promising although there are some issues to be addressed. The key issue of the biometric encryption is how to deal with the contradiction about the veracity of cryptography and the simulation of biometric. The shielding function is a potential tool to solve this problem. In this paper, we shall review and analyze the classic biometrics-based cryptographic key management. And then, we construct a new fingerprint-based key binding scheme by using the shielding function and the WFMT method.',\n",
       " '53e997cbb7602d9701fbd6da': 'An optimization framework for three-dimensional conformal radiation therapy is presented. In conformal therapy, beams of radiation are applied to a patient from different directions, where the aperture through which the beam is delivered from each direction is chosen to match the shape of the tumor, as viewed from that direction. Wedge filters may be used to produce a gradient in beam intensity across the aperture. Given a set of equispaced beam angles, a mixed-integer linear program can be solved to determine the most effective angles to be used in a treatment plan, the weight (exposure time) to be used for each beam, and the type and orientation of wedges to be used. Practical solution techniques for this problem are described; they include strengthening of the formulation and solution of smaller approximate problems obtained by a reduced parametrization of the treatment region. In addition, techniques for controlling the dose-volume histogram implicitly for various parts of the treatment region using hot-and cold-spot control parameters are presented. Computational results are given that show the effectiveness of the proposed approach on practical data sets.',\n",
       " '53e997cbb7602d9701fbd164': 'Energy wastage in buildings is to be minimized to reduce the carbon footprint of electricity. Wireless sensor and actor networks (WSAN) have been providing solutions for effective energy management within buildings. In this paper, we present a decisive server based context aware energy management system for smart buildings through Cyber Physical System (CPS) models. A layered architecture for building energy management is proposed to enhance scalability of the system. Heterogeneous wireless network based multiple radio gateway is proposed and implemented to make the system more adaptive to different applications catering to variable data rates. A smart room test bed is deployed in the IIT Hyderabad campus, where the decisive server collects various physical parameters through sensors, and based on the context generates wireless control messages to power electronics based actuators. Integrating context awareness into the system increases the efficiency in terms of energy savings and was observed to be significant, around 30%. The paper also presents a detailed analysis on the turnaround time required to realise the real saving after recovering investments. Applications are developed to integrate smart phones and tabloids providing web enablement to the end user. In this paper, each of the sensors and actuators in the smart room are associated with a state machine, which enables modelling of the system using Hybrid automata for future scope of applications.',\n",
       " '53e997cbb7602d9701fbd479': 'This paper introduces a refined evaluation function, called �, for the Minimum Linear Arrangement problem (MinLA). Compared with the classical evaluation function (LA), � integrates additional in- formation contained in an arrangement to distinguish arrangements with the same LA value. The main characteristics ofare analyzed and its practical usefulness is assessed within both a Steepest Descent (SD) al- gorithm and a Memetic Algorithm (MA). Experiments show that the use ofallows to boost the performance of SD and MA, leading to the improvement on some previous best known solutions.',\n",
       " '53e997cbb7602d9701fbd707': 'We study local filters for two properties of functions of the form f : {0,1}d → R: the Lipschitz property and monotonicity. A local filter with additive error a is a randomized algorithm that is given black-box access to a function f and a query point x in the domain of f. It outputs a value F(x) such that (i) the reconstructed function F(x) satisfies the property (in our case, is Lipschitz or monotone) and (ii) if the input function f satisfies the property, then for every point x in the domain (with high constant probability) the reconstructed value F(x) differs from f(x) by at most a. Local filters were introduced by Saks and Seshadhri [2010]. The relaxed definition we study is due to Bhattacharyya et al. [2012], except that we further relax it by allowing additive error. Local filters for Lipschitz and monotone functions have applications to areas such as data privacy. We show that every local filter for Lipschitz or monotone functions runs in time exponential in the dimension d, even when the filter is allowed significant additive error. Prior lower bounds (for local filters with no additive error, that is, with a=0) applied only to a more restrictive class of filters, for example, nonadaptive filters. To prove our lower bounds, we construct families of hard functions and show that lookups of a local filter on these functions are captured by a combinatorial object that we call a c-connector. Then we present a lower bound on the maximum outdegree of a c-connector and show that it implies the desired bounds on the running time of local filters. Our lower bounds, in particular, imply the same bound on the running time for a class of privacy mechanisms.',\n",
       " '53e997cbb7602d9701fbd176': 'A major challenge in nanoscience is the design of synthetic molecular devices that run autonomously and are programmable. DNA-based synthetic molecular devices have the advantage of being relatively simple to design and engineer, due to the predictable secondary structure of DNA nanostructures and the well-established biochemistry used to manipulate DNA nanostructures. We present the design of a class of DNAzyme based molecular devices that are autonomous, programmable, and further require no protein enzymes. The basic principle involved is inspired by a simple but ingenious molecular device due to Mao et al [25]. Our DNAzyme based designs include (1) a finite state automata device, DNAzyme FSA that executes finite state transitions using DNAzymes, (2) extensions to it including probabilistic automata and non-deterministic automata, (3) its application as a DNAzyme router for programmable routing of nanostructures on a 2D DNA addressable lattice, and (4) a medical-related application, DNAzyme doctor that provide transduction of nucleic acid expression: it can be programmed to respond to the underexpression or overexpression of various strands of RNA, with a response by release of an RNA.',\n",
       " '53e997cbb7602d9701fbd179': 'The approach of thinking about systems as a whole with the explicit consideration of complex interactions between its constituents is the key conceptual framework underlying systems thinking. Such an approach is particularly useful for unraveling the dynamic interactions that confounds present-day maritime systems. In this work, we propose the DIVER systems inquiry framework as a logical and platform to guide the systems thinking process in elucidating the maritime dynamics. We describe the implementation of system dynamics modeling and simulation as a particularly relevant tool within this framework. Its role as a structural modeling and analysis tool for various supply-demand problems is described. Furthermore, as a means to facilitate the systems thinking process in this framework, we propose a set of archetypical system structures that can explain some commonly observed maritime dynamics. Finally, a case study on the evolution of global container ship capacity based on DIVER is presented. © 2010 Wiley Periodicals, Inc. Syst Eng 14: © 2011 Wiley Periodicals, Inc.',\n",
       " '53e997cbb7602d9701fbd494': 'Punctured polygons are polygons with internal holes which are also polygons. The external and internal polygons are of the same type, and they are mutually as well as self-avoiding. Based on an assumption about the limiting area distribution for unpunctured polygons, we regorously analyse the effect of a finite number of punctures on the limiting area distribution in a uniform ensemble, where punctured polygons with equal perimeter have the same probability of occurrence. Our analysis leads to conjectures about the scaling behaviour of the models. We also analyse exact enumeration data. For staircase polygons with punctures of fixed size, this yields explicit expressions for the generating functions of the first few area moments. For staircase polygons with punctures of arbitrary size, a careful numerical analysis yield very accurate estimates for the area moments. Interestingly, we find that the leading correction term for each area moment is proportional to the corresponding area moment with one less puncture. We finally analyse corresponding quantities for punctured self-avoiding polygons and find agreement with the conjectured formulas to at least 3-4 significant digits.',\n",
       " '53e997cbb7602d9701fbd72e': \"The increasing amount of available digital data motivates the development of techniques for the management of the information overload which risks to actually reduce people's knowledge instead of increasing it. Research is concentrating on topics related to the problem of filtering/suggesting a subset of available information that is likely to be of interest to the user, besides this subset may vary and is often determined by the context the user is currently in. We cannot actually expect only a collaborative approach, where users manually specify the long list of preferences that might be applied to all available data, that is why in this paper we propose a preliminary methodology, described by using a realistic running example, that tries to combine the following research topics: context-awareness, data mining, and preferences. In particular, data mining is used to infer contextual preferences from the previous user's querying activity on static data and on available dynamic values coming from sensors.\",\n",
       " '53e997cbb7602d9701fbd728': 'Array data flow analysis is known to be crucial to the success of array privatization, one of the most important techniques for program parallelization. It is clear that array data flow analysis should be performed interprocedurally and symbolically, and that it often needs to handle the predicates represented by IF conditions. Unfortunately, such a powerful program analysis can be extremely time-consuming if not carefully designed. How to enhance the efficiency of thk analysis to a practical level remains an issue largely untouched to date. This paper documents our experience with building a highly efficient array data flow analyzer which is based on guarded array regions and which runs faster, by one or two orders of magnitude, than other similarly powerful tools.',\n",
       " '53e997cbb7602d9701fbd731': 'We present a method for dynamically generating Bayesian networks from knowledge bases consisting of first-order probability logic sentences. We present a subset of probability logic sufficient for representing the class of Bayesian networks with discrete-valued nodes. We impose constraints on the form of the sentences that guarantee that the knowledge base contains all the probabilistic information necessary to generate a network. We define the concept of d-separation for knowledge bases and prove that a knowledge base with independence conditions defined by d-separation is a complete specification of a probability distribution. We present a network generation algorithm that, given an inference problem in the form of a query Q and a set of evidence E, generates a network to compute P(Q|E). We prove the algorithm to be correct.',\n",
       " '53e997cbb7602d9701fbd1a3': ' Motion estimation presents a class of algorithms well-suitedto reconfigurable hardware due to their variable computationalload, highly structured array architectures, robustreduced complexity algorithms, and a motivation for lowpower implementations in portable video products. Motionestimation is the most computationally demanding part ofvideo compression algorithms and hence usually requireshardware support for real-time implementation. Howeverdedicated hardware usually requires that... ',\n",
       " '53e997cbb7602d9701fbd19a': 'BACKGROUND: Activation of naïve B lymphocytes by extracellular ligands, e.g. antigen, lipopolysaccharide (LPS) and CD40 ligand, induces a combination of common and ligand-specific phenotypic changes through complex signal transduction pathways. For example, although all three of these ligands induce proliferation, only stimulation through the B cell antigen receptor (BCR) induces apoptosis in resting splenic B cells. In order to define the common and unique biological responses to ligand stimulation, we compared the gene expression changes induced in normal primary B cells by a panel of ligands using cDNA microarrays and a statistical approach, CLASSIFI (Cluster Assignment for Biological Inference), which identifies significant co-clustering of genes with similar Gene Ontology™ annotation. RESULTS: CLASSIFI analysis revealed an overrepresentation of genes involved in ion and vesicle transport, including multiple components of the proton pump, in the BCR-specific gene cluster, suggesting that activation of antigen processing and presentation pathways is a major biological response to antigen receptor stimulation. Proton pump components that were not included in the initial microarray data set were also upregulated in response to BCR stimulation in follow up experiments. MHC Class II expression was found to be maintained specifically in response to BCR stimulation. Furthermore, ligand-specific internalization of the BCR, a first step in B cell antigen processing and presentation, was demonstrated. CONCLUSION: These observations provide experimental validation of the computational approach implemented in CLASSIFI, demonstrating that CLASSIFI-based gene expression cluster analysis is an effective data mining tool to identify biological processes that correlate with the experimental conditional variables. Furthermore, this analysis has identified at least thirty-eight candidate components of the B cell antigen processing and presentation pathway and sets the stage for future studies focused on a better understanding of the components involved in and unique to B cell antigen processing and presentation.',\n",
       " '53e997cbb7602d9701fbd78f': 'A notation for expressing the control algorithms (subgoaling strategies) of natural deduction theorem provers is presented.\\n The language provides tools for building widely known, fundamental theorem proving strategies and is independent of the problem\\n area and inference rule system chosen, facilitating formulation of high level algorithms that can be compared, analyzed, and\\n even ported across theorem proving systems. The notation is a simplification and generalization of the tactic language of\\n Edinburgh LCF. Examples using a natural deduction system for propositional logic are given.\\n ',\n",
       " '53e997cbb7602d9701fbd78d': 'At the Laboratory for International Fuzzy Engineering Research in Japan (LIFE), we are now developing FINEST (Fuzzy Inference Environment Software with Tuning). The special features are (1) improved generalized modus ponens, (2) mechanism which can tune the inference method as well as fuzzy predicates and (3) software environment for debugging and tuning. In this paper, we give an outline of the software, and describe an important concept, a deep combination of the fuzzy inference and the neural network in FINEST, which enables FINEST to tune the inference method itself. FINEST is now being used as a tool for quantification of the meaning of natural language sentences as well as a tool for fuzzy modelling and fuzzy control.',\n",
       " '53e997cbb7602d9701fbd1a0': 'This paper describes our Experience-centered Design (ECD) inquiry into the current and potential role of digital photography to support interpersonal communication and expression in a class at a mixed special education needs school. Presented as a case study, we describe five classroom-based Creative Photography workshops that engaged pupils with a broad range of complex special needs, along with classroom staff. We further describe how, from these workshops, we generated a set of qualitative considerations for the design of digital photographic tools to support interpersonal communication and expression in this setting. Additionally, we present the preliminary evaluation of a photo-sorting tool that we developed in response. Our case study demonstrates how an ECD approach can guide an interaction design process in a special education needs setting, supporting interaction designers in understanding and responding pragmatically to the complex and dynamic interactions at play between the stakeholders.',\n",
       " '53e997cbb7602d9701fbd1b7': 'In this paper, we discuss the issue of implementing signal processing algorithms on a standard PC. As an example, we present a software implementation of a wavelet-based image processing algorithm. The implementation has been optimized for the Pentium III processor. Based on a careful analysis of the problem, we show that the implementation performance is limited by the memory bandwidth. This implies that more computational complexity could be allowed in the algorithm without decreasing the performance. In addition, we describe how to optimize the implementation performance at the C language level.',\n",
       " '53e997cbb7602d9701fbd802': 'A line code called DmB1M that is suitable for a very high-speed, optical, digital-transmission system is described. This code possesses a good balance of marks, spaces, and jitter-suppression capability, as well as a simple in-service error-monitoring function. A fundamental analysis of the DmB1M code is presented, along with experimental results at 1.8 Gb/s. Advantages of this code in parallel signal processing, such as error checking to avoid error propagation, and applications of this code to phase-shift keyed (PSK) systems are also discussed. >',\n",
       " '53e997cbb7602d9701fbd7f2': 'Considering both, the ecological and economical aspects in building construction engineering, is of high importance for a balanced and efficient building design. For high competitiveness on the markets, the need for novel efficiency metrics and automated optimization of the building plan arises. We introduce an efficiency measure for balancing the trade-off between construction cost and the heating demand as energy efficiency aspect. Thereby the building physics are precisely modeled and all possible variations of the particular material choice can be evaluated via simulation. By considering all possible variations of the particular plan, a large multi-dimensional search space can be defined, allowing search for the optimal design utilizing heuristic methods. Exploitation of the search space allows the quantitative assessment of plans with respect to the theoretical optimum and generally the qualitative comparison of different construction settings and different material choice for development of current best standard practice in building construction engineering.',\n",
       " '53e997cbb7602d9701fbd516': \"This research investigates the influence of computerized search engines on consumer decision making in the electronic commerce environment. The results indicate that by providing well-designed decision aids to consumers, it is possible to significantly increase consumer confidence, satisfaction, and decision quality. Consumers who have access to query-based decision aids perceive increased cost savings and lower cognitive decision effort associated with the purchase decision. The future challenge in developing consumer-oriented computerized decision aids does not reside in technological advances, but rather in developing systems that are useful and appealing to the intended consumer. This is necessary to avoid consumer perceptions of non-utility, and ultimately non-use of the computerized decision aids. The challenge for marketing managers is to provide consumers with information systems that change over time such that they fulfill the consumers' short-term needs without sacrificing the consumers' long-term interests.\",\n",
       " '53e997cbb7602d9701fbd864': \"We study self-organization of receptive fields (RFs) of cortical minicolumns. Input driven self-organization is induced by Hebbian synaptic plasticity of afferent fibers to model minicolumns based on spiking neurons and background oscillations. If input in the form of spike patterns is presented during learning, the RFs of minicolumns hierarchically specialize to increasingly small groups of similar RFs in a series of nested group subdivisions. In a number of experiments we show that the system finds clusters of similar spike patterns, that it is capable of evenly cover the input space if the input is continuously distributed, and that it extracts basic features from input consisting of superpositions of spike patterns. With a continuous version of the bars test we, furthermore, demonstrate the system's ability to evenly cover the space of extracted basic input features. The hierarchical nature and its flexibility with respect to input distinguishes the presented type of self-organization from others including similar but non-hierarchical self-organization as discussed in [Lucke J., & von der Malsburg, C. (2004). Rapid processing and unsupervised learning in a model of the cortical macrocolumn. Neural Computation 16, 501-533]. The capabilities of the presented system match crucial properties of the plasticity of cortical RFs and we suggest it as a model for their hierarchical formation.\",\n",
       " '53e997cbb7602d9701fbd877': 'Most commercial and academic floating point librariesfor FPGAs provide only a small fraction of all possiblefloating point units. In contrast, the floating point unit generationapproach outlined in this paper allows for the creationof a vast collection of floating point units with differingthroughput, latency, and area characteristics. Givenperformance requirements, our generation tool automaticallychooses the proper implementation algorithm and architectureto create a compliant floating point unit. Ourapproach is fully integrated into standard C++ using ASC,a stream compiler for FPGAs, and the PAM-Blox II modulegeneration environment. The floating point units created byour approach exhibit a factor of two latency improvementversus commercial FPGA floating point units, while consumingonly half of the FPGA logic area.',\n",
       " '53e997cbb7602d9701fbd262': 'A blind estimation of MIMO channels using genetic algorithm was proposed for the frequency selective channel. By exploiting the output statistics, an objective function that coming from blind estimation of MIMO channels based on the subspace could be obtained. Then the fitness function based on the objective function is proposed for optimization by a genetic algorithm-based approach. It has been proved that this new method could be used in channel estimation. Compared with the existing blind estimation method, this method in this paper brings forward a new point of view to solve the channel estimation. The simulation results show the effectiveness of the proposed method.',\n",
       " '53e997cbb7602d9701fbd8a2': 'Medical literature has been an important information source for clinical professionals. As the body of medical literature expands rapidly, keeping this knowledge up-to-date becomes a challenge for medical professionals. One question is that for a given disease how can we find the most influential treatments currently available from online medical publications? In this paper we propose MedRank, a new network-based algorithm that ranks heterogeneous objects in a medical information network. The network is extracted from MEDLINE, a large collection of semi-structured medical literature. Different types of objects such as journal articles, pathological symptoms, diseases, clinical trials, treatments, authors, and journals are linked together through their relationships. The experimental results are compared with the expert rankings collected from doctors and two baseline methods, namely degree centrality and NetClus. The evaluation shows that our algorithm is effective and efficient. The success of categorized entity ranking in medical literature domain suggests a new methodology and a potential success in ranking semi-structured data in other domains.',\n",
       " '53e997cbb7602d9701fbd8a4': 'We address the semantic gapproblem in behavioral monitoring by using hierarchical behavior graphs to infer high-level behaviors from myriad low-level events. Our experimental system traces the execution of a process, performing data-flow analysis to identify meaningful actions such as \"proxying\", \"keystroke logging\", \"data leaking\", and \"downloading and executing a program\" from complex combinations of rudimentary system calls. To preemptively address evasive malware behavior, our specifications are carefully crafted to detect alternative sequences of events that achieve the same high-level goal. We tested eleven benign programs, variants from seven malicious bot families, four trojans, and three mass-mailing worms and found that we were able to thoroughly identify high-level behaviors across this diverse code base. Moreover, we effectively distinguished malicious execution of high-level behaviors from benign by identifying remotely-initiated actions.',\n",
       " '53e997cbb7602d9701fbd28f': 'We present a novel precoding or modulation scheme (matrix modu- lation) that allows parallel transmission of several data signals over an unknown multiple-input multiple-output (MIMO) channel. We first present a theorem on unique signal demodulation and an ef- ficient iterative demodulation algorithm for transmission over an unknown instantaneous-mixture channel. We then generalize our results to an unknown MIMO channel with memory.',\n",
       " '53e997cbb7602d9701fbd299': 'Community-based knowledge forums, such as Wikipedia, are susceptible to vandalism, i.e., ill-intentioned contributions that are detrimental to the quality of collective intelligence. Most previous work to date relies on shallow lexico-syntactic patterns and metadata to automatically detect vandalism in Wikipedia. In this paper, we explore more linguistically motivated approaches to vandalism detection. In particular, we hypothesize that textual vandalism constitutes a unique genre where a group of people share a similar linguistic behavior. Experimental results suggest that (1) statistical models give evidence to unique language styles in vandalism, and that (2) deep syntactic patterns based on probabilistic context free grammars (PCFG) discriminate vandalism more effectively than shallow lexico-syntactic patterns based on n-grams.',\n",
       " '53e997cbb7602d9701fbd8cc': 'A small portable network forensic evidence collection device is presented which is built using inexpensive embedded hardware and open source software. The device offers several modes of operation for different live network evidence collection scenarios involving single network nodes. This includes the use of promiscuous packet capturing to enhance evidence collection from remote network sources, such as websites or other remote services. It operates at the link layer allowing the device to be transparently inserted inline between a network node and the rest of a network. It is simple to deploy, requiring no reconfiguration of the node or surrounding network infrastructure. The device can be preconfigured in the forensics lab, and deployment delegated to staff not specifically trained in forensics. Details of the architecture, construction and operation are described. Special attention is given to information security aspects of live network evidence collection.',\n",
       " '53e997cbb7602d9701fbd5d4': 'When the code length is moderate, non-binary low-density parity-check (NB-LDPC) codes can achieve better error correcting performance than their binary counterparts at the expense of higher decoding complexity. The check node processing is a major bottleneck of NB-LDPC decoding. This paper proposes novel schemes for both the Min-max and the simplified Min-sum check node processing by making use of the cyclical-shift property of the power representation of finite field elements. Compared to previous designs based on the Min-max algorithm with forward-backward scheme, the proposed check node units (CNUs) do not need the complex switching network. Moreover, the multiplications of the parity check matrix entries are efficiently incorporated. For a Min-max NB-LDPC decoder over GF(32), the proposed scheme reduces the CNU area by at least 32 %, and leads to higher clock frequency. Compared to the prior simplified Min-sum based design, the proposed CNU is more regular, and can achieve good throughput-area tradeoff.',\n",
       " '53e997cbb7602d9701fbd8d2': 'Argumentation-based negotiation has gained increasing prominence in the multi-agent field over the last years. There is currently a long literature on the use of argumentation in negotiation and especially the modeling of negotiation protocols or decision making mechanisms. However the study of strategic issues that define the behavior of an agent during the negotiation has been largely neglected. This work fills this gap by providing, profile, behavior and time constraints based tactics that can be combined together to implement complex strategies similar to those studied in game-theoretic negotiation. These different tactics lead to different types of concessions. An experimental evaluation shows how tactics and concessions may influence the negotiation length and outcome, under the assumptions of deadlines and the availability of information on the opponent.',\n",
       " '53e997ccb7602d9701fbd8e3': 'Pulsed electric field permeabilization of living cell membranes forms the basis for widely used biotechnology protocols and an increasing number of therapeutic applications. Experimental observations of artificial membranes and whole cells and molecular and analytical models provide evidence that a membrane-spanning, hydrophilic, conductive pore can form in nanoseconds. An external electric field lowers the energy barrier for this stochastic process, reducing the mean time for pore formation and increasing the pore areal density. Molecular dynamic simulations reveal the key role played by interfacial water in electropermeabilization. These model systems, validated in the laboratory, are deepening our understanding of the factors governing pore initiation, construction, and lifetime, knowledge that will translate to enhanced utilization of this method in biomedicine and bioengineering.',\n",
       " '53e997cbb7602d9701fbd2b1': \"To construct a conceptual model of a device, the user must conceptualize the device's representation of the task domain. This knowledge can be represented by three components: a device-based problem space, which specifies the ontology of the device in terms of the objects that can be manipulated and their interrelations, plus the operators that perform the manipulations; a goal space, which represents the objects in terms of which user's goals are expressed; and a semantic mapping, which determines how goal space objects are represented in the device space. The yoked state space (YSS) model allows an important distinction concerning the mental representation of procedures. If a step in a procedure specifies a transformation of the user's device space, then it has an autonomous meaning for the user, independent of its role in the sequence or method. The device space provides a figurative account of the operator. However, some operators do not affect the minimal device space, and their only meaning for the user derives from their role in a method: The method affords an operational account of the operator. Figurative accounts can be constructed from operational accounts only by elaborating the device space with new concepts. The YSS is illustrated through a simple description of a device model for a cut-and-paste text editor. Three experiments addressed the claims of this model. The first experiment used a sorting paradigm to show that users do acquire the novel device space concept of a string of adjacent characters (including space and return). The second and third experiments asked novices to make inferences about text editor behavior on the basis of simple demonstrations. They showed that (a) the availability of the string concept is critically dependent on the details of interface design, (b) figurative accounts of the copy operation afford more efficient methods and may be promoted by appropriate names for procedure steps, and (c) a conceptual model may transfer from one device to another. Together, the three experiments supported the YSS hypothsis.\",\n",
       " '53e997cbb7602d9701fbd2b6': ' Probabilistic AI planning methods that minimizeexpected execution cost have a neutralattitude towards risk. We demonstratehow one can transform planning problemsfor risk-sensitive agents into equivalent onesfor risk-neutral agents provided that exponentialutility functions are used. The transformedplanning problems can then be solvedwith these existing AI planning methods. Todemonstrate our ideas, we use a probabilisticplanning framework (&quot;probabilistic decisiongraphs&quot;)... ',\n",
       " '53e997cbb7602d9701fbd61f': 'Companies have rushed to set up presences on the World Wide Web. In most instances, they are advertising their products and services, and in some instances, are actually delivering them over the Internet. Many of these companies are small, and there is at least anecdotal evidence that small companies are at the forefront of commercial use of the Web. In this paper, we consider various perspectives for assessing the role and use of the Web. We argue that most small companies will view the Web as an electronic marketplace. We consider the attraction of the Web as a marketplace for small companies, and the key strategic drivers that are moving small companies to join the Web. We also suggest a number of inhibitors that will make the Web less attractive for small companies in the future. The result is a model that can be used to understand the position of small companies with respect to the Web. Despite the youth of this new medium, some marketing paradigms common to many entrants are emerging. We discuss these paradigms and give examples of their use. We briefly review some successful cases.',\n",
       " '53e997ccb7602d9701fbd90b': 'We describe a two-year study of a rich secondary science curriculum that was codesigned in close partnership with teachers, technology specialists and even school administrators. The goal of the research was to provide empirical support for a recent model of learning and instruction that blends the two perspectives of knowledge communities and scaffolded inquiry. A design-oriented method was employed, where the first iteration of the curriculum was evaluated in terms of its fit to the model, as well as its impact on student learning. Based on a set of design recommendations, a much more substantive curriculum was developed for the second iteration, leading to rich measures of student collaboration and deep understanding of the targeted science concepts. This paper describes our co-design process, which allowed teachers to lead the curriculum design and classroom enactment while researchers contributed design guidelines according to the theoretical model.',\n",
       " '53e997cbb7602d9701fbd65b': 'In this article, we consider the performance of energy detection in composite multipath/shadowing fading environment. We also investigate the mitigation of the effect of these fading components on detection performance with the use of diversity techniques. Single channel, maximum ratio combining (MRC), square law combining (SLC), and square-law selection (SLS) diversities are analyzed using the rapidly convergent canonical series representation of Marcum Q-function with derivatives of moment generating function (MGF) of signal-to-noise ratio (SNR) of composite channel. On the other hand, the selection diversity combining (SDC) is analyzed using single integral of cumulative distribution function (CDF) of SNR. In all cases, we model the composite channel fading using the G-distribution which has been shown to be more accurate in representing the Suzuki and Nakagami-Lognormal distributions than K and KG and in closed formed compared to single integral expression for Rice-Lognormal distribution. Using this framework, we found out that the performance of energy detection does not degrade significantly at low and moderate shadowing conditions and that diversity detection greatly mitigates the effect of shadowing on detection performance with MRC giving the best performance, followed by SLC and then SDC or SLS depending on the average channel SNR, number of samples and level of shadowing. To the best of our knowledge, such a simple framework and resulting novel observations have never been reported before.',\n",
       " '53e997cbb7602d9701fbd68d': 'A model-based approach for minimization of test sets for human-computer systems is introduced. Test cases are efficiently generated and selected to cover both the behavioral model and the complementary fault model of the system under test (SUT). Results known from state-based conformance testing and graph theory are used and extended to construct algorithms for minimizing the test sets.',\n",
       " '53e997cbb7602d9701fbd6e7': ' . This article describes an architecture for the recognition ofthree-dimensional objects on the basis of viewer centred representationsand temporal associations. Considering evidence from psychophysics,neurophysiology, as well as computer science we have decided to usea viewer centred approach for the representation of three-dimensionalobjects. Even though this concept quite naturally suggests utilizing thetemporal order of the views for learning and recognition, this aspect is... ',\n",
       " '53e997ccb7602d9701fbd938': \"The well-known game of Prisoner's Dilemma, which reflects a basic situation in which one must decide whether or not to cooperate with a competitor, is systematically solved using a fuzzy approach to modeling trust. When involved in a dispute, two or more parties need to make decisions with fully or partially conflicting objectives. In situations where reaching a more favorable outcome depends upon cooperation and trust between the two conflicting parties, some of the mental and subjective attitudes of the decision makers must be considered. While the decision to cooperate with others bears some risks due to uncertainty and loss of control, not cooperating means giving up potential benefits. In practice, decisions must be made under risk, uncertainty, and incomplete or fuzzy information. Because it is able to work well with vague, ambiguous, imprecise, noisy or missing information, the fuzzy approach is effective for modeling such multicriteria conflicting situations. The fuzzy procedure is used to take into account some of the subjective attitudes of the decision makers, especially with respect to trust, that are difficult to model using game theory.\",\n",
       " '53e997cbb7602d9701fbd76b': 'This paper presents a component model for designing and implementing flexible software components in Java. Our model defines a mapping of how the fundamental concepts of component-based development (CBD) should be implemented using the object-oriented (OO) constructs, available in the Java programming language. The benefit of this mapping is to shorten the distance between a component-based software architecture and its implementation, enhancing the reusability, adaptability and maintainability of component-based software systems.\\n\\n',\n",
       " '53e997cbb7602d9701fbd778': 'Approximation algorithms embedding hypergraphs in a cycle so as to minimize the maximum congestion are presented. Our algorithms generate an embedding by transforming the problem into another problem that can be solved in polynomial time. One transforms it to a Linear Programming (LP) problem, and the other one (LP-Free) to the problem of embedding a graph in a cycle. Both algorithms generate an embedding with congestion at most twice of that in an optimal solution, and we give problem instances for which the solutions generated by both of these algorithms is about twice of optimal. Our problem has applications in CAD and parallel computation.',\n",
       " '53e997cbb7602d9701fbd781': 'The human vision is usually considered a multiscale, hierarchical knowledge extraction system. Inspired by this fact, multiscale techniques for computer vision perform a sequential analysis, driven by different interpretations of the concept of scale. In the case of edge detection, the scale usually relates to the size of the region where the intensity changes are measured or to the size of the regularization filter applied before edge extraction. Multiscale edge detection methods constitute an effort to combine the spatial accuracy of fine-scale methods with the ability to deal with spurious responses inherent to coarse-scale methods. In this work we introduce a multiscale method for edge detection based on increasing Gaussian smoothing, the Sobel operators and coarse-to-fine edge tracking. We include visual examples and quantitative evaluations illustrating the benefits of our proposal.',\n",
       " '53e997cbb7602d9701fbd7bf': 'In this paper we present an approach to approximate reachability computation for nonlinear continuous systems. Rather than studying a complex nonlinear system x = g(x), we study an approximating system x = f(x) which is easier to handle. The class of approximating systems we consider in this paper is piecewise linear, obtained by interpolating g over a mesh. In order to be conservative, we add a bounded input in the approximating system to account for the interpolation error. We thus develop a reachability method for systems with input, based on the relation between such systems and the corresponding autonomous systems in terms of reachable sets. This method is then extended to the approximate piecewise linear systems arising in our construction. The final result is a reachability algorithm for nonlinear continuous systems which allows to compute conservative approximations with as great degree of accuracy as desired, and more importantly, it has good convergence rate. If g is a C2 function, our method is of order 2. Furthermore, the method can be straightforwardly extended to hybrid systems.',\n",
       " '53e997cbb7602d9701fbd7cb': ' . As research progresses in distributed robotic systems, more and more aspects of multi-robot systems are being explored. This article surveys the current state of the art in distributed mobile robot systems. Our focus is principally on research that has been demonstrated in physical robot implementations. We have identi ed eight primary research topics within multi-robot systems | biological inspirations, communication, architectures, localization/mapping/exploration, object transport and... ',\n",
       " '53e997cbb7602d9701fbd7cc': 'Distributed AI systems are intended to fill the gap between classical AI and distributed computer science. Such networks of different problem solvers are required for naturally distributed problems, and for tasks which exhaust the resource of an individual node. To guarantee a certain degree of consistency in a distributed AI system, it is necessary to inspect the beliefs of both single nodes and the whole net. This task is performed by Distributed Truth Maintenance Systems. Based on classical TMS theories, distributed truth maintenance extends the conventional case to incorporate reason maintenance in DAI scenarios.',\n",
       " '53e997cbb7602d9701fbd7d4': 'Concerning industrial automation, the management of abnormal situations becomes more important everyday. The ability to detect, isolate, and handle abnormal situations in industrial installations, could save huge amounts of money which is normally invested in reparations and/or wasted because of unjustified stoppage of processing plants. In this work, a system for the management of abnormal situations in an artificially gas-lifted well based on agents Abnormal Situations Management System (ASMS) is developed, which is part of the architecture of the industrial automation based on multi-agents systems (SADIA) proposed in Bravo, Aguilar, and Rivas (2004). This agent is based on the intelligent distributed control system based on agents (IDCSBA) reference model proposed in Aguilar, Cerrada, Mousalli, Rivas, and Hidrobo (2005). The MASINA methodology (Aguilar, Hidrobo, and Cerrada 2007) is used in matters of analysis, design, and implementation.',\n",
       " '53e997cbb7602d9701fbd82a': 'The problem of scheduling jobs to minimize total weighted tardiness in flowshops, with the possibility of evolving into hybrid flowshops in the future, is investigated in this paper. As this research is guided by a real problem in industry, the flowshop considered has considerable flexibility, which stimulated the development of an innovative methodology for this research. Each stage of the flowshop currently has one or several identical machines. However, the manufacturing company is planning to introduce additional machines with different capabilities in different stages in the near future. Thus, the algorithm proposed and developed for the problem is not only capable of solving the current flow line configuration but also the potential new configurations that may result in the future. A meta-heuristic search algorithm based on tabu search is developed to solve this NP-hard, industry-guided problem. Six different initial solution finding mechanisms are proposed. A carefully planned nested split-plot design is performed to test the significance of different factors and their impact on the performance of the different algorithms. To the best of our knowledge, this research is the first of its kind that attempts to solve an industry-guided problem with the concern for future developments.',\n",
       " '53e997cbb7602d9701fbd831': 'A call for user engagement cannot be scheduled at a specific point in the software development process. Valuable user input comes in waves throughout the project and developers would do best to listen and learn.',\n",
       " '53e997cbb7602d9701fbd83d': 'The paper proposes to combine stochastic frontier models and linear programming methods by using DEA measures as priors of efficiency in the stochastic frontier model. These prior measures are revised to obtain posterior measures using Bayes’ theorem. Monte Carlo methods are developed to perform empirical Bayes inference in the new model. The methods are organized around Gibbs sampling with data augmentation. The new techniques are illustrated in the context of efficiency measurement in US airlines.',\n",
       " '53e997ccb7602d9701fbdb29': \"Computational Linguistics and Logic Programming have strong connections, but the former uses concepts that are absent from the most familiar implementations of the latter. We advocate that a Logic Programming language need not feature the Computational Linguistics concepts exactly, it must only provide a logical way of dealing with them. We focus on the manipulation of higher-order terms and the logical handling of context, and we show that the advanced features of Prolog II and lambda Prolog are useful for dealing with these concepts. Higher-order terms are native in lambda Prolog, and Prolog II's infinite trees provide a handy data-structure for manipulating them. The formula language of lambda Prolog can be transposed in the Logic Grammar realm to allow for a logical handling of context.\",\n",
       " '53e997ccb7602d9701fbd9b4': 'Ideal video fingerprinting should be robust to various practical distortions. Conventional fingerprinting mainly copes with natural distortions (brightness change, resolution reduction, etc.), while always gives poor performance in case of text insertion. One alterative way is to apply a weighting scheme based on the probability of text insertion for feature similarity calculation. However, the weights must be learned with labeled samples. In this paper, we propose a method that first addresses valid regions where the saliency values keep consistent between the query and original frames, namely saliency-consistent regions. Other regions, probably the inserted ones, are discarded. Then a DCT-based hamming distance is calculated on those saliency-consistent regions. Besides, the saliency-based distance is also considered and a further weighted linear distance is evaluated. The proposed algorithm is tested on the MPEG-7 video fingerprint dataset, achieving a false rate of 0.7% in case of text insertion and 0.32% in average for other 8 distortions.',\n",
       " '53e997ccb7602d9701fbdb41': 'Cutting planes for mixed integer problems (MIP) are nowa- days an integral part of all general purpose software to solve MIP. The most prominent,and computationally significant, class of general cutting planes are Gomory mixed integer cuts (GMI). However finding other classes of general cuts for MIP that work well in practice has been elu- sive. Recent advances on the understanding of valid inequalities derived from the infinite relaxation introduced by Gomory and Johnson for mixed integer problems, has opened a new possibility of finding such an exten- sion. In this paper, we investigate the computational impact of using a subclass of minimal valid inequalities from the infinite relaxation, using different number of tableau rows simultaneously, based on a simple sepa- ration procedure. We test these ideas on a set of MIPs, including MIPLIB 3.0 and MIPLIB 2003, and show that they can improve MIP performance even when compared against commercial software performance.',\n",
       " '53e997ccb7602d9701fbdb45': 'Suvorova, S., and Schroeder, J., Automated Target Recognition Using the Karhunen–Loéve Transform with Invariance, Digital Signal Processing12 (2002) 295–306',\n",
       " '53e997cbb7602d9701fbd8b2': 'For many engineering tasks, constraints are a useful knowledge representation formalism to specify functional relationships. In this paper, we describe a programming language based on constraints which, unlike former approaches, enables us to handle sets of possible values (rather than single values) and to filter them until consistent subsets are obtained that satisfy the constraints.As, e.g., in technical systems, the constraint relations are of different complexity, structure, and applicability, several ways to describe a relation are provided. Different control strategies are available, computing local consistency as well as globally consistent solutions. It is sketched how to realize a compiler for the language, which optimizes constraint descriptions during definition time. As a result, combinatorial explosion can be reduced, depending on the number of variables used in a constraint description.CONSAT is used in several applications, for example, in a system for process diagnosis [Voss 1988], where constraints are applied to maintain functional relationships in physical units. For this purpose, CONSAT was incorporated into the hybrid knowledge representation system BABYLON in order to use constraints together with rules, frames, and Prolog [Guesgen et al. 1987].',\n",
       " '53e997ccb7602d9701fbdb50': 'In this paper, we exploit a Chinese machine-readable dictionary to extract the conceptual knowledge, i.e. the <attribute, value> pairs involving in hypernym, (artificiality) material, (artificiality) function and (medicine) usage from the corresponding definitions of nominal entries. Our method focuses on (1) constructing the extraction patterns and (2) the statistical decision for applying these patterns. Therefore our work is designed to be a new three-step procedure. Firstly, annotate the definitions of a number of nominal entries that are used as training samples of these four attributes and contextual linguistic features; secondly, design different patterns for extracting such conceptual knowledge, and learn the applicability of the patterns by a Maximum Entropy (ME) classifier to decide whether a pattern can be used in current context or not; at last, apply these patterns to the remaining nominal entries of the dictionary, and we achieve relatively satisfying results.',\n",
       " '53e997ccb7602d9701fbd9c6': 'We present a local algorithm for finding dense subgraphs of bipartite graphs, according to the definition of density proposed by Kannan and Vinay. Our algorithm takes as input a bipartite graph with a specified starting vertex, and attempts to find a dense subgraph near that vertex. We prove that for any subgraph S with k vertices and density �, there are a significant number of starting vertices within S for which our algorithm produces a subgraph Swith density (�/log n) on at most O(�k2) ver- tices, whereis the maximum degree. The running time of the algorithm is O(�k2), independent of the number of vertices in the graph.',\n",
       " '53e997ccb7602d9701fbd9d5': 'In this note, we consider transitive permutation groups of degree 2p, where p is an odd prime, that admit blocks of imprimitivity of size 2 but no blocks of imprimitivity of size p. Primitive permutation groups of degree twice a prime were flrst considered by Wielandt (6), while more recently, imprimitive permutation groups of degree twice a prime were studied by Lefµevre (2), Marusic (3), and Marusic and Potocnik (4). In this note we present a new result on imprimitive permutation groups of degree twice a prime that serves as a crucial tool in the classiflcation of homogeneously almost self-complementary graphs of order four times a prime in (5).',\n",
       " '53e997ccb7602d9701fbd9d6': 'XML has been widely applied in kinds of areas and applications as a data exchange format. Recently, plenty of of languages based on XML emerge. There are many constraints specified by natural language in these XML-based language specifications to regulate the rules that are supposed to be obeyed by designers or programmers. To specify those constraints precisely, we present a relational calculus to capture them. Meanwhile, by means of the constraint solvers (SAT or SMT solver), we can decide that whether a XML-based document satisfies the constraints specified in its corresponding specification.',\n",
       " '53e997ccb7602d9701fbd9fc': 'Traffic congestion in urban areas is posing many challenges, and traffic flow model provides accurate traffic status estimation and prediction can be beneficial for congestion management. With the limitation of infrastructure, probe data from individual vehicles is an attractive alternative to inductive loop detectors as a mean to collect traffic data for traffic flow modelling. This paper investigates the optimal deployment strategy of probe vehicles. Data assimilation technique, Newtonian relaxation method, is used to incorporate probe data into macroscopic traffic flow model, and synthetic traffic is used to study the optimization problem. The tradeoff between the quality of traffic density estimation and operation cost of probing are investigating using multi-objective genetic algorithm. The results indicates that it is possible to decrease probe data for congested traffic with negligible degradation on the quality of traffic status estimation.',\n",
       " '53e997ccb7602d9701fbdba6': 'Users hesitate to submit negative feedback in reputation systems due to the fear of retaliation from the recipient user. A privacy preserving reputation protocol protects users by hiding their individual feedback and revealing only the reputation score. We present a privacy preserving reputation protocol for the malicious adversarial model. The malicious users in this model actively attempt to learn the private feedback values of honest users as well as to disrupt the protocol. Our protocol does not require centralized entities, trusted third parties, or specialized platforms, such as anonymous networks and trusted hardware. Moreover, our protocol is efficient. It requires an exchange of $O(n+\\\\\\\\log~N)$ messages, where $n$ and $N$ are the number of users in the protocol and the environment, respectively.',\n",
       " '53e997ccb7602d9701fbdbbe': 'The contribution of this paper is that illustrates the use of funneling actions in combination with local deictic reference frames for forming consistent and useful large scale maps These maps do not rely on any geodetic sensors Indications for the feasibility of such representations in humans, and other species, can be found in studies of spatial cognition However, such implementations or applications in robotics have not been illustrated until now.',\n",
       " '53e997ccb7602d9701fbda3a': 'In this paper we present a novel method of fusing of the sequences of images obtained from multimodal surveillance cameras and subject to distortions typical for visual sensor networks environment. The proposed fusion method uses the structural similarity measure (SSIM) to measure a level of noise in regions of a received image in order to optimize the selection of regions in the fused image. The region-based image fusion algorithm using the dual-tree complex wavelet transform (DT-CWT) is used to fuse the selected regions. The performance of the proposed method was extensively tested for a number of multimodal surveillance image sequences and proposed method outperformed the state-of-the-art algorithms, increasing significantly the quality of the fused image, both visually and in terms of the Petrovic image fusion metric.',\n",
       " '53e997ccb7602d9701fbdbd3': 'This paper proposes an original set-membership approach for loop detection of mobile robots in the situation where proprioceptive sensors only are available. To detect loops, the new concepts of thet-plane (which is a two dimensional space with time coordinates) are introduced. Intervals of functions (or tubes) are then used to represent uncertain trajectories and tests are provided in order to eliminate parts of the t-plane that do not correspond to any loop. An experiment with an actual underwater robot is proposed in order to illustrate the principle and the efficiency of the approach.',\n",
       " '53e997ccb7602d9701fbdbd5': 'In this paper, by using the concept of (A,@h)-accretive mappings and the new resolvent operator technique associated with (A,@h)-accretive mappings, we introduce and study a system of general mixed quasivariational inclusions involving (A,@h)-accretive mappings in Banach spaces, and construct a new perturbed iterative algorithm with mixed errors for this system of nonlinear (A,@h)-accretive variational inclusions in q-uniformly smooth Banach spaces. Our results improve and generalize the corresponding results of recent works.',\n",
       " '53e997ccb7602d9701fbdbdc': 'From the point of view of a price-taking hydropower producer participating in the day-ahead power market, market prices are highly uncertain. The present paper provides a model for determining optimal bidding strategies taking this uncertainty into account. In particular, market price scenarios are generated and a stochastic mixed-integer linear programming model that involves both hydropower production and physical trading aspects is developed. The idea is to explore the effects of including uncertainty explicitly into optimization by comparing the stochastic approach to a deterministic approach. The model is illustrated with data from a Norwegian hydropower producer and the Nordic power market at Nord Pool.',\n",
       " '53e997ccb7602d9701fbda52': \"Abstract--We present a MAC-layer, soft real-time packet scheduling algorithm called UPA. UPA considers a message model where message packets have end-to-end timeliness requirements that are specified using Jensen's Time-Utility Functions (TUFs). The algorithm seeks to maximize system-wide, aggregate packet utility. Since this scheduling problem is NP-hard, UPA heuristically computes schedules with a quadratic worst-case cost, faster than the previously best CMA algorithm. Our simulation studies show that UPA performs the same as or significantly better than CMA for a broad set of TUFs. Furthermore, we implement UPA and prototype a TUF-driven switched Ethernet system. The performance measurements of UPA from the implementation reveal its strong effectiveness. Finally, we derive timeliness feasibility conditions of TUF-driven switched Ethernet systems that use the UPA algorithm.\",\n",
       " '53e997ccb7602d9701fbda64': 'A functional computation involves substituting for function applications in an expression until that expression is reduced\\n to normal form. The views of computations presented by the sequence- and state-oriented debugging tools of imperative systems\\n are inappropriate for use with functional computations. New debugging tools are needed to aid the development of functional\\n programs, especially in the context of lazy evaluation.\\n \\n After surveying previously reported debugging tools, we discuss a new debugging tool. Its implementation involves changing\\n the reduction rules of the machine. The new reduction rules are applied to an interrupted computation to give a snapshot of\\n that computation in source-level terms.\\n \\n \\n \\n We have implemented tools to produce snapshots for eager SECD and lazy combinator reduction machines. Example snapshots are\\n shown from each. The implementation for an eager SECD machine is relatively straightforward, so we confine discussion of this\\n to a brief sketch. A more detailed account is given of the implementation for a lazy combinator reduction machine, as this\\n offers one solution to well-known problems with debugging functional programs in the context of lazy evaluation and combinator\\n code.',\n",
       " '53e997ccb7602d9701fbdbf4': 'A global brightness-variation compensation (GBC) scheme is proposed to improve the coding efficiency for video scenes that contain global brightness-variations caused by fade in/out, camera-iris adjustment, flicker, illumination change, and so on. In this scheme, a set of two brightness-variation parameters, which represent multiplier and offset components of the brightness-variation in the whole frame, is estimated. The brightness-variation is then compensated using the parameters. We also propose a method to apply the GBC scheme to component signals for video coding. Furthermore, a block-by-block on/off control method for GBC is introduced to improve the coding performance even for scenes including local variations in brightness caused by camera flashes, spotlights, and the like. Simulation results show that the proposed GBC scheme with the on/off control method improves the peak signal-to-noise ratio (PSNR) by 2-4 dB when the coded scenes contain brightness-variations',\n",
       " '53e997ccb7602d9701fbdc08': 'The application of genetic algorithm (GA) optimization has been proven to be highly successful in the area of non-linear optimizations. For a Multiple-Input, Multiple-Output (MIMO) wireless communication system, the optimum performance of the users with respect to the arrangement of antennas is a highly non-linear function which a genetic algorithm seems highly suited to. In this paper, a four-by-four MIMO system model is considered using a simplified 2-D LOS radio channel with additive white noise. Placing the users in a known arrangement, the optimum placement of the four antennas is determined through a genetic algorithm optimization using the average MSE of the four users as the fitness function. Initial results show a tendency towards an arrangement in which at least two antennas seen by the all transmitters are separated by a symbol wavelength.',\n",
       " '53e997ccb7602d9701fbda6f': 'The main purpose of building data integration systems is to facilitate access to a multitude of data sources. A data integration system must contain a module that uses source descriptions in order to reformulate user queries which are posed in terms of the composite global schema, into sub-queries that refer directly to the schemas of the component data sources. In this paper we propose a method for this user query translation task to target distributed heterogeneous structured data residing in relational databases and semi-structured data held in well-formed XML documents (XML documents which have no referenced DTD or XML schema) produced by Internet applications or human-coded. These XML documents can be XML files on local hard drives or remote documents on Web servers. Our method is based on mappings between the master (composite) view and the participating data source schema structures that are defined in a generated XML Metadata Knowledge Base (XMKB).',\n",
       " '53e997ccb7602d9701fbdc41': 'Large-scale object-oriented (OO) software systems have recently been found to share global network characteristics such as\\n small world and scale free, which go beyond the scope of traditional software measurement and assessment methodologies. To measure the complexity at\\n various levels of granularity, namely graph, class (and object) and source code, we propose a hierarchical set of metrics\\n in terms of coupling and cohesion — the most important characteristics of software, and analyze a sample of 12 open-source\\n OO software systems to empirically validate the set. Experimental results of the correlations between cross-level metrics\\n indicate that the graph measures of our set complement traditional software metrics well from the viewpoint of network thinking, and provide more effective information about fault-prone classes in practice.',\n",
       " '53e997ccb7602d9701fbdc53': 'We calculate the Clarke and Michel-Penot subdifferentialsof the function which maps a symmetric matrix to its mth largesteigenvalue. We show these two subdifferentials coincide, and areidentical for all choices of index m corresponding to equaleigenvalues. Our approach is via the generalized directionalderivatives of the eigenvalue function, thereby completing earlierstudies on the classical directional derivative.',\n",
       " '53e997ccb7602d9701fbe39b': 'This paper describes and reviews a class of hierarchical probabilistic models of images and objects. Visual structures are represented in a hierarchical form where complex structures are composed of more elementary structures following a design principle of recursive composition. Probabilities are defined over these structures which exploit properties of the hierarchy--e.g. long range spatial relationships can be represented by local potentials at the upper levels of the hierarchy. The compositional nature of this representation enables efficient learning and inference algorithms. In particular, parts can be shared between different object models. Overall the architecture of Recursive Compositional Models (RCMs) provides a balance between statistical and computational complexity.The goal of this paper is to describe the basic ideas and common themes of RCMs, to illustrate their success on a range of vision tasks, and to gives pointers to the literature. In particular, we show that RCMs generally give state of the art results when applied to a range of different vision tasks and evaluated on the leading benchmarked datasets.',\n",
       " '53e997ccb7602d9701fbe3ac': 'In this paper, monomial reachability and zero controllability properties of discrete-time positive switched systems are investigated. Necessary and sufficient conditions for these properties to hold, together with some interesting examples and some testing algorithms, are provided.',\n",
       " '53e997ccb7602d9701fbe3cb': \"This paper presents a coding protocol that allows naïve users to annotate dialogue transcripts for anaphora and ellipsis. Cohen's kappa statistic demonstrates that the protocol is sufficiently robust in terms of reliability. It is proposed that quantitative ellipsis data may be used as an index of mutual-engagement. Current and potential uses of ellipsis coding are described.\",\n",
       " '53e997ccb7602d9701fbe3df': 'We present a reliable universal method for ranking sequential patterns (itemset-sequences) with respect to significance in the problem of frequent sequential pattern mining. We approach the problem by first building a probabilistic reference model for the collection of itemset-sequences and then deriving an analytical formula for the frequency for sequential patterns in the reference model. We rank sequential patterns by computing the divergence between their actual frequencies and their frequencies in the reference model. We demonstrate the applicability of the presented method for discovering dependencies between streams of news stories in terms of significant sequential patterns, which is an important problem in multi-stream text mining and the topic detection and tracking research.',\n",
       " '53e997ccb7602d9701fbe3e4': 'Capacity estimation in code division multiple access (CDMA) systems is an important issue which is closely related to power control. Strength-based power control has been assumed in most analyses in which other cell interference was considered as a known and fixed variable. However, in signal-to-interference ratio (SIR)-based power-control systems, power control and other cell interference are closely related to each other and capacity can be obtained by considering this relationship. This study derives the reverse-link capacity of an SIR-based power-controlled multicode CDMA system supporting heterogeneous CBR and on-off traffic in a multiple cell environment. Mean and variance statistics of total other cell interference, and the effects of traffic and propagation parameters on system capacity are investigated',\n",
       " '53e997ccb7602d9701fbe45d': 'Different geometries of nitromethane dimer and nitromethane trimer have been fully optimized employing the density functional theory B3LYP method and the 6-31++G** basis set. Three-body interaction energy has been obtained with the ab initio supermolecular approach at the levels of MP2/6-31++G**//B3LYP/6-31++G** and MP2/aug-cc-pVDZ//B3LYP/6-31++G**. The internal rotation of methyl group induced by intermolecular interaction has been observed theoretically. For the optimized structures of nitromethane dimer, the strength of C-H...O-N H-bond ranges from -9.0 to -12.4 kJ mol(-1) at the MP2/aug-cc-pVDZ//B3LYP/6-31++G** level, and the B3LYP method underestimates the interaction strength compared with the MP2 method, while MP2/6-31++G**//B3LYP/631++G** calculated DeltaE(C) is within 2.5 kJ mol(-1) of the corresponding value at the MP4(SDTQ)/6-31G**//B3LYP/6-31++G** level. The analytic atom-atom intermolecular potential has been successfully regressed by using the MP2/6-31++G**//B3LYP/6-31++G** calculated interaction energies of nitromethane dimer. For the optimized structures of nitromethane trimer the three-body interaction energies occupy small percentage of corresponding total binding energies, but become important for the compressed nitromethane explosive. In addition, it has been discovered that the three-body interaction energy in the cyclic nitromethane trimer is more and more negative as intermolecular distances decrease from 2.2 to 1.7 Angstrom.',\n",
       " '53e997ccb7602d9701fbe508': 'Getting the rules from a domain expert is often a time consuming and expensive part of building ES. Methods of ES development which do not require the intervention of a knowledge engineer are receiving increasing attention. This paper discusses modes of automated learning possible in conventional (rule-based), statistical and neural network implemented classification-type ES, and their benefits and limitations.',\n",
       " '53e997ccb7602d9701fbe4a3': 'Data sharing is a feasible solution to support a large-scale video-on-demand (VoD) system, however, it is still an open issue to provide a full set of continuous interactive functions in such an environment that a single channel is used to serve a group of customers. In this paper, we first investigate the performance of client buffer management and its hybrid use with contingency channels for providing VCR functionality in a staggered broadcast VoD system. Results show that directly combining the two methods may not gain any advantages since the resources are not fully utilized. To tackle this problem, a greedy algorithm is proposed to efficiently manage the contingency channels such that the system performance can be significantly improved by exploiting the property of the staggered broadcast scheme and the use of multiple loaders in the receiver. Simulation results show that the proposed greedy system can efficiently provide a full set of VCR functions with satisfied user performance in a broadcast VoD system.',\n",
       " '53e997ccb7602d9701fbe559': 'In this paper, an innovative watermarking scheme based on differential evolution (DE) in the transform domain is proposed. The insertion and extraction of the watermark are performed in discrete wavelet transform-singular value decomposition (DWT–SVD) transform domain. In the embedding process, the host image is transformed into sub-bands of different frequencies by third-level DWT and then subsequent application of SVD on low pass (LL) and high pass (HH) sub-bands at level third. The watermark image is properly scaled down by multiplying with different scaling factors (SFs) and embedded in the Singular value matrix of LL and HH sub-bands of the host image to make the watermark invisible and robust. We applied an optimization technique, differential evolution, to search optimal scaling factors to improve the quality of watermarked image and robustness of the watermark. In order to overcome the false positive problem, a binary watermark is also embedded in the host image in a lossless manner. According to the numerical results, the quality of watermarked image is satisfactory and embedded watermark is extracted successfully even if the watermarked image is exposed to various image processing and geometric attacks.',\n",
       " '53e997ccb7602d9701fbe567': 'In this paper, we try to clarify some of the questions related to a key concept in multivariate polynomial solving algorithm over a finite field: the degree of regularity. By the degree of regularity, here we refer to a concept first presented by Dubois and Gama, namely the lowest degree at which certain nontrivial degree drop of a polynomial systemoccurs. Currently, it is somehow commonly accepted that we can use this degree to estimate the complexity of solving a polynomial system, even though we do not have systematic empirical data or a theory to support such a claim. In this paper, we would like to clarify the situation with the help of experiments. We first define a concept of solving degree for a polynomial system. The key question we then need to clarify is the connection of solving degree and the degree of regularity with focus on quadratic systems. To exclude the cases that do not represent the general situation, we need to define when a system is degenerate and when it is irreducible. With extensive computer experiments, we show that the two concepts, the degree of regularity and the solving degree, are related for irreducible systems in the sense that the difference between the two degrees is indeed small, less than 3. But due to the limitation of our experiments, we speculate that this may not be the case for high degree cases. © Springer-Verlag Berlin Heidelberg 2013.',\n",
       " '53e997ccb7602d9701fbe4c7': 'Optical flow can be used to segment a moving object from its background provided the velocity of the object is distinguishable from that of the background, and has expected characteristics. Existing optical flow techniques often detect flow (and thus the object) in the background. To overcome this, we propose a new optical flow technique, which only determines optical flow in regions of motion. We also propose a method by which output from a tracking system can be fed back into the motion segmenter/optical flow system to reinforce the detected motion, or aid in predicting the optical flow.',\n",
       " '53e997ccb7602d9701fbe546': \"A new distributed algopithmfor the dynamic computation of mul- tlple loop-free pathsf~om source to destination in a computer net- work or internet arepresented, validated, and analyzed, According to this algorithms, which is called DASM (D@using Algorithm for Shortest Multzpath), each router maintains a set of entries for each destination in its routing table, and each such ent~ consists of a set of tuples spect~ing the next router and distance in a loop- free path to the destination. DASM guarantees instantaneous loop freedom of multipath routing tables by means of a generalization ofDijkstra and Scholten 's dljiising computations, With generalized d@sing computations, a node in a directed acyclic graph (DAG) dejined for a given destination has multiple next nodes in the DAG and is able to modlfi the DAG without creating a directed loop. DASM is shown to be loop-free at eve~ instant, and its average performance is analyzed by simulation and compared against an ideal link-state algorithm and the D@using Update Algorithm (DUAL).\",\n",
       " '53e997ccb7602d9701fbe5d9': 'Recently, a semi definite programming relaxation of the power flow equations has been applied to the optimal power flow problem. When this relaxation is \"tight\" (i.e., the solution has zero duality gap), a globally optimal solution is obtained. Existing literature investigates sufficient conditions whose satisfaction guarantees zero duality gap solutions. However, there is limited study of non-zero duality gap solutions. By illustrating the feasible spaces for optimal power flow problems and their semi definite relaxations, this paper investigates examples of non-zero duality gap solutions. Results for large system models suggest that non-convexities associated with small subsections of the network are responsible for non-zero duality gap solutions.',\n",
       " '53e997ccb7602d9701fbe663': 'Compared with other existing video coding standards, H.264/AVC can achieve a significant improvement in compression performances.\\n A robust criterion named the rate distortion optimization (RDO) is employed to select the optimal coding modes and motion\\n vectors for each macroblock (MB), which achieves a high compression ratio while leading to a great increase in the complexity\\n and computational load unfortunately. In this paper, a fast mode decision algorithm for H.264/AVC intra prediction based on\\n integer transform and adaptive threshold is proposed. Before the intra prediction, integer transform operations on the original\\n image are executed to find the directions of local textures. According to this direction, only a small part of the possible\\n intra prediction modes are tested for RDO calculation at the first step. If the minimum mean absolute error (MMAE) of the\\n reconstructed block corresponding to the best mode is smaller than an adaptive threshold which depends on the quantization\\n parameter (QP), the RDO calculation is terminated. Otherwise, more possible modes need to be tested. The adaptive threshold\\n aims to balance the compression performance and the computational load. Simulation results with various video sequences show\\n that the fast mode decision algorithm proposed in this paper can accelerate the encoding speed significantly only with negligible\\n PSNR loss or bit rate increment.',\n",
       " '53e997ccb7602d9701fbe585': \"Chat is a crucial function in current virtual worlds. Since virtual worlds are becoming increasingly popular, we need a realistic and efficient communication framework for multi-agents participating in a virtual world. This paper proposes a unified communication framework for multiple avatars in a virtual world, (e.g., second-life). Current chat systems in virtual worlds provide a set of special communication protocols such as pairwise, group and secret chat, independently. Therefore, modern virtual communication systems have adapted artificial chat techniques such as 'one-to-one chat' and 'group chat' to a virtual 3-D space. Our framework exploits spatial relationships (distance, and viewing vector) between several agents, without including a specialized chat protocol layer in communication. The main contribution of our work is threefold. First, we propose a realistic and unified communication framework which enables 'complete chat' and 'partial chat' in terms of spatial relationships between avatar agents without including an additional communication protocol. Second, we propose a single greedy algorithm to find an optimal position in 3-D virtual space which optimizes chat availability. Third, our system reconstructs a dialogue graph which maintains all transcripts in the form of directed graphs with temporal (dialogue sequences) and spatial information (physical positions) about communicating agents.\",\n",
       " '53e997ccb7602d9701fbe588': 'Safety assessment of complex systems traditionally requires the combination ofv arious results derived from various models. The Altarica language was designed to formally specify the behaviour of systems when faults occurs. A unique Altarica model can be assessed by means of complementary tools such as fault tree generator and model-checker. This paper reports how the Altarica language was used to model a system in the style oft he hydraulic system oft he Airbus A320 aircraft family. It presents how fault tree generation and model-checking can be used separately then combined to assess safety requirements.',\n",
       " '53e997ccb7602d9701fbe58b': 'This thesis is concerned with adaptive learning algorithms for Bayesian network classifiers (BNCs) in a prequential (on-line) learning scenario capable of handling the cost-performance trade-off and concept drift. All these algorithms are integrated into the adaptive prequential framework for supervised learning, AdPreqFr4SL. We evaluated our algorithms on artificial domains and benchmark problems and show their advantages and future applicability in real-world on-line learning systems.',\n",
       " '53e997ccb7602d9701fbe591': \"The fast multipole method (FMM) is a technique allowing the fast calculation of long-range interactions between N points in O(N) or O(N log N) steps with some prescribed error tolerance. The FMM has found many applications in the field of integral equations and boundary element methods, in particular by accelerating the solution of dense linear systems arising from such formulations. Standard FMMs are derived from analytical expansions of the kernel, for example using spherical harmonics or Taylor expansions. In recent years, the range of applicability and the ease of use of FMMs have been extended by the introduction of black-box and kernel independent techniques. In these approaches, the user provides only a subroutine to numerically calculate the interaction kernel. This allows changing the definition of the kernel with minimal changes to the computer program. This paper presents a novel kernel independent FMM, which leads to diagonal multipole-to-local operators. The result is a significant reduction in the computational cost, particularly when high accuracy is needed. The approach is based on Cauchy's integral formula and the Laplace transform. We will present a numerical analysis of the convergence and numerical results in the case of a multilevel one-dimensional FMM.\",\n",
       " '53e997ccb7602d9701fbe5a2': 'In this work we present a method to intuitively issue control over devices in smart environments, to display data that smart objects and sensors provide, and to create and manipulate flows of information in smart environments. This makes it easy to customize smart environments by linking arbitrary data sources to various display modalities on the fly. Touchscreen smartphones - as readily available multi-purpose devices - are used to overlay real objects with virtual controls. We evaluated this system with a first qualitative user study.',\n",
       " '53e997ccb7602d9701fbe67e': \"This paper proposes a novel and unconventional Memetic Computing approach for solving continuous optimization problems characterized by memory limitations. The proposed algorithm, unlike employing an explorative evolutionary framework and a set of local search algorithms, employs multiple exploitative search within the main framework and performs a multiple step global search by means of a randomized perturbation of the virtual population corresponding to a periodical randomization of the search for the exploitative operators. The proposed Memetic Computing approach is based on a populationless (compact) evolutionary framework which, instead of processing a population of solutions, handles its statistical model. This evolutionary framework is based on a Differential Evolution which cooperatively employs two exploitative search operators: the first is based on a standard Differential Evolution mutation and exponential crossover, and the second is the trigonometric mutation. These two search operators have an exploitative action on the algorithmic framework and thus contribute to the rapid convergence of the virtual population towards promising candidate solutions. The action of these search operators is counterbalanced by a periodical stochastic perturbation of the virtual population, which has the role of ''disturbing'' the excessively exploitative action of the framework and thus inhibits its premature convergence. The proposed algorithm, namely Disturbed Exploitation compact Differential Evolution, is a simple and memory-wise cheap structure that makes use of the Memetic Computing paradigm in order to solve complex optimization problems. The proposed approach has been tested on a set of various test problems and compared with state-of-the-art compact algorithms and with some modern population based meta-heuristics. Numerical results show that Disturbed Exploitation compact Differential Evolution significantly outperforms all the other compact algorithms present in literature and reaches a competitive performance with respect to modern population algorithms, including some memetic approaches and complex modern Differential Evolution based algorithms. In order to show the potential of the proposed approach in real-world applications, Disturbed Exploitation compact Differential Evolution has been implemented for performing the control of a space robot by simulating the implementation within the robot micro-controller. Numerical results show the superiority of the proposed algorithm with respect to other modern compact algorithms present in literature.\",\n",
       " '53e997ccb7602d9701fbe697': 'Petri nets are a discrete event simulation approach developed for system representation, in particular for their concurrency and synchronization properties. Various extensions to the original theory of Petri nets have been used for modeling molecular biology systems and metabolic networks. These extensions are stochastic, colored, hybrid and functional. This paper carries out an initial review of the various modeling approaches based on Petri net found in the literature, and of the biological systems that have been successfully modeled with these approaches. Moreover, the modeling goals and possibilities of qualitative analysis and system simulation of each approach are discussed.',\n",
       " '53e997ccb7602d9701fbe637': 'The commercial market for computer games and other multimedia products is extremely large and young people have a considerable experience of such games. Disabled users have very limited access to this important part of the youth culture. Indeed there are few entertaining computer games which are accessible for them. Research and development in the field of IT and the disabled has focused on education rather than leisure',\n",
       " '53e997ccb7602d9701fbe6e7': 'The design of software for the minicomputer business data processing environments poses significant practical design problems. This paper deals with the history of the design and implementation of a specific system for that environment. Changes in the system design due to problems in the operational environment are examined. Conclusions are made concerning the design of the system.',\n",
       " '53e997ccb7602d9701fbe65c': 'The aim of this paper is to study the Walrasian equilibrium problem when the data are time dependent. For this model an existence result is provided using the variational inequality theory in infinite dimensional spaces. Our results are the generalization of some of the results obtained by several authors in the static case (see e.g. Donato et\\xa0al. (2008) [5], Donato et\\xa0al. (2008) [4] and Mordukhovich (2006) [11], Nagurney (1993) [2] and the references therein).',\n",
       " '53e997ccb7602d9701fbe6fb': 'We present a way of exploiting domain knowledge in the design and implementation of data mining algorithms, with special attention to frequent patterns discovery, within a deductive framework. In our framework, domain knowledge is represented by way of deductive rules, and data mining algorithms are specified by means of iterative user-defined aggregates and implemented by means of user-defined predicates. This choice allows us to exploit the full expressive power of deductive rules without loosing in performance. Iterative user-defined aggregates have a fixed scheme, in which user-defined predicates are to be added. This feature allows the modularization of data mining algorithms, thus providing a way to integrate the proper domain knowledge exploitation in the right point. As a case study, the paper presents how user-defined aggregates can be exploited to specify and implement a version of the a priori algorithm. Some performance analyzes and comparisons are discussed in order to show the effectiveness of the approach.',\n",
       " '53e997ccb7602d9701fbe6a1': 'Along with the theoretical and practical research on introducing a social dimension to adaptive educational hypermedia, the evaluation of such systems becomes more important. Existing evaluation methods are mostly based on statistical and qualitative analysis, in which researcher bias is built in and unavoidable. Moreover, they adopt either a traditional \"as a whole\" approach making it difficult to evaluate a system from different perspectives, or a \"goal specified\" approach, which only covers a specific aspect. Therefore, this study proposes a generic method for evaluating system functionality.',\n",
       " '53e997ccb7602d9701fbe716': 'In this study, we investigate the relative ability of least square algorithm to retrieve sea surface salinity (SSS) from MODIS satellite data. We also examine with comprehensive comparison of the root mean square of bias the difference between second polynomial order algorithm and least square algorithm. Both the least squares algorithm and second polynomial order algorithm are used to retrieve the sea surface salinity (SSS) from multi MODIS bands data. Thus, the basic linear model has been solved by using second polynomial order algorithm and least square estimators. The accuracy of this work has been examined using the root mean square of bias of sea surface salinity retrieved from MODIS satellite data and the in situ measurements that are collected along the east coast of Peninsular Malaysia by using hydrolab instrument. The study shows comprehensive relationship between least square method and in situ SSS measurements with high r2 of 0.96 and RMS of bias value of ±0.37 psu. The second polynomial order algorithm, however, has lower performance as compared to least square algorithm. Thus, RMS of bias value of ± 7.34 psu has performed with second polynomial order algorithm. In conclusions, the least square algorithm can be used to retrieve SSS from MODIS satellite data.',\n",
       " '53e997ccb7602d9701fbe721': 'In this paper, we propose an effective data pipelining technique, SPDP (scratch-pad data pipelining), for dynamic scratch-pad memory (SPM)management with DMA (Direct Memory Access). InSPDP, we group multiple iterations of a loop into a block for SPM allocation, and implement a data pipeline by overlapping the execution of CPU instructions and DMA operations. We have implemented our SPDP technique into the IMPACT compiler,and conduct experiments using a set of benchmarks from DSP stone, Mibench and Mediabench on the cycle-accurate VLIW simulator of Trimaran. The experimental results show that our technique achieves significant performance improvement compared with the previous work.',\n",
       " '53e997ccb7602d9701fbeb16': 'We propose a constraint-based formulation of Hindley/Milner style type inference system as opposed to the standard substitution-based formulation. This allows us to make important distinctions between different phases of type inference: Constraint generation/propagation, constraint solving, constraint simplification and term reconstruction. The inference system is parametric in the constraint domain, covering a wide range of application domains. A problem, incompleteness of substitution-based inference, identified by A. J. Kennedy can be solved naturally by employing a constraint-based view of type inference. In addition, our formulation of type inference can easily be tailored to different inference algorithms such as W and M. On the technical side, we present concise soundness and completeness results.',\n",
       " '53e997ccb7602d9701fbe765': 'Most existing wormhole networks do not provide support for prioritized traffic at the link level. Conventional demand multiplexing does not allow flexibility for fast movement of high priority messages such as for synchronization and control information. In this paper, an approach to prioritized physical channel scheduling is proposed. The motivation behind the proposed approach and its description are presented. The approach is evaluated and compared against the conventional demand multiplexing for a wide range of system parameters. The results demonstrate significant potential for designing high performance wormhole systems to support prioritized traffic. (C) 1998 Published by Elsevier Science B.V.',\n",
       " '53e997ccb7602d9701fbe781': 'Most of the existing research on multivariate time series concerns supervised forecasting problems. In comparison, little research has been devoted to their exploration through unsupervised clustering and visualization. In this paper, the capabilities of Generative Topographic Mapping Through Time, a model with foundations in probability theory, that performs simultaneous time series clustering and visualization, are assessed in detail. Focus is placed on the visualization of the evolution of signal regimes and the exploration of sudden transitions, for which a novel identification index is defined. The interpretability of time series clustering results may become extremely difficult, even in exploratory visualization, for high dimensional datasets. Here, we define and test an unsupervised time series relevance determination method, fully integrated in the Generative Topographic Mapping Through Time model, that can be used as a basis for time series selection. This method should ease the interpretation of time series clustering results.',\n",
       " '53e997ccb7602d9701fbeb43': \"Denial of Service attacks are presenting an increasing threat to the global inter-networking infrastructure. While TCP's congestion control algorithm is highly robust to diverse network conditions, its implicit assumption of end-system cooperation results in a well-known vulnerability to attack by high-rate non-responsive flows. In this paper, we investigate a class of low-rate denial of service attacks which, unlike high-rate attacks, are difficult for routers and counter-DoS mechanisms to detect. Using a combination of analytical modeling, simulations, and Internet experiments, we show that maliciously chosen low-rate DoS traffic patterns that exploit TCP's retransmission timeout mechanism can throttle TCP flows to a small fraction of their ideal rate while eluding detection. Moreover, as such attacks exploit protocol homogeneity, we study fundamental limits of the ability of a class of randomized timeout mechanisms to thwart such low-rate DoS attacks.\",\n",
       " '53e997ccb7602d9701fbe784': 'In this paper, we propose a novel closed-form approximation of the Energy Efficiency vs. Spectral Efficiency (EE-SE) trade-off for the uplink/downlink of distributed multiple-input multiple-output (DMIMO) system with two cooperating base stations. Our closed-form expression can be utilized for evaluating the idealistic and realistic EE-SE performances of various antenna configurations as well as assessing how DMIMO compares against MIMO system in terms of EE. Results show a tight match between our closed-form approximation and the Monte-Carlo simulation for both idealistic and realistic EESE trade-off. Our results also show that given a target SE requirement, there exists an optimal antenna setting that maximizes the EE. In addition, DMIMO scheme can offer significant improvement in terms of EE over the MIMO scheme.',\n",
       " '53e997ccb7602d9701fbeb49': 'On the basis of the relationship of the mth power of a polynomial and its modular form (polynomial whose coefficients are the moduli of the coefficients of that polynomial), we derive a necessary and sufficient condition for the modulus of the mth power of a polynomial for contacting its modular form on the boundary of a disc. Combined with the result about distribution of zeros of analytic function, some new sufficient conditions are derived which give bounds of the absolute values of the roots of a quasi-critical polynomial. These results extend certain earlier similar tests for linear discrete-time systems. Finally, four examples are given to demonstrate the results, Example 2.1 gives a state feedback application, Examples 2.2 and 2.4 deal with r-stability, and Example 2.3 display that our theorems give better results when m increases but at the cost of increasing complexity.',\n",
       " '53e997ccb7602d9701fbeb5e': 'In this paper, we present error-correcting codes that achieve the information-theoretically best possible tradeoff between the rate and error-correction radius. Specifically, for every 0 < R < 1 and epsiv < 0, we present an explicit construction of error-correcting codes of rate that can be list decoded in polynomial time up to a fraction (1- R - epsiv) of worst-case errors. At least theoretically, this meets one of the central challenges in algorithmic coding theory. Our codes are simple to describe: they are folded Reed-Solomon codes, which are in fact exactly Reed-Solomon (RS) codes, but viewed as a code over a larger alphabet by careful bundling of codeword symbols. Given the ubiquity of RS codes, this is an appealing feature of our result, and in fact our methods directly yield better decoding algorithms for RS codes when errors occur in phased bursts. The alphabet size of these folded RS codes is polynomial in the block length. We are able to reduce this to a constant (depending on epsiv) using existing ideas concerning ldquolist recoveryrdquo and expander-based codes. Concatenating the folded RS codes with suitable inner codes, we get binary codes that can be efficiently decoded up to twice the radius achieved by the standard GMD decoding.',\n",
       " '53e997ccb7602d9701fbeb8a': \"Over recent years some mathematicians and computer scientists have used fractal geometry to generate, study and analyze complex images. Fractal techniques offer a rich source for exploitation by expert users and artists generally to 'dirty up' images to eliminate the computer graphics feel and look of a sterile environment. There are several models for fractal shapes and each has certain benefits and drawbacks. All of them provide a basis for object generation but the rendering. process is far from reaching the performance levels needed for interactivity. Linear fractal models such as the Iterated Function Systems seem to offer more in that matter. IFSs can serve as elegant test beds for research into interactive modeling of complex natural and artificial phenomena. This article discusses the IFS model with encoded RGB colour transformations. The method surpasses classical stochastic approaches and promises real-time generation of a wider variety of complex objects. (C) 1998 Elsevier Science Ltd. All rights reserved.\",\n",
       " '53e997ccb7602d9701fbe7f0': 'This paper presents Multilingual Document Clustering (MDC) on comparable corpora. Wikipedia has evolved to be a major structured multilingual knowledge base. It has been highly exploited in many monolingual clustering approaches and also in comparing multilingual corpora. But there is no prior work which studied the impact of Wikipedia on MDC. Here, we have studied availing Wikipedia in enhancing MDC performance. We have leveraged Wikipedia knowledge structure (such as cross-lingual links, category, outlinks, Infobox information, etc.) to enrich the document representation for clustering multilingual documents. We have implemented Bisecting k-means clustering algorithm and experiments are conducted on a standard dataset provided by FIRE for their 2010 Ad-hoc Cross-Lingual document retrieval task on Indian languages. We have considered English and Hindi datasets for our experiments. By avoiding language-specific tools, our approach provides a general framework which can be easily extendable to other languages. The system was evaluated using F-score and Purity measures and the results obtained were encouraging.',\n",
       " '53e997ccb7602d9701fbeb98': \"A study of requirements elicitation and validation within an industrial environment is reported. The key features in this part of the requirements process are: scenarios, as the prime means of elicitation; identification of domain objects, to capture the language of the domain and Fagan inspections for scenario validation by stakeholders. The process has been evaluated from both the requirements engineer's perspective and the viewpoint of the various stakeholders. The findings highlight a number of issues, both positive and negative, which are discussed. The deficiencies identified have stimulated our research. In particular it is our contention that requirements documentation need to break away from the fixation with purely textual documents to ones that are media rich. Examples of this research, such as the hypermedia Scenario Manager are described.\",\n",
       " '53e997ccb7602d9701fbe7cd': 'In this paper, a model for optimizing bus route headway is presented in a given network configuration and demand matrix, which aims to find an acceptable balance between passenger costs and operator costs, namely the maximization of service quality and the minimization of operational costs. An integrated approach is also proposed in the paper to determine the relative weights between passenger costs and operator costs. A parallel genetic algorithm (PGA), in which a coarse-grained strategy and a local search algorithm based on Tabu search are applied to improve the performance of genetic algorithm, is developed to solve the headway optimization model. Data collected in Dalian City, China, is used to verify the feasibility of the model and the algorithm. Results show that the reasonable resource assessment can increase the benefits of transit system.',\n",
       " '53e997ccb7602d9701fbe7fb': 'We consider cooperative transmission in wireless relay networks, in which a source communicates with the destination with the help of a set of N cooperating amplify-and-forward relays. The relay weights are obtained to maximize the received signal-to-noise ratio at the destination, subject to individual power constraint. We consider two schemes that have appeared in the literature, i.e., (i) the optimal weight vector design method, which has been solved via second-order cone programming plus a bisection search, with complexity of O(N3.5), and (ii) the one-bit feedback phase control scheme, which has been formulated as a binary quadratic programming and has been solved for exact solution via exhaustive search. We propose algorithms for these two problems that have substantially reduced complexity, i.e., O(N log2 N) or O(N) for the first problem, and polynomial time O(N log2 N) for the second problem.',\n",
       " '53e997ccb7602d9701fbebd2': 'The simulation of the dynamics of a cellular systems based on cellular automata (CA) can be computationally expensive. This is particularly true when such simulation is part of a procedure of rule induction to find suitable transition rules for the CA. Several efforts have been described in the literature to make this problem more treatable. This work presents a study about the efficiency of dynamic behavior forecasting parameters (DBFPs) used for the induction of transition rules of CA for a specific problem: the classification by the majority rule. A total of 8 DBFPs were analyzed for the 31 best-performing rules found in the literature. Some of these DBFPs were highly correlated each other, meaning they yield the same information. Also, most rules presented values of the DBFPs very close each other. An evolutionary algorithm, based on gene expression programming, was developed for finding transition rules according a given preestablished behavior. The simulation of the dynamic behavior of the CA is not used to evaluate candidate transition rules. Instead, the average values for the DBFPs were used as reference. Experiments were done using the DBFPs separately and together. In both cases, the best induced transition rules were not acceptable solutions for the desired behavior of the CA. We conclude that, although the DBFPs represent interesting aspects of the dynamic behavior of CAs, the transition rule induction process still requires the simulation of the dynamics and cannot rely only on the DBFPs.',\n",
       " '53e997ccb7602d9701fbebdc': 'Many statistical methods often fail to identify biologically meaningful biomarkers related to a specific disease under study from expression data alone. In this paper, we develop a novel strategy, namely knowledge-driven multi-level independent component analysis (ICA), to infer regulatory signals and identify biologically relevant biomarkers from microarray data. Specifically, based on multi-level clustering results and partial prior knowledge, we apply ICA to find stable disease specific linear regulatory modes and then extract associated biomarker genes. A statistical test is designed to evaluate the significance of transcription factor enrichment for extracted gene set based on motif information. The experimental results on an Rsf-1 induced microarray data set show that our knowledge-driven method can extract more biologically meaningful biomarkers with significant enrichment of transcription factors related to ovarian cancer compared to other gene selection methods with/without prior knowledge.',\n",
       " '53e997ccb7602d9701fbebea': 'The ability to recognize the strokes drawn by the user, is central to most sketch-based interfaces. However, very few solutions that rely on recognition are robust enough to make sketching a definitive alternative to traditional WIMP user interfaces. In this paper, we propose an approach based on classification that given an unconstrained sketch, can robustly assign a label to each stroke that comprises the sketch. A key contribution of our approach is a technique for grouping strokes that eliminates outliers and enhances the robustness of the classification. We also propose a set of features that capture important attributes of the shape and mutual relationship of strokes. These features are statistically well-behaved and enable robust classification with Support Vector Machines (SVM). We conclude by presenting a concrete implementation of these techniques in an interface for driving facial expressions.',\n",
       " '53e997ccb7602d9701fbebe6': 'Recently, there has been a lot of success in using the deterministic approach to provide approximate characterization of Gaussian network capacity. In this paper, we take a deterministic view and revisit the problem of wiretap channel with side information. A precise characterization of the secrecy capacity is obtained for a linear deterministic model, which naturally suggests a coding scheme which we show to achieve the secrecy capacity of the degraded Gaussian model (dubbed as “secret writing on dirty paper”) to within half a bit.',\n",
       " '53e997ccb7602d9701fbebf7': 'In this paper, we investigate relative complexity between #P and other classes of functions. Our particular interest is to compare #P with #PH and with PFH by using polynomial-time reducibility and to demonstrate that a weaker notion of polynomial-time reducibility is sufficiently powerful for reducing #PH functions to #P functions. Our main result is stated as follows: Every function in #PH is polynomial-time 1-Turing reducible to some function in #P. That is, # PH ⊆ PF # P [1] . Some consequences of this result are as follows: Every function in PFH is polynomial-time 1-Turing reducible to some function in #P. If PF # P [1] ⊆# PH , then PH collapses to a finite level; furthermore, if either # P ⊆ PFH or PFH ⊆# P , then PH collapses to a finite level. We also give an affirmative answer to an open question posed by Valiant (1979), and we show a generalized result about p-rankability by Hemachandra (1987).',\n",
       " '53e997ccb7602d9701fbec03': 'Multiple imputation is a popular way to handle missing data. Automated procedures are widely available in standard software. However, such automated procedures may hide many assumptions and possible difficulties from the view of the data analyst. Imputation procedures such as monotone imputation and imputation by chained equations often involve the fitting of a regression model for a categorical outcome. If perfect prediction occurs in such a model, then automated procedures may give severely biased results. This is a problem in some standard software, but it may be avoided by bootstrap methods, penalised regression methods, or a new augmentation procedure.',\n",
       " '53e997ccb7602d9701fbe820': 'From lyrics-display on electronic music players and Karaoke videos to surtitles for live Chinese opera performance, one feature is common to all these everyday functionalities temporal: synchronization of the written text and its corresponding musical phrase. Our goal is to automate the process of lyrics alignment, a procedure which, to date, is still handled manually in the Cantonese popular song (Cantopop) industry. In our system, a vocal signal enhancement algorithm is developed to extract vocal signals from a CD recording in order to detect the onsets of the syllables sung and to determine the corresponding pitches. The proposed system is specifically designed for Cantonese, in which the contour of the musical melody and the tonal contour of the lyrics must match perfectly. With this prerequisite, we use a dynamic time warping algorithm to align the lyrics. The robustness of this approach is supported by experiment results. The system was evaluated with 70 twenty-second music segments and most samples have their lyrics aligned correctly.',\n",
       " '53e997ccb7602d9701fbe82f': \"Bisimilarity is one of the most important relations for comparing the behaviour of formal systems in concurrency theory. Decision algorithms for bisimilarity in finite state systems are usually classified into two kinds: global algorithms are generally efficient but require to generate the whole state spaces in advance, and local algorithms combine the verification of a system's behaviour with the generation of the system's state space, which is often more effective to determine that one system fails to be related to another. Although local algorithms are well established in the classical concurrency theory, the study of local algorithms in probabilistic concurrency theory is not mature. In this paper we propose a polynomial time local algorithm for checking probabilistic bisimilarity. With mild modification, the algorithm can be easily adapted to decide probabilistic similarity with the same time complexity.\",\n",
       " '53e997ccb7602d9701fbec4f': 'This paper makes two main contributions. First, it introduces Diverted Accesses, a technique that leverages the redundancy in storage systems to conserve disk energy. Second, it evaluates the previous (redundancy-oblivious) energy conservation techniques, along with Diverted Accesses, as a function of the amount and type of redundancy in the system. The evaluation is based on novel analytic models of the energy consumed by the techniques. Using these energy models and previous models of reliability, availability, and performance, we can determine the best redundancy configuration for new energy-aware storage systems. To study Diverted Accesses for realistic systems and workloads, we simulate a wide-area storage system under two file-access traces. Our modeling results show that Diverted Accesses is more effective and robust than the redundancy-oblivious techniques. Our simulation results show that our technique can conserve 20-61% of the disk energy consumed by the wide-area storage system.',\n",
       " '53e997ccb7602d9701fbec61': 'Advances in the efficient discovery of frequent itemsets have led to the development of a number of schemes that use frequent itemsets to aid developing accurate and efficient classifiers. These approaches use the frequent itemsets to generate a set of composite features that expand the dimensionality of the underlying dataset. In this paper, we build upon this work and (i) present a variety of schemes for composite feature selection that achieve a substantial reduction in the number of features without adversely affecting the accuracy gains, and (ii) show (both analytically and experimentally) that the composite features can lead to improved classification models even in the context of support vector machines, in which the dimensionality can automatically be expanded by the use of appropriate kernel functions.',\n",
       " '53e997ccb7602d9701fbec69': 'This publication contains reprint articles for which IEEE does not hold copyright. Full text is not available on IEEE Xplore for these articles.',\n",
       " '53e997ccb7602d9701fbe879': 'We discuss issues of control for constraint logic programs. The problem we try to solve is to find, from the text of a program, a computation rule which ensures finiteness of the computation tree. In a single framework, we address two related areas, namely the generation of control annotations and the local level of control for partial deduction.',\n",
       " '53e997ccb7602d9701fbe873': 'Peer-assisted on-demand video streaming services are extremely large-scale distributed systems on the Internet. Automated demand forecast and performance prediction, if implemented, can help with capacity planning and quality control so that sufficient server bandwidth can always be supplied to each video channel without incurring wastage. In this paper, we use time-series analysis techniques to automatically predict the online population, the peer upload and the server bandwidth demand in each video channel, based on the learning of both human factors and system dynamics from online measurements. The proposed mechanisms are evaluated on a large dataset collected from a commercial Internet video-on-demand system.',\n",
       " '53e997ccb7602d9701fbec7a': 'Recommender Systems allow people to find the resources they need by making use of the experiences and opinions of their nearest neighbours. Costly annotations by experts are replaced by a distributed process where the users take the initiative. While the collaborative approach enables the collection of a vast amount of data, a new issue arises: the quality assessment. The elicitation of trust values among users, termed \"web of trust\", allows a twofold enhancement of Recommender Systems. Firstly, the filtering process can be informed by the reputation of users which can be computed by propagating trust. Secondly, the trust metrics can help to solve a problem associated with the usual method of similarity assessment, its reduced computability. An empirical evaluation on Epinions.com dataset shows that trust propagation can increase the coverage of Recommender Systems while preserving the quality of predictions. The greatest improvements are achieved for users who provided few ratings.',\n",
       " '53e997ccb7602d9701fbe885': 'A new optimisation problem for design of multi-position machines and automatic transfer lines is considered. To reduce the number of pieces of equipment, machining operations are grouped into blocks. The operations of the same block are performed simultaneously by one piece of equipment (multi-spindle head). At the studied design stage, constraints related to the design of blocks and workstations, as well as precedence constraints for operations are known. The problem consists in an optimal grouping of the operations into blocks minimizing the total number of blocks and workstations while reaching a given cycle time (productivity). A constrained shortest path algorithm is developed and tested.',\n",
       " '53e997ccb7602d9701fbe89b': 'Categorical data appears in various places, and dealing with it has been a major concern in analysis fields. However, representing not only global trends but also local trends of data simultaneously by conventional techniques is difficult. We propose a visualization method called \"granular representation\" for analyzing categorical data visually. Our approach visually represents data as a set of objects and allows intuitive analysis instead of the traditional way with tables of numbers. We developed a tool by integrating granular representation and bar charts. The effectiveness of the tool is demonstrated using real data about media consumption.',\n",
       " '53e997ccb7602d9701fbec96': 'The state of the art in human interaction with computational systems blurs the line between computations performed by machine logic and algorithms, and those that result from input by humans, arising from their own psychological processes and life experience. Current socio-technical systems, known as \"social machines\" exploit the large-scale interaction of humans with machines. Interactions that are motivated by numerous goals and purposes including financial gain, charitable aid, and simply for fun. In this paper we explore the landscape of social machines, both past and present, with the aim of defining an initial classificatory framework. Through a number of knowledge elicitation and refinement exercises we have identified the polyarchical relationship between infrastructure, social machines, and large-scale social initiatives. Our initial framework describes classification constructs in the areas of contributions, participants, and motivation. We present an initial characterisation of some of the most popular social machines, as demonstration of the use of the identified constructs. We believe that it is important to undertake an analysis of the behaviour and phenomenology of social machines, and of their growth and evolution over time. Our future work will seek to elicit additional opinions, classifications and validation from a wider audience, to produce a comprehensive framework for the description, analysis and comparison of social machines.',\n",
       " '53e997ccb7602d9701fbec97': 'In the past two decades both the industry and the research community have proposed hundreds of metrics to track software projects, evaluate quality or estimate effort. Unfortunately, it is not always clear which metric works best in a particular context. Even worse, for some metrics there is little evidence whether the metric measures the attribute it was designed to measure. In this paper we propose a catalog format for software metrics as a first step towards a consolidated overview of available software metrics. This format is designed to provide an overview of the status of a metric in a glance, while providing enough information to make an informed decision about the use of the metric. We envision this format to be implemented in a (semantic) wiki to ensure that relationships between metrics can be followed with ease.',\n",
       " '53e997ccb7602d9701fbeca1': 'A convex decomposition method, called the alternating sum of volumes (ASV) method, uses convex hulls and set-difference operations. ASV decomposition may not converge, which severely limits the domain of geometric objects that can be handled. Via the combination of ASV decomposition and remedial partitioning for the non-convergence, a convergent convex decomposition is proposed that is called the alternating sum of volumes with partitioning (ASVP) decomposition. The paper describes how ASVP decomposition is used for the recognition of form features. In this approach to form-feature recognition, volumetric form features which are intrinsic to the shape of a given object are recognized from the boundary information through ASVP decomposition. Moreover, hierarchical relationships between the recognized form features are obtained in addition to global spatial information.',\n",
       " '53e997ccb7602d9701fbe8b6': 'The shared-memory programming model is a very effective way to achieve parallelism on shared memory parallel computers. As great progress was made in hardware and software technologies, performance of parallel programs with compiler directives has demonstrated large improvement. The introduction of OpenMP directives, the industrial standard for shared-memory programming, has minimized the issue of portability. In this study, we have extended CAPTools, a computer-aided parallelization toolkit, to automatically generate OpenMPbased parallel programs with nominal user assistance. We outline techniques used in the implementation of the tool and discuss the application of this tool on the NAS Parallel Benchmarks and several computational fluid dynamics codes. This work demonstrates the great potential of using the tool to quickly port parallel programs and also achieve good performance that exceeds some of the commercial tools.',\n",
       " '53e997ccb7602d9701fbecdf': 'This paper considers planar location problems with rectilinear distance and barriers where the objective function is any convex, nondecreasing function of distance. Such problems have a non-convex feasible region and a nonconvex objective function. Based on an equivalent problem with modified barriers, derived in a companion paper [3], the non convex feasible set is partitioned into a network and rectangular cells. The rectangular cells are further partitioned into a polynomial number of convex subcells, called convex domains, on which the distance function, and hence the objective function, is convex. Then the problem is solved over the network and convex domains for an optimal solution. Bounds are given that reduce the number of convex domains to be examined. The number of convex domains is bounded above by a polynomial in the size of the problem.',\n",
       " '53e997ccb7602d9701fbe8e9': 'The goal of the study was to investigate the influence of asymmetric coupling, between the soma and dendrites, on the nonlinear dynamic behaviour of a two-compartment model. We used a recently published method for generating reduced two-compartment models that retain the asymmetric coupling of anatomically reconstructed motor neurons. The passive input-output relationship of the asymmetrically coupled model was analytically compared to the symmetrically coupled case. Predictions based on the analytic comparison were tested using numerical simulations. The simulations evaluated the nonlinear dynamics of the models as a function of coupling parameters. Analytical results showed that the input resistance at the dendrite of the asymmetric model was directly related to the degree of coupling asymmetry. In contrast, a comparable symmetric model had identical input resistances at both the soma and dendrite regardless of coupling strength. These findings lead to predictions that variations in dendritic excitability, subsequent to changes in input resistance, might change the current threshold and onset timing of the plateau potential generated in the dendrite. Since the plateau potential underlies bistable firing, these results further predicted that asymmetric coupling might alter nonlinear (i.e. bistable) firing patterns. The numerical simulations supported analytical predictions, showing that the fully bistable firing pattern of the asymmetric model depended on the degree of coupling asymmetry and its correlated dendritic excitability. The physiological property of asymmetric coupling plays an important role in generating and stabilizing the bistability of motor neurons by interacting with the excitability of dendritic branches.',\n",
       " '53e997ccb7602d9701fbe8f5': 'Technological progress has made it possible to interact with computer systems and applications anywhere and any time. It is crucial that these applications are able to adapt to the user, as a person, and to its current situation, whatever that is. Contextual information and a mechanism to reason about it have demonstrated an important potential to provide solutions in this respect. This paper aims at providing an integrated CBR architecture to be used in context-aware systems. It is the result of our work to develop ePH, a system for building dynamic user communities that share public interest information and knowledge that is accessible through always-on, context-aware services.',\n",
       " '53e997ccb7602d9701fbecf5': 'Innate immunity now occupies a central role in immunology. However, artificial immune system models have largely been inspired by adaptive not innate immunity. This paper reviews the biological principles and properties of innate immunity and, adopting a conceptual framework, asks how these can be incorporated into artificial models. The aim is to outline a meta-framework for models of innate immunity.',\n",
       " '53e997ccb7602d9701fbe900': 'User-centred design (UCD) is a type of user interface design in which the needs and desires of users are taken into account at each stage of the design process for a service or product; often for software applications and websites. Its goal is to facilitate the design of software that is both useful and easy to use. To achieve this, you must characterise users’ requirements, design suitable interactions to meet their needs, and test your designs using prototypes and real life scenarios.',\n",
       " '53e997ccb7602d9701fbe909': 'In complex systems, feedback loops can build intricate emergent phenomena, so that a description of the whole system cannot be easily derived from the properties of the individual parts. Here, we propose that inter-molecular frustration mechanisms can provide non-trivial feedback loops which can develop non-trivial specificity amplification. We show that this mechanism can be seen as a more general form of a kinetic proofreading (KP) mechanism, with an interesting new property, namely the ability to tune the specificity amplification by changing the reactants concentrations. This contrasts with the classical KP mechanism in which specificity is a function of only the reaction rate constants involved in a chemical pathway. These results are also interesting because they show that a wide class of frustration models exists that share the same underlining KP mechanisms, with even richer properties. These models can find applications in different areas such as evolutionary biology, immunology, and biochemistry.',\n",
       " '53e997ccb7602d9701fbe90a': 'In this letter, we derive a multiple-symbol differential detection (MSDD) and a novel MSDD-based decision-feedback differential detection (MS-DFDD) receiver for differential space-time modulation transmitted over spatially correlated multiple-input multiple-output fading channels. We show that MS-DFDD outperforms previously proposed DFDD schemes that are based on scalar and vector prediction (SP-DFDD and VP-DFDD). In addition, we prove that at high signal-to-noise ratio (SNR) VP-DFDD is equivalent to SP-DFDD and thus fails to properly exploit the spatial fading correlations.',\n",
       " '53e997ccb7602d9701fbed1f': 'Knowledge engineering, knowledge management and conceptual mod- elling are concerned with representing knowledge of business and organizational domains. These research areas use ontologies for knowledge representation. Ontologies are understood either in the philosophical sense as firm metaphysical commitments or in the looser sense of dictionaries or taxonomies. This paper critically examines the understanding and use of ontologies and knowledge representation languages in information systems (IS) research and application. As ontologies are intended to be conceptualizations of a perceived reality, they should reflect the empirically observed reality. This motivates propos- ing psychology of language as a reference discipline for knowledge engineering and knowledge management. Natural language is argued to reflect the cognitive concepts we use to think about and perceive the world around us. These cognitive concepts are the relevant terms with which to structure and represent knowledge about the world. Psychology of language can provide empirical justification for a particular set of concepts to represent knowledge. This paper draws on psycho-linguistic research to develop a proposal for a system of cognitive structures. This is argued to provide the relevant concepts on which to found knowledge representation schemata for knowledge engineering, knowledge management and conceptual modelling.',\n",
       " '53e997ccb7602d9701fbed2b': 'This paper presents a simple near constant bandwidth amplifier constructed from two operational amplifiers. The near constant bandwidth is obtained by reducing the normally high input impedance of the opamp via local and overall feedback. Experimental results obtained using identical opamps and different opamps verify the expected theoretical results. Copyright © 2009 John Wiley & Sons, Ltd.',\n",
       " '53e997ccb7602d9701fbed3f': 'Group multicast routing problem (GMRP) is a generalization of multicasting whereby every member of the group is allowed to multicast messages to other members from the same group. The routing problem in this case involves the construction of a set of low cost multicast trees with bandwidth requirements for all the group members in the network. The traditional solutions only care for the low cost of multicast trees and sometimes the algorithms will fail during the construction due to the inefficiency of the bandwidth allocation. In this paper we study the feasible solutions to GMRP by proposing a new algorithm to improve the success rate of constructing multicast trees. Simulation results show that our new algorithm performed better in terms of bandwidth utilization and success rate of building multicast trees compared with existing algorithms.',\n",
       " '53e997ccb7602d9701fbed4e': 'The vowels /a, i, u/ spoken by American English talkers with non-pathological voices are described by means of voice source model parameters using the Liljencrants-Fant (LF) model. The sampling frequency of the data is 8 kHz which matches approxi- mately telephone bandwidth. After inverse filtering, trends of voice source characteristics depending on the LF parameters are analyzed and compared to literature and listening results.',\n",
       " '53e997ccb7602d9701fbed5a': 'In this paper we present an innovative haptic device that combines the electro-tactile stimulation with the force and visual feedbacks in order to improve the perception of a virtual world. We discuss the sensation evoked in a user by the haptic, force, and the visual interface as provided by this device, implemented as a special glove, equipped with sensors and actuators connected to a PC. The techniques used to recreate tactile and kinesthetic sensations are based on an innovative use of cutaneous stimulation integrated with actuators and 3D modelling techniques. We discuss about the specificity of haptic interfaces, their controllers, their open problems. We present results about generating the sensation of touching virtual objects with our device. Experiments show also that, using a multi-modal sensorial pattern of stimulation, the subject perceives more realistically the virtual object. We discuss the possible use of the same technique as a way to interface intelligent robots.',\n",
       " '53e997ccb7602d9701fbe953': 'Radio traffic congestion occurs in a radio network when too many users simultaneously transmit on the same channel within close geographical proximity. Such congestion can be prevented by imposing channel loading constraints, which restrict the number of users that can operate using the same channel at one instance in time. Channel loading constraints are a variety of non-binary constraint that can be represented as subsets of transmitters which cannot operate on the same channel. This makes the computational cost in generating and storing them significant.',\n",
       " '53e997ccb7602d9701fbe955': 'In this paper, a novel approach for performing classification is presented. Discriminant functions are constructed by combining selected features from the feature set with simple mathematical functions such as +, −, ×, ÷ , max, min. These discriminant functions are capable of forming non-linear discontinuous hypersurfaces. For multimodal data, more than one discriminant function may be combined with logical operators before classification is performed. An algorithm capable of making decisions as to whether a combination of discriminant functions is needed to classify a data sample, or whether a single discriminant function will suffice, is developed. The algorithms used to perform classification are not written by a human. The algorithms are learnt, or rather evolved, using evolutionary computing techniques.',\n",
       " '53e997ccb7602d9701fbe969': 'This paper considers the linear quadratic (LQ) control problem for the Ito-type stochastic system with input delays. Due to simultaneous appearances of diffusion terms (dependent on the state and the control) as well as delays in the dynamic system, the problem is very involved and remains to be solved. We not only provide the solvable condition of the problem but also the explicit expression of the causal and adapted controller for a kind of LQ problems. All of these are based on a stochastic Riccati equation. The key technique is to pursue the explicit cost value of the LQ problem by FBSDE and derive the analytical controller via the interplay between the original problem and its equivalent abstract description.',\n",
       " '53e997ccb7602d9701fbe973': 'Understanding and ameliorating the effects of network damage are of significant interest, due in part to the variety of applications in which network damage is relevant. For example, the effects of genetic mutations can cascade through within-cell signaling and regulatory networks and alter the behavior of cells, possibly leading to a wide variety of diseases. The typical approach to mitigating network perturbations is to consider the compensatory activation or deactivation of system components. Here, we propose a complementary approach wherein interactions are instead modified to alter key regulatory functions and prevent the network damage from triggering a deregulatory cascade.We implement this approach in a Boolean dynamic framework, which has been shown to effectively model the behavior of biological regulatory and signaling networks. We show that the method can stabilize any single state (e.g., fixed point attractors or time-averaged representations of multi-state attractors) to be an attractor of the repaired network. We show that the approach is minimalistic in that few modifications are required to provide stability to a chosen attractor and specific in that interventions do not have undesired effects on the attractor. We apply the approach to random Boolean networks, and further show that the method can in some cases successfully repair synchronous limit cycles. We also apply the methodology to case studies from drought-induced signaling in plants and T-LGL leukemia and find that it is successful in both stabilizing desired behavior and in eliminating undesired outcomes. Code is made freely available through the software package BooleanNet.The methodology introduced in this report offers a complementary way to manipulating node expression levels. A comprehensive approach to evaluating network manipulation should take an \"all of the above\" perspective; we anticipate that theoretical studies of interaction modification, coupled with empirical advances, will ultimately provide researchers with greater flexibility in influencing system behavior.',\n",
       " '53e997ccb7602d9701fbed91': 'The United States Air Force (USAF) is investigating the use of three levels of repair with its aircraft maintenance managerial structure. This study provides an initial look at the effect of maintenance resource collaboration among maintenance locations and the use of a centralized repair facility focusing on a critical line replacement unit for a major USAF weapon system. Maintenance data for prior year maintenance experiences are collected, fit into appropriate probability distributions and implemented in a discrete event simulation model. This model is then used within an experimental design framework to examine the potential impact of organizational changes to the USAF hierarchical maintenance structure.',\n",
       " '53e997ccb7602d9701fbe9d1': 'The stationary properties of a first-order digital phase-locked loop based on the extended Kalman filter (EKF-PLL) are investigated. A discrete Markov chain approximation of the phase error process is used to derive the asymptotic distribution of the phase error as well as the distribution of the time at which the EKF-PLL is first \"out of lock,\" given that it is in a steady state to begin with. These approximations are compared with large computer simulations.',\n",
       " '53e997ccb7602d9701fbedd7': 'This paper focuses on improving network management by exploiting the potential of \"doing\" of the Active Networks technology, together with the potential of \"planning,\" which is typical of the artificial intelligent systems. We propose a distributed multiagent architecture for Active Network management, which exploits the dynamic reasoning capabilities of the Situation Calculus in order to emulate the reactive behavior of a human expert to fault situations. The information related to network events is generated by programmable sensors deployed across the network. A logical entity collects this information, in order to merge it with general domain knowledge, with a view to identifying the root causes of faults, and to deciding on reparative actions. The logical inference system has been devised to carry out automated isolation, diagnosis, and even repair of network anomalies, thus enhancing the reliability, performance, and security of the network. Experimental results illustrate the Reasoner capability of correctly recognizing fault situations and undertaking management actions.',\n",
       " '53e997ccb7602d9701fbedde': 'Background: Software engineering practices have evolved considerably over the last four decades, changing the way software systems are developed and delivered. Such evolvement may result in improvements in software productivity and changes in factors that affect productivity. Aims: This paper reports our empirical analysis on how changes in software engineering practices are reflected in COCOMO cost drivers and how software productivity has evolved over the years. Method: The analysis is based on the COCOMO data set of 341 software projects developed between 1970 and 2009. We analyze the productivity trends over the years, comparing productivity of different types and countries. To explain the overall impact of cost drivers on productivity and explain its trends, we propose a measure named Difficulty which is based on the COCOMO model and its cost drivers. Results: The results of our analysis indicate that the overall productivity of the projects in the data set has increased noticeably over the last 40 years. Our analysis also shows that the productivity trends and productivity variability can be explained by using the proposed Difficulty measure. Conclusions: Our analysis provides empirical evidence that the productivity trends can be characterized by the improvements in software tools, processes, and platforms among other factors. The Difficulty measure can be used to justify and compare productivity among projects of different characteristics, e.g., different domains, platforms, complexity, and personnel experience. Although we define the measure using the COCOMO cost drivers, it may not fully represent the most important factors influencing productivity. One direction for our future work is to analyze the effectiveness of the measure using more cost drivers on more data points.',\n",
       " '53e997ccb7602d9701fbede3': 'Emotions recognized from Electroencephalogram (EEG) could reflect the real \"inner\" feelings of the human. Recently, research on real-time emotion recognition received more attention since it could be applied in games, e-learning systems or even in marketing. EEG signal can be divided into the delta, theta, alpha, beta, and gamma waves based on their frequency bands. Based on the Valence-Arousal-Dominance emotion model, we proposed a subject-dependent algorithm using the beta/alpha ratio to recognize high and low dominance levels of emotions from EEG. Three experiments were designed and carried out to collect the EEG data labeled with emotions. Sound clips from International Affective Digitized Sounds (IADS) database and music pieces were used to evoke emotions in the experiments. Our approach would allow real-time recognition of the emotions defined with different dominance levels in Valence-Arousal-Dominance model.',\n",
       " '53e997ccb7602d9701fbee0b': 'An approach to the mechanization of the certification of programs is the development of a canonical form for computer programs. Two programs would be declared equivalent, that is, for all inputs they produce identical outputs, if and only if they have the same canonical form. The nature of such a canonical form for computer programs is discussed. For a certain non-trivial class of programs, a methodology for producing the canonical form from the original program is developed, and the present limitations of this approach are explored. Finally, the methodology is applied to two different sort algorithms, demonstrating that these algorithms are equivalent since they possess the same canonical form.',\n",
       " '53e997ccb7602d9701fbee34': 'Intelligent memory is a new class of computer architecture, to reduce the performance gap between the processor and memory. After analyzing a region of application, we decide to take \"statement\" viewpoint to extract more potential benefit of program running on intelligent memory architecture. Then we develop our SAGE system, a \"statement\" base analysis system, different from other iteration base system. In this paper, we will describe how SAGE split statement and make an acceptable schedule to execute on PHost and PMem simultaneously. Finally we will discuss our recently result of this approach.',\n",
       " '53e997ccb7602d9701fbee53': \"We present our predictions for the SAMPL4 hydration free energy challenge. Extensive all-atom Monte Carlo simulations were employed to sample the compounds in explicit solvent. While the focus of our study was to demonstrate well-converged and reproducible free energies, we attempted to address the deficiencies in the general Amber force field force field with a simple QM/MM correction. We show that by using multiple independent simulations, including different starting configurations, and enhanced sampling with parallel tempering, we can obtain well converged hydration free energies. Additional analysis using dihedral angle distributions, torsion-root mean square deviation plots and thermodynamic cycles support this assertion. We obtain a mean absolute deviation of 1.7 kcal mol(-1) and a Kendall's τ of 0.65 compared with experiment.\",\n",
       " '53e997ccb7602d9701fbee7d': \"A systematic method is proposed for automatically identifying positive feedback loops (PFLs) in analog/mixed-signal circuits. The method first converts the netlist of a circuit into a directed dependency graph (DDG) which captures the critical relationships among branch currents and node voltages. It then utilizes graph theory techniques to find all feedback loops from the DDG and finally, criterion are developed to determine the PFLs. Since multiple states is caused by the PFLs, this method could identify the circuit's vulnerability to undesigned operating points only by its structure without the computation of DC solutions. The proposed approach is implemented in program and simulation results show it could identify all the PFLs very robustly.\",\n",
       " '53e997ccb7602d9701fbea47': 'Let X and Y be generic n by n matrices of indeterminates. Let S = k [ x 1 ,…, x r , y 1 ,…, y r ] where k is a field of characteristic 0 and r = n 2 . Let I ⊂ S be the ideal generated by the entries of the matrix XY - YX . We will consider the ring S/I and show that it is Cohen-Macaulay for the case n = 4. In order to calculate its Groebner basis we use a product order with 3 blocks of variables and reverse lexicographic order in each block. This makes the computation much smaller and less time consuming.',\n",
       " '53e997ccb7602d9701fbea4a': 'In this paper, a hybrid recommender system for job seeking and recruiting websites is presented. The various interaction features designed on the website help the users organize the resources they need as well as express their interest. The hybrid recommender system exploits the job and user profiles and the actions undertaken by users in order to generate personalized recommendations of candidates and jobs. The data collected from the website is modeled using a directed, weighted, and multi-relational graph, and the 3A ranking algorithm is exploited to rank items according to their relevance to the target user. A preliminary evaluation is conducted based on simulated data and production data from a job hunting website in Switzerland.',\n",
       " '53e997ccb7602d9701fbee8d': 'This paper describes a pattern-based technique for systematic development of UML models of secure systems using access control. Access control is viewed and specified as a design pattern. An access control pattern is applied to a functional UML model of an application to be secured using a composition algorithm. We demonstrate the technique using mandatory access control (MAC) and a model of a simple file system. We also discuss how the composed model can be evaluated for security assurance expected from the applied access control',\n",
       " '53e997ccb7602d9701fbea65': 'State-transition models are employed to project future prevalence rates of risk factors and diseases within populations. Sensitivity analysis should be performed to assess the reliability of the results but often the number of inputs of the model is so huge, and running the model is so time-consuming, that not all methods of sensitivity analysis are practically available.',\n",
       " '53e997ccb7602d9701fbea66': \"In this paper we address the joint problems of automated data acquisition and view planning for large-scale indoor and outdoor sites. Our method proceeds in two distinct stages. In the initial stage, the system is given a 2-D map with which it plans a minimal set of sufficient covering views. We then use a 3-D laser scanner to take scans at each of these views. When this planning system is combined with our mobile robot, it automatically computes and executes a tour of these viewing locations and acquires the views with the robot's onboard laser scanner. These initial scans serve as an approximate 3-D model of the site. The planning software then enters a second stage in which it updates this model by using a voxel-based occupancy procedure to plan the next best view. This next best view is acquired, and further next best views are sequentially computed and acquired until a complete 3-D model is obtained. Results are shown for Fort Jay on Governors Island in the City of New York and for the church of Saint Menoux in the Bourbonnais region of France. I. INTRODUCTION Dense and detailed 3-D models of large structures can be useful in many fields. These models can allow engineers to analyze the stability of a structure and then test possible corrections without endangering the original. The models can also provide documentation of historical sites in danger of destruction and archaeological sites at various stages of an excavation. With detailed models, professionals and students can tour such sites from thousands of miles away. Modern laser range scanners will quickly generate a dense point cloud of measurements; however, many of the steps needed for 3-D model construction require time-consuming human involvement. These steps include the planning of the viewing locations as well as the acquisition of the data at multiple viewing locations. By automating these tasks, we will ultimately be able to speed this process significantly. A plan must be laid out to determine where to take each individual scan. This requires choosing efficient views that will cover the entire surface area of the structure without occlusions from other objects and without self occlusions from the target structure itself. This is the essence of the so- called view planning problem. Manually choosing the views can be time consuming in itself. Then the scanning sensor must be physically moved from location to location which is also time consuming and physically stressful.\",\n",
       " '53e997ccb7602d9701fbea6c': 'The paper presents a methodology for using computational neurogenetic modelling (CNGM) to bring new original insights into how genes influence the dynamics of brain neural networks. CNGM is a novel computational approach to brain neural network modelling that integrates dynamic gene networks with artificial neural network model (ANN). Interaction of genes in neurons affects the dynamics of the whole ANN model through neuronal parameters, which are no longer constant but change as a function of gene expression. Through optimization of interactions within the internal gene regulatory network (GRN), initial gene/protein expression values and ANN parameters, particular target states of the neural network behaviour can be achieved, and statistics about gene interactions can be extracted. In such a way, we have obtained an abstract GRN that contains predictions about particular gene interactions in neurons for subunit genes of AMPA, GABAA and NMDA neuro-receptors. The extent of sequence conservation for 20 subunit proteins of all these receptors was analysed using standard bioinformatics multiple alignment procedures. We have observed abundance of conserved residues but the most interesting observation has been the consistent conservation of phenylalanine (F at position 269) and leucine (L at position 353) in all 20 proteins with no mutations. We hypothesise that these regions can be the basis for mutual interactions. Existing knowledge on evolutionary linkage of their protein families and analysis at molecular level indicate that the expression of these individual subunits should be coordinated, which provides the biological justification for our optimized GRN.',\n",
       " '53e997ccb7602d9701fbeac7': \"Material handling equipment selection problem (MHESP) is an important decision making area for the companies, since it has a direct effect on manufacturing and service productivity. In this study, an integrated fuzzy multi-criteria decision making methodology for MHESP is proposed. The proposed approach is utilized from fuzzy sets, Analytic Network Process (ANP) and Preference Ranking Organization METHod for Enrichment Evaluations (PROMETHEE) approaches. Evaluation criteria for the MHESP is weighted by fuzzy-ANP (F-ANP) approach, then, alternative material handling equipments are evaluated by fuzzy-PROMETHEE (F-PROMETHEE) approach. The methodology is applied for a manufacturing company to prove its effectiveness. Finally, some sensitivity analyses are conducted to show the results' sensitiveness to the changes of the weights of the evaluation criteria.\",\n",
       " '53e997ccb7602d9701fbeed9': 'In this paper we consider vector quasi-variational inequality problems over product sets (in short, VQVIP). Moreover we study generalizations of this model, namely problems of a system of vector quasi-variational inequalities (in short, SVQVIP), generalized vector quasi-variational inequality problems over product sets (in short, GVQVIP) and problems of a system of generalized vector quasi-variational inequalities (in short, SGVQVIP). We show that every solution of (VQVIP) (respectively, (GVQVIP)) is a solution of (SVQVIP) (respectively, (SGVQVIP)). By defining relatively pseudomonotone and relatively maximal pseudomonotone maps and by employing a known fixed point theorem, we establish the existence of a solution of (VQVIP) and (SVQVIP). These existence results are then used to derive the existence of a solution of (GVQVIP) and (SGVQVIP), respectively, The results of this paper extend recent results in the literature. They are obtained in a more general setting.',\n",
       " '53e997ccb7602d9701fbeadf': 'Computer implementations of theoretical concepts play an ever-increasing role in the development and application of scientific ideas. As the scale of such implementations increases from relatively small models and empirical setups to overarching frameworks from which many kinds of results may be obtained, it is important to consider the methodology by which these implementations are developed. Using cognitive architectures as an example, we discuss the relation between an implementation of an architecture and its underlying theory, a relation between a computer program and its description. We argue for the use of an agile development methodology, based around a three-layer scientific test harness and continuous refactoring, as most suitable for developing scientific software. The ideas are illustrated with extended examples of implementing unified theories of human learning, taken from the chunking and template theories.',\n",
       " '53e997ccb7602d9701fbeaea': 'A wireless sensor network consists of one or more hubs and lots of sensors. Sensors have capability of sensing, wireless communication and data processing. Therefore, how to extend the network lifetime is an important issue. In this paper, we propose a routing protocol which uses the water flow idea and allocates the loading based on the lowest energy in the paths. Our simulation result shows that the approach can extend the lifetime efficiently.',\n",
       " '53e997ccb7602d9701fbef3d': 'This paper argues that, in the management of softwaredevelopment projects involving teleworkers, certainprocesses and base practices need more carefulmanagement than in projects carried out in the traditionalcentralized development environment. Based on theresults of our surveys; the series studies conducted insoftware engineering principles and in managementmethodology of teleworking; a number of key issuesspecific to telework condition have been identified. Anew Software Process Improvement approach forTeleworking Environment (SPITE) has been developedas a major contribution to the software industry, whichfills an important gap in the software process modeling.The focus of this paper is to present the in-depth analysisof some of these issues that are not addressed in thecurrent process models. Three processes and twenty-fivebase practices are created and used to provide majorinput to the development of SPITE.',\n",
       " '53e997ccb7602d9701fbef09': 'This article introduces the Follow-Me Cloud concept and proposes its framework. The proposed framework is aimed at smooth migration of all or only a required portion of an ongoing IP service between a data center and user equipment of a 3GPP mobile network to another optimal DC with no service disruption. The service migration and continuity is supported by replacing IP addressing with service identification. Indeed, an FMC service/application is identified, upon establishment, by a session/service ID, dynamically changing along with the service being delivered over the session; it consists of a unique identifier of UE within the 3GPP mobile network, an identifier of the cloud service, and dynamically changing characteristics of the cloud service. Service migration in FMC is triggered by change in the IP address of the UE due to a change of data anchor gateway in the mobile network, in turn due to UE mobility and/or for load balancing. An optimal DC is then selected based on the features of the new data anchor gateway. Smooth service migration and continuity are supported thanks to logic installed at UE and DCs that maps features of IP flows to the session/service ID.',\n",
       " '53e997ccb7602d9701fbef2c': 'Research on source code understanding has attracted computer scientists for decades. It is known [12] that the code is the functional standard for each computer system. One can assume that most functional errors can automatically be captured from code. In this paper we describe a metatheory for software understanding, with pieces of domino as a metaphor. With the help of the tiles of the game, each of the stages of the data flow in software reverse engineering can be modeled. The last element in this hybrid construction is the maintainer, who plans modifications by using symbolic code information based on previous game movements. This process was evaluated for Java.',\n",
       " '53e997ccb7602d9701fbef3b': 'Energy use is a crucial design concern in wireless ad hoc networks. The design objectives of energy-aware routing include selecting energy-efficient paths and minimizing the protocol overhead incurred in acquiring such paths. To achieve these goals altogether, we present the design of two energy-aware on-demand routing protocols for different network environments. The key idea behind our design is to adaptively select the subset of nodes required to involve in a route-searching process to acquire a high residual-energy path or the degree to which nodes are required to participate in the process of searching for a low-power path for networks wherein nodes can adaptively adjust their transmission power: Analytical and simulation results are given to demonstrate the high performance of the designed protocols in energy-efficient utilization and in reducing the protocol overhead incurred in acquiring energy-aware routes.',\n",
       " '53e997ccb7602d9701fbef8c': 'The steady-state availability of a two-component system in series and parallel subject to individual failures (I-failures) and common-cause shock (CCS) failures is studied from a Bayesian viewpoint with different types of priors assumed for the unknown parameters in the system. Monte Carlo simulation is used to derive the posterior distribution for the steady-state availability and subsequently the highest posterior density intervals. A numerical example illustrates the results.',\n",
       " '53e997ccb7602d9701fbf18b': \"We present a functional approach to parsing unrestricted context-free grammars based on Brzozowski's derivative of regular expressions. If we consider context-free grammars as recursive regular expressions, Brzozowski's equational theory extends without modification to context-free grammars (and it generalizes to parser combinators). The supporting actors in this story are three concepts familiar to functional programmers - laziness, memoization and fixed points; these allow Brzozowski's original equations to be transliterated into purely functional code in about 30 lines spread over three functions. Yet, this almost impossibly brief implementation has a drawback: its performance is sour - in both theory and practice. The culprit? Each derivative can double the size of a grammar, and with it, the cost of the next derivative. Fortunately, much of the new structure inflicted by the derivative is either dead on arrival, or it dies after the very next derivative. To eliminate it, we once again exploit laziness and memoization to transliterate an equational theory that prunes such debris into working code. Thanks to this compaction, parsing times become reasonable in practice. We equip the functional programmer with two equational theories that, when combined, make for an abbreviated understanding and implementation of a system for parsing context-free languages.\",\n",
       " '53e997ccb7602d9701fbf189': 'The Clavinet is an electromechanical musical instrument produced in the mid-twentieth century. As is the case for other vintage instruments, it is subject to aging and requires great effort to be maintained or restored. This paper reports analyses conducted on a Hohner Clavinet D6 and proposes a computational model to faithfully reproduce the Clavinet sound in real time, from tone generation to the emulation of the electronic components. The string excitation signal model is physically inspired and represents a cheap solution in terms of both computational resources and especially memory requirements (compared, e.g., to sample playback systems). Pickups and amplifier models have been implemented which enhance the natural character of the sound with respect to previous work. A model has been implemented on a real-time software platform, Pure Data, capable of a 10-voice polyphony with low latency on an embedded device. Finally, subjective listening tests conducted using the current model are compared to previous tests showing slightly improved results.',\n",
       " '53e997ccb7602d9701fbefc8': 'Over the past few years, we have witnessed a rise in the use of the web for health purposes. Patients have begun to manage their own health data online, use health-related services, search for information, and share it with others. The cooperation of healthcare constituents towards making collaboration platforms available is known today as Health 2.0. The significance of Health 2.0 lies in the transformation of the patient from a healthcare consumer to an active participant in a new environment. We analyze the trend and propose mashups as a leading technology for the integration of relevant data, services, and applications. We present Medic-kIT, a mashup-based patient-centric Extended Personal Health Record system, which adheres to web 2.0 standards. We conclude by highlighting unique aspects that will have to be addressed to enable the development of such systems in the future.',\n",
       " '53e997ccb7602d9701fbefd8': 'Lower-bound functions are crucial for indexing time-series data under dynamic time warping (DTW) distance. In this paper, we propose a unified framework to explain the existing lower-bound functions. Based on the framework, we further propose a group of lower-bound functions for DTW and investigate their performances through extensive experiments. Experimental results show that the new lower-bound functions are better than the existing one in most cases. An index structure based on the new lower-bound functions is also implemented.',\n",
       " '53e997ccb7602d9701fbf1fa': 'In this paper, we present Dynamic Re-keying with Key Hopping (DRKH) encryption protocol that uses RC4 encryption technique to ensure a strong security level with the advantage of low execution cost compared to other IEEE 802.11 security schemes. Low computational complexity makes DRKH suitable for solar- and battery-powered handheld devices such as nodes in Solar ESS (Extended Service Set) and wireless sensor networks. Our design goal is to eventually integrate DRKH with different emerging wireless technologies. However, in this paper, we will focus on the integration of DRKH with 802.11 standard since it is the most widely deployed wireless technology. The results and analysis show that DRKH overcomes all the security threats with Wired Equivalent Privacy (WEP) protocol while consuming a much lower power than WEP, Wi-Fi Protected Access (WPA) 1.0 and WPA 2.0.ag5',\n",
       " '53e997ccb7602d9701fbefec': 'High-throughput methods that allow for measuring the expression of thousands of genes or proteins simultaneously have opened new avenues for studying biochemical processes. While the noisiness of the data necessitates an extensive pre-processing of the raw data, the high dimensionality requires effective statistical analysis methods that facilitate the identification of crucial biological features and relations. For these reasons, the evaluation and interpretation of expression data is a complex, labor-intensive multi-step process. While a variety of tools for normalizing, analysing, or visualizing expression profiles has been developed in the last years, most of these tools offer only functionality for accomplishing certain steps of the evaluation pipeline.Here, we present a web-based toolbox that provides rich functionality for all steps of the evaluation pipeline. Our tool GeneTrailExpress offers besides standard normalization procedures powerful statistical analysis methods for studying a large variety of biological categories and pathways. Furthermore, an integrated graph visualization tool, BiNA, enables the user to draw the relevant biological pathways applying cutting-edge graph-layout algorithms.Our gene expression toolbox with its interactive visualization of the pathways and the expression values projected onto the nodes will simplify the analysis and interpretation of biochemical pathways considerably.',\n",
       " '53e997ccb7602d9701fbeff6': 'It is shown that there is an optimal finite linear combination of B-splines, denominated modified B-splines, such that a pertinent low frequency condition called M −flatness is satis- fied. A profound relationship of the modified B-splines with the Beta distribution implies an asymptotic sampling theorem with exact reconstruction requiring only small oversampling.',\n",
       " '53e997ccb7602d9701fbf003': 'Device scaling trends dramatically increase the susceptibility of microprocessors to soft errors. Further, mounting demand for embedded microprocessors in a wide array of safety critical applications, ranging from automobiles to pacemakers, compounds the importance of addressing the soft error problem. Historically, soft error tolerance techniques have been targeted mainly at high-end server markets, leading to solutions such as coarse-grained modular redundancy and redundant multithreading. However, these techniques tend to be prohibitively expensive to implement in the embedded design space. To address this problem, we first present a thorough analysis of the effects of soft errors on a production-grade, fully synthesized implementation of an ARM926EJ-S embedded microprocessor. We then leverage this analysis in the design of two orthogonal low-costs of terror protection techniques that can be tuned to achieve variable levels of fault coverage as a function of area and power constraints. The first technique uses a small cache of live register values in order to provide nearly twice the fault coverage of a register file protected using traditional error correcting codes at little or no additional area cost. The second technique is a statistical method used to significantly reduce the overhead of deploying time-delayed shadow latches for low-latency fault detection.',\n",
       " '53e997ccb7602d9701fbf00d': 'Protein-protein interactions (PPIs) play a crucial role in initiating infection in a host-pathogen system. Identification of these PPIs is important for understanding the underlying biological mechanism of infection and identifying putative drug targets. Database resources for studying host-pathogen systems are scarce and are either host specific or dedicated to specific pathogens.Here we describe \"HPIDB\" a host-pathogen PPI database, which will serve as a unified resource for host-pathogen interactions. Specifically, HPIDB integrates experimental PPIs from several public databases into a single, non-redundant web accessible resource. The database can be searched with a variety of options such as sequence identifiers, symbol, taxonomy, publication, author, or interaction type. The output is provided in a tab delimited text file format that is compatible with Cytoscape, an open source resource for PPI visualization. HPIDB allows the user to search protein sequences using BLASTP to retrieve homologous host/pathogen sequences. For high-throughput analysis, the user can search multiple protein sequences at a time using BLASTP and obtain results in tabular and sequence alignment formats. The taxonomic categorization of proteins (bacterial, viral, fungi, etc.) involved in PPI enables the user to perform category specific BLASTP searches. In addition, a new tool is introduced, which allows searching for homologous host-pathogen interactions in the HPIDB database.HPIDB is a unified, comprehensive resource for host-pathogen PPIs. The user interface provides new features and tools helpful for studying host-pathogen interactions. HPIDB can be accessed at http://agbase.msstate.edu/hpi/main.html.',\n",
       " '53e997ccb7602d9701fbf022': 'Benchmark problems should be hard. I report on the solution of the five open benchmark problems introduced by Falkenauer in this journal for testing bin packing problems. Since the solutions were found either by hand or by using very simple heuristic methods, these problems would appear to be easy. In four cases I give improved packings to refute conjectures that previously reported packings were optimal, and I give a proof that the fifth conjecture was correct. In some cases this led to implemented heuristic methods which produced better solutions than those reported by Falkenauer and up to 10,000 times faster. Future experimenters should be careful to perform tests on problems that can reasonably be regarded as hard.',\n",
       " '53e997ccb7602d9701fbf21b': 'We study formally the consistency problem, for replicated shared data, in the Action-Constraint framework (ACF). ACF can describe a large range of application semantics and replication protocols, including optimistic and/or partial replication. ACF is used to decompose the consistency problem into simpler sub-problems. Each is easily understood. Existing algorithms from the literature can be explained as combinations of concrete sub-problem implementations. Using ACF, we design a new serialisation algorithm that does not cause aborts and only needs pairwise agreement (not global consensus).',\n",
       " '53e997ccb7602d9701fbf036': 'This paper proposes two model reduction methods for large-scale bidirectional networks that fully utilize a network structure transformation implemented as positive tridiagonalization. First, we present a Krylov-based model reduction method that guarantees a specified error precision in terms of the H-norm. Positive tridiagonalization allows us to derive an approximation error bound for the input-to-state model reduction without computationally expensive operations such as matrix factorization. Second, we propose a novel model reduction method that preserves network topology among clusters, i.e., node sets. In this approach, we introduce the notion of cluster uncontrollability based on positive tridiagonalization, and then derive its theoretical relation to the approximation error. This error analysis enables us to construct clusters that can be aggregated with a small approximation error. The efficiency of both methods is verified through numerical examples, including a large-scale complex network.',\n",
       " '53e997ccb7602d9701fbf087': 'This paper proposes an integrated tool to analyze trend visualization graph called \"FACT-Graph\". FACT-Graph is generated from text data with time stamp and is useful for trend analysis. However, it faces three key problems: First, it is difficult to configure parameters (such as analysis span, exceptive keywords and thresholds) to generate FACT-Graph; Second, a FACT-Graph does not provide the required information and interface for trend analysis because the process of generating the FACT-Graph eliminates that information; and third, it cannot reflect a user\\'s awareness in a FACT-Graph. In order to solve these problems, the authors have developed a tool called \"Loopo\". Loopo integrates a term database, analysis components, and a graph-drawing function and provides users (i.e., analyzers) with information for trend analysis. Loopo also provides an interactive GUI for configuring parameters at ease and to reflect a user\\'s awareness in a FACT-Graph instantly.',\n",
       " '53e997ccb7602d9701fbf08f': 'This paper presents the design, implementation and characterization of an energy--efficient smart power unit for a wireless sensor network with a versatile nano-Watt wake up radio receiver. A novel Smart Power Unit has been developed featuring multi-source energy harvesting, multi-storage adaptive recharging, electrochemical fuel cell integration, radio wake-up capability and embedded intelligence. An ultra low power on board microcontroller performs maximum power point tracking (MPPT) and optimized charging of supercapacitor or Li-Ion battery at the maximum efficiency. The power unit can communicate with the supplied node via serial interface (I2C or SPI) to provide status of resources or dynamically adapt its operational parameters. The architecture is very flexible: it can host different types of harvesters (solar, wind, vibration, etc.). Also, it can be configured and controlled by using the wake-up radio to enable the design of very efficient power management techniques on the power unit or on the supplied node. Experimental results on the developed prototype demonstrate ultra-low power consumption of the power unit using the wake-up radio. In addition, the power transfer efficiency of the multi-harvester and fuel cell matches the state-of-the-art for Wireless Sensor Networks.',\n",
       " '53e997ccb7602d9701fbf093': 'Virtual colon unfolding is a method to produce an unfold image from volume data. The previous method generates only 2D images casting rays from a central path toward the colon surface and maps the color value into 2D regular grid. However, since rays cannot be reached behind folds, some regions are not represented on the final image. In order to represent those areas adequately, we exploit a height field derived from ray casting. Since the problematic areas have high gradient in comparison to neighboring regions, the differences of height value in those areas are relatively large. Therefore, we can find problematic areas using the height field. To visualize those areas, we exploit supersampling method that casts additional rays toward perpendicular direction of the original rays. Experimental results show that our method represents colon folds accurately.',\n",
       " '53e997ccb7602d9701fbf0aa': 'The purpose of this paper is to determine the route of the vehicle routing problem with backhauls (VRPB), delivering new items\\n and picking up the reused items or wastes, and resolve the inventory control decision problem simultaneously since the regular\\n VRPB does not. Both the vehicle routing decision for delivery and pickup, and the inventory control decision affect each other\\n and must be considered together. Hence, a mathematical model of vehicle routing problem with backhauls and inventory (VRPBI)\\n is proposed. Since finding the optimal solution(s) for VRPBI is a NP-hard problem, this paper proposes a heuristic method,\\n variable neighborhood tabu search (VNTS), adopting six neighborhood searching approaches to obtain the optimal solution. Moreover,\\n this paper compares the proposed heuristic method with two other existing heuristic methods. The experimental results indicate\\n that the proposed method is better than the two other methods in terms of average logistic cost (transportation cost and inventory\\n cost).',\n",
       " '53e997ccb7602d9701fbf3a3': 'This study is conducted to establish an alternative, creative technique for the structure of Advanced Encryption Standard-Counter Mode with Cipher Block Chaining Message Authentication Code Protocol (AES-CCMP) key in IEEE 802.11i. the structure of proposed method increase the length of AES-CCMP key from 128 bits to 256 bits to eliminate Time-Memory Trade-Off (TMTO) attacks by using three proposed solutions including Random Nonee(Key), Four Way Handshake alteration and Pseudo Random Function (PRF). Besides, two proposed and classic methods are compared in terms of TMTO attack probability, avalanche effect, changes in neighbor blocks, memory usage and execution time. According to the results, the proposed method is completely resistant to TMTO attack. In addition, avalanche effect and change in neighbor blocks of proposed method are so near to optimized state and also, two classic and proposed methods are approximately the same in case of memory usage and execution time.',\n",
       " '53e997ccb7602d9701fbf465': 'Recent theoretical and experimental investigations of coherent feedback quantum control, the feedback control of a quantum system with another quantum system, has raised the important problem of how to synthesize a class of quantum systems, called the class of linear quantum stochastic systems, from basic quantum optical components and devices in a systematic way. The synthesis theory sought in this case can be naturally viewed as a quantum analogue of linear electrical network synthesis theory and as such has potential for applications beyond the realization of coherent quantum feedback controllers. In earlier work, Nurdin et al. have established that an arbitrary linear quantum stochastic system can be realized as a cascade connection of simpler one degree of freedom quantum harmonic oscillators, together with a direct interaction Hamiltonian which is bilinear in the canonical operators of the oscillators. However, from an experimental perspective and based on current methods and technologies, direct interaction Hamiltonians are challenging to implement for systems with more than just a few degrees of freedom. In order to facilitate more tractable physical realizations of these systems, this technical note develops a new synthesis algorithm for linear quantum stochastic systems that relies solely on field-mediated interactions, including in implementation of the direct interaction Hamiltonian. Explicit synthesis examples are provided to illustrate the realization of two degrees of freedom linear quantum stochastic systems using the new algorithm.',\n",
       " '53e997ccb7602d9701fbf0e0': 'What Does A Body Know? is a concert work for Digital Ventriloquized Actor (DiVA) and sound clips. A DiVA is a real time gesture-controlled formant-based speech synthesizer using a Cyberglove®, touchglove, and Polhemus Tracker® as the main interfaces. When used in conjunction with the performer\\'s own voice solos and \"duets\" can be performed in real time.',\n",
       " '53e997ccb7602d9701fbf141': 'We have recently proposed a general approach to engineering protective wrappers as a means of detecting errors or unwanted behaviour in systems employing an OTS (Off-The-Shelf) item, and launching appropriate recovery actions. This paper presents results of a case study in protective wrapper development, using a Simulink model of a steam boiler system together with an OTS PID (Proportional, Integral and Derivative) controller. The protective wrappers are developed for the model of the system in such a way that they allow detection and tolerance of typical errors caused by unavailability of signals, violations of constraints, and oscillations.',\n",
       " '53e997ccb7602d9701fbf151': 'Listeners show remarkable flexibility in Processing variation in speech signal. One striking example is the ease with which they adapt to novel speech distortions such as listening to someone with a foreign accent. Behavioural studies suggest that significant improvements in comprehension Occur rapidly - often within 10-20 sentences. In the present experiment, we investigate the neural changes underlying on-line adaptation to distorted speech using time-compressed speech. Listeners performed a sentence verification task on normal-speed and time-compressed sentences while their neural responses were recorded using fMRI. The results showed that rapid learning of the time-compressed speech occurred during presentation of the first block of 16 sentences and was associated with increased activation in left and right auditory association cortices and in left ventral premotor Cortex. These findings suggest that the ability to adapt to a distorted speech signal may, in part, rely on mapping novel acoustic patterns onto existing articulatory motor plans, consistent with the idea that speech perception involves integrating multi-modal information including auditory and motoric cues. (C) 2009 Elsevier Inc. All rights reserved.',\n",
       " '53e997ccb7602d9701fbf4a9': 'Optical Packet Switched Metropolitan Area Networks (OPS MAN) are among the most promising solutions for Next Generation MAN architectures. As far as the network synchronization and the packet format are concerned, compared to an asynchronous MAN that supports packets of variable size, a synchronous network with large fixed-size packets offers a significant gain in the network throughput. It avoids bandwidth fragmentation and reduces the number of generated optical headers [1]. In such systems, client packets of variable size are aggregated and accommodated into optical fixed-size containers (fixed-size packets). In this paper, we show how delay constraints and the lack of segmentation mechanism may lead to the creation of optical fixed-size containers which are only partially filled with client packets. When optical containers pass intermediate without O/E/O conversion, the remaining unfilled space in such containers constitutes a wasted amount of bandwidth. Therefore, we propose a novel mechanism that improves the filling-ratio of optical containers. Our algorithm (so called DCUM for Dynamic CoS-Upgrade Mechanism) is based on the use of timers, which values change dynamically, in order to create containers with high filling ratio while limiting the time needed for their creation. We investigate the performance of our algorithm through simulation works. Our experiments are performed on an Optical MAN network with a ring topology. Numerical results show that, compared to existing solutions, DCUM provides optical containers with high filling ratios, and thus keeps the network performance (in terms of packet loss ratio and mean access delay) at safe-levels, regardless to the network load and the timeslot duration (transmission time of one optical container).',\n",
       " '53e997ccb7602d9701fbf4b2': 'Water resources and aquatic ecosystems are facing increasing threats from climate change, improper waste disposal, and oil spill incidents. It is of great interest to deploy mobile sensors to detect and monitor certain diffusion processes (e.g., chemical pollutants) that are harmful to aquatic en-vironments. In this paper, we propose an accuracy-aware diffusion process profiling approach using smart aquatic mobile sensors such as robotic fish. In our approach, the robotic sensors collaboratively profile the characteristics of a diffusion process including source location, discharged substance amount, and its evolution over time. In particular, the robotic sensors reposition themselves to progressively improve the profiling accuracy. We formulate a novel movement scheduling problem that aims to maximize the profiling accuracy subject to limited sensor mobility and energy budget. We develop an efficient greedy algorithm and a more complex near-optimal radial algorithm to solve the problem. We conduct extensive simulations based on real data traces of robotic fish movement and wireless communication. The results show that our approach can accurately profile dynamic diffusion processes under tight energy budgets. More-over, a preliminary evaluation based on the implementation on TelosB motes validates the feasibility of deploying our movement scheduling algorithms on mote-class robotic sensor platforms.',\n",
       " '53e997ccb7602d9701fbf4bf': 'Facial expressions of a person representing similar emotion are not always unique. Naturally, the facial features of a subject taken from different instances of the same emotion have wide variations. In the presence of two or more facial features, the variation of the attributes together makes the emotion recognition problem more complicated. This variation is the main source of uncertainty in the emotion recognition problem, which has been addressed here in two steps using type-2 fuzzy sets. First a type-2 fuzzy face space is constructed with the background knowledge of facial features of different subjects for different emotions. Second, the emotion of an unknown facial expression is determined based on the consensus of the measured facial features with the fuzzy face space. Both interval and general type-2 fuzzy sets (GT2FS) have been used separately to model the fuzzy face space. The interval type-2 fuzzy set (IT2FS) involves primary membership functions for m facial features obtained from n-subjects, each having l-instances of facial expressions for a given emotion. The GT2FS in addition to employing the primary membership functions mentioned above also involves the secondary memberships for individual primary membership curve, which has been obtained here by formulating and solving an optimization problem. The optimization problem here attempts to minimize the difference between two decoded signals: the first one being the type-1 defuzzification of the average primary membership functions obtained from the n-subjects, while the second one refers to the type-2 defuzzified signal for a given primary membership function with secondary memberships as unknown. The uncertainty management policy adopted using GT2FS has resulted in a classification accuracy of 98.333% in comparison to 91.667% obtained by its interval type-2 counterpart. A small improvement (approximately 2.5%) in classification accuracy by IT2FS has been attained by pre-processing measurements using the well-known interval approach.',\n",
       " '53e997ccb7602d9701fbf4cd': 'The information about cultural heritage artifacts that archeologists must manage is usually very heterogenous, and, due to its spatial nature, cannot be easily represented using conventional data management frameworks. The strong spatial dependence of this data suggests that the information should be linked to a 3D model of the artifact. This article presents a 3D information system that has been designed to manage cultural heritage information. The system allows information layers to be associated with the surface of the artifact, following an approach similar to that used in geographical information systems. This permits relationships between the different elements to be ascertained, and allows both specialists and the layperson to more easily understand the information. We describe here the structure and functionality of the system.',\n",
       " '53e997ccb7602d9701fbf50d': \"Raising the level of abstraction in system design promises to enable faster exploration of the design space at early stages. While scheduling decision for embedded software has great impact on system performance, it's much desired that the designer can select the right scheduling algorithm at high abstraction levels so as to save him from the error-prone and time consuming task of tuning code delays or task priority assignments at the final stage of system design. In this paper we tackle this problem by introducing a RTOS model and an approach to refine any unscheduled transaction level model (TLM) to a TLM with RTOS scheduling support. The refinement process provides a useful tool to the system designer to quickly evaluate different dynamic scheduling algorithms and make the optimal choice at an early stage of system design.\",\n",
       " '53e997ccb7602d9701fbf517': 'This paper describes a novel speech analysis method that creates a readable pattern based on locally linear embedding (LLE). LLE is an unsupervised learning algorithm for feature extraction. If the speech variability is described by a small number of continuous features, then we can imagine the data as lying on a low dimensional manifold in the high dimensional space of speech waveforms. The goal of feature extraction is to reduce the dimensionality of the speech signal while preserving the informative signatures. In this paper we have present results from the analysis of speech data using PCA and LLE. And we observed that the nonlinear embeddings of LLE separated certain Chinese phonemes better than the linear projections of PCA.',\n",
       " '53e997ccb7602d9701fbf52c': 'The research discussed here is a component of a larger study to explore the accessibility and usability of spatial data presented through multiple sensory modalities including haptic, auditory, and visual interfaces. Geographical Information Systems (GIS) and other computer-based tools for spatial display predominantly use vision to communicate information to the user, as sight is the spatial sense par excellence. Ongoing research is exploring the fundamental concepts and techniques necessary to navigate through multimodal interfaces, which are user, task, domain, and interface specific. This highlights the necessity for both a conceptual / theoretical schema, and the need for extensive usability studies. Preliminary results presented here exploring feature recognition, and shape tracing in non-visual environments indicate multimodal interfaces have a great deal of potential for facilitating access to spatial data for blind and visually impaired persons. The research is undertaken with the wider goals of increasing information accessibility and promoting \"universal access\".',\n",
       " '53e997ccb7602d9701fbf2fb': 'We present a trainable sequential-inference technique for processes with large state and observation spaces and relational structure. Our method assumes \"reliable observations\", i.e. that each process state persists long enough to be reliably inferred from the observations it generates. We introduce the idea of a \"state-inference function\" (from observation sequences to underlying hidden states) for representing knowledge about a process and develop an efficient sequential-inference algorithm, utilizing this function, that is correct for processes that generate reliable observations consistent with the state-inference function. We describe a representation for state-inference functions in relational domains and give a corresponding supervised learning algorithm. Experiments, in relational video interpretation, show that our technique provides significantly improved accuracy and speed relative to a variety of recent, hand-coded, non-trainable systems.',\n",
       " '53e997ccb7602d9701fbf2f1': 'The increased popularity of hybrid intelligent systems in recent times lies to the extensive success of these systems in many real-world complex problems. The main reason for this success seems to be the synergy derived by the computational intelligent components, such as machine learning, fuzzy logic, neural networks and genetic algorithms. Each of these methodologies provides hybrid systems with complementary reasoning and searching methods that allow the use of domain knowledge and empirical data to solve complex problems. In this paper, we briefly present most of those computational intelligent combinations focusing in the development of intelligent systems for the handling of problems in real-world applications. We emphasize the appropriateness of hybrid computational intelligence techniques for dealing with specific problems, we try to point particularly suitable areas of application for different combinations of intelligent techniques and we briefly state advantages and disadvantages of the \"hybrid\" idea, seen as the next theoretical step in the evolving impact and success of artificial intelligence tools and techniques.',\n",
       " '53e997ccb7602d9701fbf59c': 'Proliferation of network access in the age of the internet has enabled information and knowledge sharing to an extent that was beyond thought a few years ago. In this paper we discuss the impacts of two heavily intertwined trends that have emerged, especially during the past few years. On the one hand, an increasing number of business-to-customer relationships almost requires customers to be online savvy. On the other, the internet has enabled alternative information dissemination channels where information can be published bypassing traditional media control instances. Online communities have shown themselves to be social settings in which effective information and knowledge sharing can be observed. In this paper we illustrate some key aspects of the potential power of online communities and we argue that knowing about relevant online activities is becoming an increasingly important aspect of knowledge management in the e-business era.',\n",
       " '53e997ccb7602d9701fbf5a7': 'Modern computer systems for distributed service computing become highly complex and difficult to manage. A selfadaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate to a dynamically changing environment. The main objective of this paper is to develop an architecture-based self-adaptive framework to improve performance and resource efficiency of a server while maintaining reliable services. The target problem is distributed and concurrent systems. This paper proposes a Self-Adaptive Framework for Concurrency Architecture (SAFCA) that includes multiple concurrency architectural patterns or alternatives. The framework has monitoring and managing capabilities that can invoke another architectural alternative at run-time to cope with increasing demands or for reliability purpose. Two control mechanisms have been developed: SAFCA-Q and SAFCA-R. With SAFCA-Q, the system does not need to be statically configured for the highest workloads; hence, resource usage becomes more efficient in normal conditions and the system still is able to handle busty demands. SAFCA-R is used to improve reliability in the case of a failure by conducting a switchover to another software architecture. Experiment results demonstrate that the performance of SAFCA-Q is better than systems using only standalone concurrency architecture and resources are also better utilized. SAFCA-R also shows fast recovery in the face of a failure.',\n",
       " '53e997ccb7602d9701fbf5ad': 'A set of vertices S in a graph G is a routing set if it ensures some kind of connectivity between all pairs of vertices outside of S. Additional constraints may apply; a connected dominating set, for instance, is a special case of a routing set. We determine the size of a minimum routing set in subgraphs of the integer lattice, as well as (asymptotically) for the lattice itself.',\n",
       " '53e997ccb7602d9701fbf5ba': 'The paper addresses a commonly encountered motion blur problem in photographic images, when there is relative motion between the camera and the object being captured. When a photograph is taken in low light conditions or of a fast moving object, motion blur can cause significant degradation of the image. Both the moving object and camera shake contribute to this blurring. The overall approach comprises of taking a standard (non-blurred) image, creating a known blurring function (point spread function-PSF) and then filtering the image with this function so as to add blur into it. This image is further corrupted by different amount of additive Gaussian noise. The aim is to deblur this image by various deblurring algorithms viz., direct and pseudo-inverse filtering, Wiener and parametric Wiener filtering, constrained least squares filtering and Richardson-Lucy algorithm, then analyze and compare their properties. Experimental evaluation is carried out in MATLAB environment on standard lena and cameraman images and these methods are compared in a variety of blur and noise conditions. Both qualitative and quantitative assessment based on popular performance metrics in image processing i. e., peak signal-to-noise ratio (PSNR) and mean squared error (MSE), provides an objective and subjective standards to compare the deblurring methods.',\n",
       " '53e997ccb7602d9701fbf3c3': \"Nowadays local search services are essential to provide access to geographic entities and to satisfy users' spatial information needs. While the users of these services can look for the entities of interest on map interface via sequential search or browsing of individual categories, the visualization of multiple categories simultaneously is still not well supported. This limits the end users on abstracted view, exploration, and comparison of spatial areas with respect to multiple criteria of interests. In this research we investigate the end-user interaction for multi-criteria local search with two popular representations such as aggregated pixel and icon based visualizations. We present a grid-based interactive interface where users can select multiple criteria of interests and explore the relevant spatial regions via aggregated heatmap visualizations. We evaluate the design against icon/marker based visualization, which is an easy adaption of current commercial local search interfaces. We found that heatgrid visualization for local search performs as good as the more popular marker based interface. We report our findings on both visualizations regarding several user-centered aspects such as exploration ability, information overload and cognitive demand.\",\n",
       " '53e997ccb7602d9701fbf62d': 'This paper proposes LEAP, a simple framework for Enterprise Architecture (EA) that views an organization as an engine that executes in terms of hierarchically decomposed communicating components. The approach allows all aspects of the architecture to be precisely defined using standard modelling notations. Given that the approach is simple and precisely defined it can form the basis for a wide range of EA analysis techniques including simulation, compliance and consistency checking. The paper defines the LEAP framework and shows that it can be used to represent the key features of ArchiMate whilst containing fewer orthogonal concepts. We also show that the precision of LEAP, achieved through the use of OCL, can be used to verify both the claims made for inter-layer relationships in EA models and for extensions to ArchiMate.',\n",
       " '53e997ccb7602d9701fbf626': '\\n Technology enhanced learning (TEL) aims to design, develop and test socio-technical innovations that will support and enhance\\n learning practices of both individuals and organisations. It is therefore an application domain that generally covers technologies\\n that support all forms of teaching and learning activities. Since information retrieval (in terms of searching for relevant\\n learning resources to support teachers or learners) is a pivotal activity in TEL, the deployment of recommender systems has\\n attracted increased interest. This chapter attempts to provide an introduction to recommender systems for TEL settings, as\\n well as to highlight their particularities compared to recommender systems for other application domains.\\n \\n ',\n",
       " '53e997ccb7602d9701fbf3e1': 'The lexicographic multi-objective optimization problem (r-LMOP) with r (sum or bottleneck) objectives optimizes a first objective, then as far as a choice remains, a second one, then a third one and so on. A general solution scheme from literature is based on scaling; for the case that only sum criteria are involved the complexity is O(rlogn)T(n,m) with T(n,m) the complexity of optimization on combinatorial (sum) problem instances with m arcs and n nodes. For cases with at least one bottleneck criterion the complexity is at least O(rnlog2n)T(n,m). We describe a solution scheme that is suited to solve the shortest path r-LMOP, the minimum spanning tree r-LMOP and the linear assignment r-LMOP; the complexity is in all cases O(r)T(n,m).',\n",
       " '53e997ccb7602d9701fbf3e6': 'This paper is concerned with the exponential stability analysis problem for a class of uncertain stochastic neural networks with Markovian switching. The parameter uncertainties are assumed to be norm bounded. Based on Lyapunov---Krasovskii stability theory and the nonnegative semimartingale convergence theorem, delay-dependent and delay- independent sufficient stability conditions are established. It is also shown that the result in this paper cover some recently published works. Two examples are provided to demonstrate the usefulness of the proposed criteria.',\n",
       " '53e997ccb7602d9701fbf40b': 'We investigate the performance of simple cognitive personal area networks (CPANs) with cooperative sensing. Nodes are equipped with small buffers of capacity K each, and each node is allowed to transmit a batch of up to μ packets in one transmission cycle. Upon transmission, each node must support the operation of the CPAN by performing sensing duty in the amount obtained by multiplying the number of packets sent in a batch by a variable penalty coefficient. We model this system and show the relationship between values of design parameters and piconet performance. We also show that performance and bandwidth utilization can be improved by a simple technique of dynamically adjusting the duration of the superframe to cater to instantaneous traffic volume. Copyright © 2011 John Wiley & Sons, Ltd.',\n",
       " '53e997ccb7602d9701fbf658': 'A fast and accurate statistical method that estimates at gate level the leakage power consumption of CMOS digital circuits is demonstrated. Means, variances and correlations of logic gate leakages are extracted at library characterization step, and used for subsequent circuit statistical computation. In this paper, the methodology is applied to an eleven thousand cells ST test IP. The circuit leakage analysis computation time is 400 times faster than a single fast-Spice corner analysis, while providing coherent results.',\n",
       " '53e997ccb7602d9701fbf428': 'Nowadays, the prevalence of multimedia applications mandates that wireless networks provide higher throughput and increased spatial efficiency. Aimed primarily at achieving this goal, the first IEEE 802.11n draft was recently approved. In the draft, MIMO and frame aggregation are utilized to improve the PHY and MAC layers in single-hop WLANs by achieving high raw rates and goodput. However, in the context of multi-hop wireless ad hoc networks, the hidden terminal, exposed terminal, and deafness problems remain unsolved by IEEE 802.11n. Consequently, this greatly degrades the MAC efficiency in multi-hop wireless ad hoc networks, even in the presence of frame aggregation. In this paper, we propose a novel MAC scheme that utilizes MIMO distributed spatial multiplexing to solve all the aforementioned problems, resulting in high throughput and fairness in multi-hop wireless ad hoc networks. Moreover, our scheme can be integrated with frame aggregation to further enhance the network performance.',\n",
       " '53e997ccb7602d9701fbf445': 'In this paper we consider the coverage problem for target detection applications in wireless sensor networks. Unlike conventional coverage problems which assume sensing regions are disks around sensors, we define the sensing region according to detection constraints in terms of false alarm probability and missing probability. We show that exploiting cooperation between sensors can extend the overall sensing region while maintain the same constraints on false alarm probability and missing probability. We then propose an energy efficient cooperative detection scheme and study the trade-offs on energy consumption between cooperative and non-cooperative schemes. The cooperative scheme can use half the number of sensors to monitor the whole field compared to disk model in networks deployed on grids. We also study the communication overheads incurred by the cooperative scheme, and show that only cooperation between limited number of nearby sensors is profitable in terms of energy consumption. In our simulations on randomly deployed networks, cooperation reduces the number of sensors to cover the area by 30% and nearly doubles the number of disjoint sensor sets where each can fully cover the area. Appropriately trading off energy consumption with coverage extension, our cooperative detection scheme can increase the network lifetime by nearly 70%.',\n",
       " '53e997ccb7602d9701fbf447': \"In the increasingly competitive credit industry, one of the most interesting and challenging problems is how to manage existing customers. Behavior scoring models have been widely used by financial institutions to forecast customer's future credit performance. In this paper, a hybrid GA+SVM model, which uses genetic algorithm (GA) to search the promising subsets of features and multi-class support vector machines (SVM) to make behavior scoring prediction, is presented. A real life credit data set in a major Chinese commercial bank is selected as the experimental data to compare the classification accuracy rate with other traditional behavior scoring models. The experimental results show that GA+SVM can obtain better performance than other models.\",\n",
       " '53e997ccb7602d9701fbf6a1': 'Hybrid Tutoring, Simulation-Based Learning, Assessment, Military.',\n",
       " '53e997ccb7602d9701fbf4e1': 'This paper introduces appropriate concepts of input-to-state stability (ISS) and integral-ISS for impulsive systems, i.e., dynamical systems that evolve according to ordinary differential equations most of the time, but occasionally exhibit discontinuities (or impulses). We provide a set of Lyapunov-based sufficient conditions for establishing these ISS properties. When the continuous dynamics are ISS, but the discrete dynamics that govern the impulses are not, the impulses should not occur too frequently, which is formalized in terms of an average dwell-time (ADT) condition. Conversely, when the impulse dynamics are ISS, but the continuous dynamics are not, there must not be overly long intervals between impulses, which is formalized in terms of a novel reverse ADT condition. We also investigate the cases where (i) both the continuous and discrete dynamics are ISS, and (ii) one of these is ISS and the other only marginally stable for the zero input, while sharing a common Lyapunov function. In the former case, we obtain a stronger notion of ISS, for which a necessary and sufficient Lyapunov characterization is available. The use of the tools developed herein is illustrated through examples from a Micro-Electro-Mechanical System (MEMS) oscillator and a problem of remote estimation over a communication network.',\n",
       " '53e997ccb7602d9701fbf4fb': '<P>This note considers the problem of locating an absolute 2-median on an undirected tree network having discrete nodal demands as well as a continuum of link demands. The service facilities are assumed to have known finite capacities which sum to the total demand on the network. For this capacitated, balanced case, we derive certain necessary optimality conditions which admit useful localization results. These results permit us to identify and characterize a reduced, finite set of candidate solutions which may be compared to obtain an optimal 2-median for the problem.</P>',\n",
       " '53e997ccb7602d9701fbf4ee': 'In this paper, we show how by a very simple modification of bivariate spline discrete quasi-interpolants, we can construct a new class of quasi-interpolants which have remarkable properties such as high order of regularity and polynomial reproduction. More precisely, given a spline discrete quasi-interpolation operator Qd, which is exact on the space Pm of polynomials of total degree at most m, we first propose a general method to determine a new differential quasi-interpolation operator QrD which is exact on Pm+r. QrD uses the values of the function to be approximated at the points involved in the linear functional defining Qd as well as the partial derivatives up to the order r at the same points. From this result, we then construct and study a first order differential quasi-interpolant based on the C1 cubic B-spline on the equilateral triangulation with a hexagonal support. When the derivatives are not available or extremely expensive to compute, we approximate them by appropriate finite differences to derive new discrete quasi-interpolants Q̃d. We estimate with small constants the quasi-interpolation errors f−QrD[f] and f−Q̃d[f] in the infinity norm. Finally, numerical examples are used to analyze the performance of the method.',\n",
       " '53e997ccb7602d9701fbf700': 'We report on an extensive series of behavioral experiments in which 36 human subjects collectively build a communication network over which they must solve a competitive coordination task for monetary compensation. There is a cost for creating network links, thus creating a tension between link expenditures and collective and individual incentives. Our most striking finding is the poor performance of the subjects, especially compared to our long series of prior experiments. We demonstrate that the subjects built difficult networks for the coordination task, and compare the structural properties of the built networks to standard generative models of social networks. We also provide extensive analysis of the individual and collective behavior of the subjects, including free riding and factors influencing edge purchasing decisions.',\n",
       " '53e997ccb7602d9701fbf714': 'This study aimed to reduce reliance on large training datasets in support vector machine (SVM)-based clinical text analysis by categorizing keyword features. An enhanced Mayo smoking status detection pipeline was deployed. We used a corpus of 709 annotated patient narratives. The pipeline was optimized for local data entry practice and lexicon. SVM classifier retraining used a grouped keyword approach for better efficiency. Accuracy, precision, and F-measure of the unaltered and optimized pipelines were evaluated using k-fold cross-validation. Initial accuracy of the clinical Text Analysis and Knowledge Extraction System (cTAKES) package was 0.69. Localization and keyword grouping improved system accuracy to 0.9 and 0.92, respectively. F-measures for current and past smoker classes improved from 0.43 to 0.81 and 0.71 to 0.91, respectively. Non-smoker and unknown-class F-measures were 0.96 and 0.98, respectively. Keyword grouping had no negative effect on performance, and decreased training time. Grouping keywords is a practical method to reduce training corpus size.',\n",
       " '53e997ccb7602d9701fbf738': 'The present work is focused on a global image characterization based on a description of the 2D displacements of the different shapes present in the image, which can be employed for CBIR applications.To this aim, a recognition system has been developed, that detects automatically image ROIs containing single objects, and classifies them as belonging to a particular class of shapes.In our approach we make use of the eigenvalues of the covariance matrix computed from the pixel rows of a single ROI. These quantities are arranged in a vector form, and are classified using Support Vector Machines (SVMs). The selected feature allows us to recognize shapes in a robust fashion, despite rotations or scaling, and, to some extent, independently from the light conditions.Theoretical foundations of the approach are presented in the paper, together with an outline of the system, and some preliminary experimental results.',\n",
       " '53e997ccb7602d9701fbf73f': 'In this paper, we present a rigorous theoretical formulation of the fundamental problem—indirect illumination from area sources via curved ideal specular surfaces. Intensity and area factors are introduced to clarify this problem and to rectify the radiance from these specular surfaces. They take surface geometry, such as Gaussian curvature, into account. Based on this formulation, an algorithm for integrating ideal specular transfers into global illumina- tion is also presented. This algorithm can deal with curved specular reflectors and transmitters. An implementation is described based on wavefront tracing and progressive radiosity. Sample images generated by this method are presented.',\n",
       " '53e997ccb7602d9701fbf74e': 'Multiple-input multiple-output (MIMO) systems with hybrid automatic-repeat-request (HARQ) promises high throughput with high reliability. However, combining-scheme design for such systems faces challenges, the presence of interference and the existence of multiple signal-to-interference-and-noise power ratios (SINRs). To overcome these challenges, this paper suggests to design combining schemes with the objective of directly optimizing the log-likelihood ratio (LLR) values. Using this approach, this paper proposes several combining schemes and then analyzes them based on three key design factors: decoding performance, scalability, and memory requirement. Computer simulations under the IEEE 802.16e standard settings show the performances of the proposed combining schemes, which align well with the analysis. Based on the analyses and the simulation results, this paper suggests preferable combining schemes respectively for MIMO systems with HARQ-Chase combining (HARQ-CC) and HARQ-incremental redundancy (HARQ-IR).',\n",
       " '53e997ccb7602d9701fbf774': 'The Real-Time CORBA and minimumCORBA specifications are important steps towards defining standard-based middleware which can satisfy real-time requirements in an embedded system. To reach a broad acceptance in the real-time and embedded community, these specifications have to facilitate the utilization of traditional real-time networks for embedded systems. The Controller Area Network (CAN) is one of the most important networks in the field of real-time embedded systems. Consequently, this paper presents a CAN-based connection-oriented communication model and its integration into Real-Time CORBA. In order to make efficient use of the advantages of CAN, we present a new inter-ORB protocol, which uses smaller message headers for CAN and maps the CAN priorities to bands of CORBA priorities. We also present design and implementation details and evaluate the performance of the new inter-ORB protocol.',\n",
       " '53e997ccb7602d9701fbf785': 'This paper discusses RDF related work in the context of OpenLink Vir- tuoso, a general purpose relational / federated database and applications platform. We discuss adapting a relational engine for native RDF support with dedicated data types, bitmap indexing and SQL optimizer techniques. We further discuss mapping existing relational data into RDF for SPARQL access without converting the data into physical triples. We present conclusions and metrics as well as a number of use cases, from DBpedia to bio informatics and collaborative web applications.',\n",
       " '53e997ccb7602d9701fbf786': \"We generalized Daubechies' (see IEEE Transactions on Information Theory, vol.36, no.5, p.961-1004, 1990) results on 1D wavelets to 2D, and elucidated the conditions in which a family of Gabor wavelets will provide a complete representation of an image. We found that the phase space (scale, orientation, x, y) sampling density of the striate cortical simple cells is sufficiently dense to form a tight wavelet frame. This tight frame approximates the continuous wavelet transform, and its redundancy allows high resolution information to be represented in the low-resolution firings of the simple cells. We presented some experimental results to demonstrate this idea\",\n",
       " '53e997ccb7602d9701fbf789': 'In the first part of this paper we address several weighted satisfiability problems. Among others, we provide linear time algorithms solving the optimization problems MINV(MAXV)-NAESAT and MINV (MAXV)-XSAT for 2CNF formulas and arbitrary real weights assigned to the variables. In a second part we consider the relationship between the problems maximum weight independent set (MAX-IS) in a graph and the problem XSAT. We show that the counting problem #XSAT can be solved in time O(20.40567n) thereby significantly improving on a bound O(20.81131n) provided in [4].',\n",
       " '53e997ccb7602d9701fbf7a3': 'The Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) began as an alternative to distributed searching of scholarly eprint repositories. The model embraced by the OAI-PMH is that of metadata harvesting, where value-added services (by a \"service provider\") are constructed on cached copies of the metadata extracted from the repositories of the harvester\\'s choosing. While this model dispenses with the well known problems of distributed searching, it introduces the problem of synchronization. Stated simply, this problem arises when the service provider\\'s copy of the metadata does not match the metadata currently at the constituent repositories. We define some metrics for describing the synchronization problem in the OAI-PMH. Based on these metrics, we study the synchronization problem of the OAI-PMH framework and propose several approaches for harvesters to implement better synchronization. In particular, if a repository knows its update frequency, it can publish it in an OAI-PMH Identify response using an optional About container that borrows from RDF Site Syndication (RSS) Format.',\n",
       " '53e997ccb7602d9701fbf7b1': 'Given the ever-increasing size of supercomputers, fault resilience and the ability to tolerate faults have become more of a necessity than an option. Checkpoint-Restart protocols have been widely adopted as a practical solution to provide reliability. However, traditional checkpointing mechanisms suffer from heavy I/O bottleneck while dumping process snapshots to a shared filesystem. In this context, we study the benefits of data staging, using a proposed hierarchical and modular data staging framework which reduces the burden of checkpointing on client nodes without penalizing them in terms of performance. During a checkpointing operation in this framework, the compute nodes transmit their process snapshots to a set of dedicated staging I/O servers through a high-throughput RDMA-based data pipeline. Unlike the conventional checkpointing mechanisms that block an application until the checkpoint data has been written to a shared filesystem, we allow the application to resume its execution immediately after the snapshots have been pipelined to the staging I/O servers, while data is simultaneously being moved from these servers to a backend shared filesystem. This framework eases the bottleneck caused by simultaneous writes from multiple clients to the underlying storage subsystem. The staging framework considered in this study is able to reduce the time penalty an application pays to save a checkpoint by 8.3 times.',\n",
       " '53e997ccb7602d9701fbf7f2': 'The nature of specific protein-nucleic acid interaction between restriction endonucleases (RE) and their recognition sequences (RS) was studied by bioinformatics methods. It was found that the frequency of 5-6 residue long RS-like oligonucleotides is unexpectedly high in the nucleic acid sequence of the corresponding RE (p',\n",
       " '53e997ccb7602d9701fbf7fa': 'Reading is one of the most well-studied visual activities. Vision research traditionally focuses on understanding the perceptual and cognitive processes involved in reading. In this work we recognize reading activity by jointly analyzing eye and head movements of people in an everyday environment. Eye movements are recorded using an electrooculography (EOG) system; body movements using body-worn inertial measurement units. We compare two approaches for continuous recognition of reading: String matching (STR) that explicitly models the characteristic horizontal saccades during reading, and a support vector machine (SVM) that relies on 90 eye movement features extracted from the eye movement data. We evaluate both methods in a study performed with eight participants reading while sitting at a desk, standing, walking indoors and outdoors, and riding a tram. We introduce a method to segment reading activity by exploiting the sensorimotor coordination of eye and head movements during reading. Using person-independent training, we obtain an average precision for recognizing reading of 88.9&percnt; (recall 72.3&percnt;) using STR and of 87.7&percnt; (recall 87.9&percnt;) using SVM over all participants. We show that the proposed segmentation scheme improves the performance of recognizing reading events by more than 24&percnt;. Our work demonstrates that the joint analysis of eye and body movements is beneficial for reading recognition and opens up discussion on the wider applicability of a multimodal recognition approach to other visual and physical activities.',\n",
       " '53e997ccb7602d9701fbf806': 'Attribute grammars are a powerful specification paradigm for many language processing tasks, particularly semantic analysis of programming languages. Recent attribute grammar systems use dynamic scheduling algorithms to evaluate attributes by need. In this paper, we show how to remove the need for a generator, by embedding a dynamic approach in a modern, object-oriented programming language to implement a small, lightweight attribute grammar library. The Kiama attribution library has similar features to current generators, including cached, uncached, circular, higher-order and parameterised attributes, and implements new techniques for dynamic extension and variation of attribute equations. We use the Scala programming language because of its combination of object-oriented and functional features, support for domain-specific notations and emphasis on scalability. Unlike generators with specialised notation, Kiama attribute grammars use standard Scala notations such as pattern-matching functions for equations and mixins for composition. A performance analysis shows that our approach is practical for realistic language processing.',\n",
       " '53e997ccb7602d9701fbf80d': 'We consider constraint optimization problems where costs (or preferences) are all given, but some are tagged as possibly unstable, and provided with a range of alternative values. We also allow for some uncontrollable variables, whose value cannot be decided by the agent in charge of taking the decisions, but will be decided by Nature or by some other agent. These two forms of uncertainty are often found in many scheduling and planning scenarios. For such problems, we define several notions of desirable solutions. Such notions take into account not only the optimality of the solutions, but also their degree of robustness (of the optimality status, or of the cost) w.r.t. the uncertainty present in the problem. We provide an algorithm to find solutions accordingly to the considered notions of optimality, and we study the properties of these algorithms. For the uncontrollable variables, we propose to adopt a variant of classical variable elimination, where we act pessimistically rather than optimistically.',\n",
       " '53e997ccb7602d9701fbf815': 'This paper presents a case study of worst case timing analysis for a RISC processor. The target machine consists of the R3000 CPU and R3010 FPA (Floating Point Accelerator). This target machine is typical of a RISC system with pipelined execution units and cache memories. Our methodology is an extension of the existing timing schema. The extended timing schema provides means to reason about the execution time variation of a program construct by surrounding program constructs due to pipelined execution and cache memories of RISC processors. The main focus of this paper is on explaining the necessary steps for performing timing analysis of a given target machine within the extended timing schema framework. This paper also gives results from experiments using a timing tool for the target machine that is built based on the extended timing schema approach.',\n",
       " '53e997ccb7602d9701fbf840': 'In this paper we provide a new compact formulation of rigid shape interpolation in terms of normal equations, and propose several enhancements to previous techniques. Specifically, we propose 1) a way to improve mesh independence, making the interpolation result less influenced by variations in tessellation, 2) a faster way to make the interpolation symmetric, and 3) simple modifications to enable controllable interpolation. Finally we also identify 4) a failure mode related to large rotations that is easily triggered in practical use, and we present a solution for this as well.',\n",
       " '53e997ccb7602d9701fbf848': 'The robust system identification method using the neural network is developed based on the canonical variate analysis (CVA). The main contribution of this algorithm is using CVA to obtain the k-step optimal prediction value. Therefore, the method to obtain the comparatively accurate estimate is introduced without iteration calculations. We show that this algorithm can be applied to successfully identify the nonlinear system in the presence of comparatively loud noise. Results from several simulation studies have been included to the effectiveness of this method.',\n",
       " '53e997ccb7602d9701fbf85c': 'Nonmonotonic logics are meant to be a formalization of nonmonotonic reasoning. However, for the most part they fail to embody two of the most important aspects of such reasoning: the explicit computational nature of nonmonotonic inference, and the assignment of preferences among competing inferences. We propose a method of nonmonotonic reasoning in which the notion of inference from specific bodies of evidence plays a fundamental role. The formalization is based on autoepistemic logic, but introduces additional structure, a hierarchy of evidential spaces. The method offers a natural formalization of many different applications of nonmonotonic reasoning, including reasoning about action, speech acts, belief revision, and various situations involving competing defaults.',\n",
       " '53e997ccb7602d9701fbf882': 'We present the first version of PEPL, a declarative Process-oriented Event-based Programming Language based on the recently introduced Dynamic Condition Response (DCR) Graphs model. DCR Graphs allow for specification, distributed execution and verification of pervasive, event-based workflow and business processes by declaring condition, response, exclude and include relations between events. To provide a basis for PEPL we extend DCR Graphs to allow 1) events and relations to be parametrized with data values, 2) sub processes to be dynamically created, and 3) specification of events to be executed by a processor. We present the PEPL language and constructs, and we provide a reference to where one can find the first implementation of PEPL, which is made in JavaScript and thus allows the example programs to be defined and executed in a common Web browser.',\n",
       " '53e997ccb7602d9701fbf885': 'We propose a complete algorithm for real solution isolation for semialgebraic systems by using interval arithmetic. The algorithm is implemented as a Maple program Nrealzero and its performance on several examples is reported.',\n",
       " '53e997ccb7602d9701fbf88b': 'We assume that allele frequency data have been extracted from several large DNA pools, each containing genetic material of up to hundreds of sampled individuals. Our goal is to estimate the haplotype frequencies among the sampled individuals by combining the pooled allele frequency data with prior knowledge about the set of possible haplotypes. Such prior information can be obtained, for example, from a database such as HapMap. We present a Bayesian haplotyping method for pooled DNA based on a continuous approximation of the multinomial distribution. The proposed method is applicable when the sizes of the DNA pools and/or the number of considered loci exceed the limits of several earlier methods. In the example analyses, the proposed model clearly outperforms a deterministic greedy algorithm on real data from the HapMap database. With a small number of loci, the performance of the proposed method is similar to that of an EM-algorithm, which uses a multinormal approximation for the pooled allele frequencies, but which does not utilize prior information about the haplotypes. The method has been implemented using Matlab and the code is available upon request from the authors.',\n",
       " '53e997ccb7602d9701fbef13': 'In some cases a pediatrician seeks help from super specialist so as to diagnose the problem accurately. In a Mutli-agent environment, an agent called Intelligent Pediatric Agent (IPA) is imitating the behavior of a pediatrician. The aim is to design a decision making framework for this agent so that it can select a Super Specialist Agent (SSA) among several agents for consultation. A Bayesian Network (BN) based decision making system has been designed with the help of a pediatrician. The prototype system first selects a probable disease, out of 11; and then suggests one super specialist out of 5 super specialists. To verify the results produced by BN, a questionnaire containing 15 different cases was distributed to 21 pediatricians. Their responses are compared with the output of the system using KS test. The result suggests that 91.83% pediatricians agree with the result produced by the system. So, we can conclude that BN provides an appropriate framework to imitate the behavior of a pediatrician during selection of an appropriate specialist.',\n",
       " '53e997ccb7602d9701fbf8b9': \"Heterogeneous sensor networks are increasingly deployed to support users in the field requiring many different kinds of sensing tasks. There may be multiple alternative kinds of sensors suitable for a given task. Sensing tasks might compete for the exclusive usage of available sensors. Such an environment is highly dynamic with users moving and generating tasks at different rates. Users typically lack the time and expertise to manually decide which are the best sensors for their tasks. We need therefore to design a distributed system to automatically allocate sensors to tasks. We formalize this problem as Multi-Sensor Task Allocation (MSTA) and show that the heterogeneity of sensors and tasks requires knowledge-based sensor-task matching. We extend a pre-existent well known coalition formation protocol to propose a novel layered distributed system which by using qualitative and quantitative measures provides allocation flexibility. We demonstrate that it is feasible to perform the knowledge-based sensor-task matching on a user's device by presenting a proof-of-concept mobile app which allows a user in the field to interact with the system. We run simulations to demonstrate that our architecture is scalable and that the allocation quality improves by allowing preemption of sensing resources from ongoing tasks and a reallocation mechanism.\",\n",
       " '53e997ccb7602d9701fbf8bd': 'We address the problem of detecting consensus motifs, that occur with subtle variations, across multi- ple sequences. These are usually functional domains in DNA sequences such as transcriptional binding factors or other regulatory sites. The problem in its generality has been considered difficult and vari- ous benchmark data serve as the litmus test for different computational methods. We present a method centered around unsupervised combinatorial pattern discovery. The parameters are chosen using a careful statistical analysis of consensus motifs. This method works well on the benchmark data and is general enough to be extended to a scenario where the variation in the consensus motif includes indels (along with mutations). We also present some results on detection of transcription binding factors in human DNA sequences. Availability: The system will be made available at www.research.ibm.com/computationalgenomics.',\n",
       " '53e997ccb7602d9701fbface': \"In this paper we consider a single-server, cyclic polling system with switch-over times. A distinguishing feature of the model is that the rates of the Poisson arrival processes at the various queues depend on the server location. For this model we study the joint queue length distribution at polling epochs and at the server's departure epochs. We also study the marginal queue length distribution at arrival epochs, as well as at arbitrary epochs (which is not the same in general, since we cannot use the PASTA property). A generalised version of the distributional form of Little's law is applied to the joint queue length distribution at customer's departure epochs in order to find the waiting time distribution for each customer type. We also provide an alternative, more efficient way to determine the mean queue lengths and mean waiting times, using Mean Value Analysis. Furthermore, we show that under certain conditions a Pseudo-Conservation Law for the total amount of work in the system holds. Finally, typical features of the model under consideration are demonstrated in several numerical examples.\",\n",
       " '53e997ccb7602d9701fbf8c8': 'With recent popularity of 3D models, retrieval and recognition of 3D models based on their shape has become an important subject of study. This paper proposes a 3D model retrieval algorithm that is invariant to global deformation as well as to similarity transformation of 3D models. The algorithm is based on a set of local 3D geometrical features combined with bag-of-features approach. The algorithm employs a novel local feature, which is a combination of local geometrical feature enhanced with its spatial context computed as histogram of diffusion distance computed over mesh surface. Experimental evaluation of retrieval accuracy by using benchmark databases showed that adding positional context significantly improves retrieval accuracy.',\n",
       " '53e997ccb7602d9701fbf8cb': '“Evidence-based practices” and “scientifically based research” have become watch-words of legislation and organizations concerned with education, and technology integration across the curriculum has been implicated in this call. As a portion of the validation of a Technology Innovation Challenge Grant program, the present work sought to make a fair test of learning the same curriculum-relevant subject matter with technology versus without technology. Results from elementary and secondary schools indicated that students had significantly greater pretest to posttest gains when the target subject matter was integrated with appropriate technologies than when the same subject matter was not integrated with technologies. The topics selected by teachers from www.thesolutionsite.com/ included a broad range of subject matter in the different grade levels (e.g., cultures, syntax, farm animals, employment, butterflies, family origins).',\n",
       " '53e997ccb7602d9701fbfae9': 'This study shows the effectiveness of using gamma distribution in the speech power domain as a more general prior distribution for the model-based speech enhancement approaches. This model is a super-set of the conventional Gaussian model of the complex spectrum and provides more accurate prior modeling when the optimal parameters are estimated. We develop a method to adapt the modeled distribution parameters from each actual noisy speech in a frame-by-frame manner. Next, we derive and investigate the minimum mean square error (MMSE) and maximum a posterior probability (MAP) estimations in different domains of speech spectral magnitude, generalized power and its logarithm, using the proposed gamma modeling. Finally, a comparative evaluation of the MAP and MMSE filters is conducted. As the MMSE estimations tend to more complicated using more general prior distributions, the MAP estimations are given in closed-form extractions and therefore are suitable in the implementation. The adaptive estimation of the modeled distribution parameters provides more accurate prior modeling and this is the principal merit of the proposed method and the reason for the better performance. From the experiments, the MAP estimation is recommended due to its high efficiency and low complexity. Among the MAP based systems, the estimation in log-magnitude domain is shown to be the best for the speech recognition as the estimation in power domain is superior for the noise reduction.',\n",
       " '53e997ccb7602d9701fbf8df': 'Telecommunication service providers are racing to deliver IPTV/video on demand (VoD), voice, and data, the so called triple-play services. IPTV traffic, supported by the UDP protocol, has highly variable data rates and stringent quality of services (QoS) requirements in terms of delay and loss. Data and VoD flows are normally supported by TCP, which has its own congestion control loop to adjust the sending rate, so the traffic load is also highly dynamic. If IPTV and TCP traffic is simply multiplexed, their performance is difficult to predict and the competition between them will jeopardize their QoS. To efficiently utilize network resources and provide satisfactory QoS for both traffic types, we propose multiplexing IPTV and TCP traffic with the protection of a class based queuing (CBQ) scheme. We also develop an analytical framework to model the multiplexed IPTV traffic and TCP traffic with CBQ. The analytical results can be used as a guide to determine the admission region of IPTV and the CBQ parameters. Simulation results are presented which validate the analytical results and demonstrate the effectiveness of the proposed solution. By multiplexing IPTV and TCP traffic appropriately, network resources can be more efficiently utilized, the QoS of IPTV can be maintained, and TCP flows can obtain higher throughputs.',\n",
       " '53e997ccb7602d9701fbf8ed': 'Natural human-robot interaction requires different and more robust models of language understanding (NLU) than non-embodied NLU systems. In particular, architectures are required that (1) process language incrementally in order to be able to provide early backchannel feedback to human speakers; (2) use pragmatic contexts throughout the understanding process to infer missing information; and (3) handle the underspecified, fragmentary, or otherwise ungrammatical utterances that are common in spontaneous speech. In this paper, we describe our attempts at developing an integrated natural language understanding architecture for HRI, and demonstrate its novel capabilities using challenging data collected in human-human interaction experiments.',\n",
       " '53e997ccb7602d9701fbf933': 'Observation on query log of search engine indicates that queries are usually ambiguous. Similar to document ranking, search intents should be ranked to facilitate information search. Previous work attempts to rank intents with merely relevance score. We argue that diversity is also important. In this work, unified models are proposed to rank intents underlying a query by combining relevance score and diversity degree, in which the latter is reflected by non-overlapping ratio of every intent and aggregated non-overlapping ratio of a set of intents. Three conclusions are drawn according to the experiment results. Firstly, diversity plays an important role in intent ranking. Secondly, URL is more effective than similarity in detecting unique subtopics. Thirdly, the aggregated non-overlapping ratio makes some contribution in similarity based intent ranking but little in URL based intent ranking. © 2013 Springer-Verlag Berlin Heidelberg.',\n",
       " '53e997ccb7602d9701fbfb5f': 'In 1987, Hartman showed that the necessary condition v 驴 4 or 8 (mod 12) for the existence of a resolvable SQS(v) is also sufficient for all values of v, with 23 possible exceptions. These last 23 undecided orders were removed by Ji and Zhu in 2005 by introducing the concept of resolvable H-designs. In this paper, we first develop a simple but powerful construction for resolvable H-designs, i.e., a construction of an RH(g 2n ) from an RH((2g) n ), which we call group halving construction. Based on this construction, we provide an alternative existence proof for resolvable SQS(v)s by investigating the existence problem of resolvable H-designs with group size 2. We show that the necessary conditions for the existence of an RH(2 n ), namely, n 驴 2 or 4 (mod 6) and n 驴 4 are also sufficient. Meanwhile, we provide an alternative existence proof for resolvable H-designs with group size 6. These results are obtained by first establishing an existence result for resolvable H-designs with group size 4, that is, the necessary conditions n 驴 1 or 2 (mod 3) and n 驴 4 for the existence of an RH(4 n ) are also sufficient for all values of n except possibly n 驴 {73, 149}. As a consequence, the general existence problem of an RH(g n ) is solved leaving mainly the case of g 驴 0 (mod 12) open. Finally, we show that the necessary conditions for the existence of a resolvable G-design of type g n are also sufficient.',\n",
       " '53e997ccb7602d9701fbf95f': 'The cutoff rate is derived for a digital communication system employing an optical carrier and direct detection. The coordinated design of the encoder, optical modulator, and demodulator is then studied using the cutoff rate as a performance measure rather than the more commonly employed error probability. Modulator design is studied when transmitted optical signals are subject simultaneously to average-energy and peak-value constraints. Pulse-position modulation is shown to maximize the cutoff rate when the average-energy constraint predominates, and the best signals when the peak-value constraint predominates are identified in terms of Hadamard matrices. A time-sharing of these signals maximizes the cutoff rate when neither constraint dominates the other. Problems of efficient energy utilization, choice of input and output alphabet dimension, and the effect of random detector gain are addressed.',\n",
       " '53e997ccb7602d9701fbfb6d': 'On the basis of support vector regression (SVR), a new adaptive blind digital audio watermarking algorithm is proposed. This algorithm embeds the template information and watermark signal into the original audio by adaptive quantization according to the local audio correlation and human auditory masking. The procedure of watermark extraction is as follows. First, the corresponding features of template and watermark are extracted from the watermarked audio. Then, the corresponding feature of template is selected as training sample to train SVR and an SVR model is returned. Finally, the actual outputs are predicted according to the corresponding feature of watermark, and the digital watermark is recovered from the watermarked audio by using the well-trained SVR. Experimental results show that our audio watermarking scheme is not only inaudible, but also robust against various common signal processing (such as noise adding, resampling, requantization, and MP3 compression), and also has high practicability. In addition, the algorithm can extract the watermark without the help of the original digital audio signal, and the performance of it is better than other SVM audio watermarking schemes.',\n",
       " '53e997ccb7602d9701fbfb77': 'It is a challenging work to design a robust digital audio watermarking scheme against desynchronization attacks. On the basis of support vector machines (SVMs), a new robust digital audio watermarking algorithm against desynchronization attacks is proposed in this paper, and in this the audio statistics characteristics and synchronization code are utilized. Firstly, the optimal embedding positions are located adaptively by using the SVM theory. Secondly, the 16-bit Barker code is chosen as synchronization mark and embedded into the digital audio by modifying the statistics average value of several samples. Finally, the digital watermark are embedded into the statistics average value of low-frequency components in wavelet domain by making full use of auditory masking. Experimental results show that the proposed scheme is inaudible and robust against common signal processing such as MP3 compression, low-pass filtering, noise addition, equalization, etc., and is robust against desynchronization attacks such as random cropping, amplitude variation, pitch shifting, time-scale modification, jittering, etc.',\n",
       " '53e997ccb7602d9701fbf99e': \"The rapid development of location-based services, which make use of the location information of the user, presents both opportunities and challenges. Users can benefit from these services; however, they must often disclose their location information, which may lead to privacy problems. In this regard, the authors propose a solution with a memorizing algorithm, using trusted middleware that organizes space in an adaptive grid where it cloaks the user's location information in an anonymization area before sending it to the service providers. This newly introduced memorizing algorithm calculates on the spatial grid to decrease the overlapped areas as much as possible, which helps conceal users' locations. This solution protects the user's privacy while using the service, but also against data mining techniques with respect to their history location data. Experimental results with a user activities map establishes this theoretical analyses as well as the practical value of the proposed solution.\",\n",
       " '53e997ccb7602d9701fbf97f': 'In general, localization is a very important step in the manufacturing process, which can be considered as a prior process of assembly, machining, transportation, etc. Localization can be achieved with or without sensors. Compared with localization with sensors, localization without sensors can be more reliable, cheaper, and can have lower requirements on the environment. For example, localization without sensors can be achieved in a dark environment. However, it is more restrictive to the condition of the system. Localization of 2-D objects without sensors has been deeply investigated. Some theoretical results on 3-D-object localization without sensors have been given. On the other hand, some work has been carried out that tries to find methods to reduce uncertainties in the 3-D orientation of a polyhedron (i.e., the orientation of the polyhedron in a 3-D space). However, to the best of our knowledge, no practical and effective methods have been proposed, so far, to localize a polyhedron from any initial 3-D orientation to a unique 3-D orientation without sensors. This paper aims to find conditions and strategies for 3-D objects to be rotated from an unknown initial stable state to a unique stable state without sensors. The main contributions of this paper are given as follows: 1) It is discovered and proved that there are two classes of 3-D objects that can be rotated into a unique state from an arbitrary initial state without sensory feedback. 2) For these two classes of objects, the practical strategies are presented, and one example for each class is given to show the validity of the strategy. 3) Based on the above results, the robotic system and localization operations are illustrated, and some experimental results are given.',\n",
       " '53e997ccb7602d9701fbf9e0': 'Clique-width is a graph parameter that measures in a certain sense the complexity of a graph. Hard graph problems (e.g., problems expressible in Monadic Second Order Logic with second-order quantification on vertex sets, that includes NP-hard problems) can be solved efficiently for graphs of small clique-width. It is widely believed that determining the clique-width of a graph is NP-hard; in spite of considerable efforts, no NP-hardness proof has been found so far. We give the first hardness proof. We show that the clique-width of a given graph cannot be absolutely approximated in polynomial time unless P=NP. We also show that, given a graph G and an integer k, deciding whether the clique-width of G is at most k is NPhy complete. This solves a problem that has been open since the introduction of clique-width in the early 1990s.',\n",
       " '53e997ccb7602d9701fbf9f2': \"Pour évaluer la composante d’émission d’une voix synthétique intégrée dans un système complexe, on a besoin de méthodes et de méthodologies qui ne sont pas fournies par les tests d’évaluation standards et indépendants d’une application. Ces derniers analysent la parole synthétique surtout dans sa forme (structure de surface) et seulement en second lieu dans la signification qui lui est attribuée (structure profonde). Pour obtenir un cadre d’évaluation fiable pour un système d'application, les aspects fonctionnels de la parole doivent être pris en compte. Dans cet article, on présente deux études d’acceptabilité des dimensions qualifiantes de la voix synthétique utilisée dans différents scénarios d’application. Pour des raisons de dépenses (temps et coûts), on s'est limité à une évaluation en laboratoire. La première étude évalue la voix synthétisée d’un système de navigation et d’information de trafic, intégré dans une voiture. La deuxième étude est relative à la voix synthétisée d’un système de dialogue naturel. Les résultats montrent que les différentes dimensions contribuent à des degrés variables à l'acceptabilité globale, en fonction du scénario d’application. Il est donc nécessaire d’utiliser des méthodes d’évaluation orientées vers les applications pour identifier les dimensions qui lui sont spécifiques. Les caractéristiques de l’application qui demandent une modélisation dans les tests d’évaluation sont discutées, et des exemples sont fournis pour les deux domaines considérés.\",\n",
       " '53e997ccb7602d9701fbf9f8': \"Editor's note:\",\n",
       " '53e997ccb7602d9701fbfca7': 'We present the fist nontrivial spat-time tradeoff lower bounds for hyperplane and halfspaceemptinessqueries. Our lower bounds apply to a general class of geometric range query data structures called partition graphs. Informally, a partition graph is a directed acyclic graph that describes a recursive decomposition of space. We show that any partition graph that supports hyperplane emptiness queries implicitly deiines a halfspace range query data structure in the Fredman/Yao semigroup arithmetic model, with the same space and time bounds. Thus, results of Bronnimann, Chazelle, and Path imply that any partition graph of size s that supports hyperplane emptiness queries in time t must satisfy the inequality st d = ~((n/logn)*–~d–Tl/ld+l)]. Using difterent techniques, we show that fl(nd/ polylog n) preprocessing time is required to achieve polylogarithmic query time, and that Cl(n( ‘–1 “d/ polylogn) query time is required if only O(n polylog n) preprocessing time is used. These two lower bounds are optimal up to polylogarithrnic factors. For twcdimensional queries, we obtain an optimal continuous tradeoff between these two extremes. Finally, using a reduction argument, we show that the same lower bounds hold for halfspace emptiness queries in R.d(‘+3)’2 on a restricted class of partition graphs.',\n",
       " '53e997ccb7602d9701fbfcb3': 'This paper presents a modular Matlab tool, namely MORPHEO, devoted to the study of particle morphology by Fourier analysis. A benchmark made of four sample images with different features (digitized coins, a pebble chart, gears, digitized volcanic clasts) is then proposed to assess the abilities of the software. Attention is brought to the Weibull distribution introduced to enhance fine variations of particle morphology. Finally, as an example, samples pertaining to a lahar deposit located in La Lumbre ravine (Colima Volcano, Mexico) are analysed. MORPHEO and the benchmark are freely available for research purposes.',\n",
       " '53e997ccb7602d9701fbfcc1': \"Initial knowledge regarding group size can be crucial for collective performance. We study this relation in the context of the Ants Nearby Treasure Search (ANTS) problem [18], which models natural cooperative foraging behavior such as that performed by ants around their nest. In this problem, k (probabilistic) agents, initially placed at some central location, collectively search for a treasure on the two-dimensional grid. The treasure is placed at a target location by an adversary and the goal is to find it as fast as possible as a function of both k and D, where D is the (unknown) distance between the central location and the target. It is easy to see that T=Ω(D+D2/k) time units are necessary for finding the treasure. Recently, it has been established that O(T) time is sufficient if the agents know their total number k (or a constant approximation of it), and enough memory bits are available at their disposal [18]. In this paper, we establish lower bounds on the agent memory size required for achieving certain running time performances. To the best our knowledge, these bounds are the first non-trivial lower bounds for the memory size of probabilistic searchers. For example, for every given positive constant ε, terminating the search by time O(log1−εk ·T) requires agents to use Ω(loglogk) memory bits. From a high level perspective, we illustrate how methods from distributed computing can be useful in generating lower bounds for cooperative biological ensembles. Indeed, if experiments that comply with our setting reveal that the ants' search is time efficient, then our theoretical lower bounds can provide some insight on the memory they use for this task.\",\n",
       " '53e997ccb7602d9701fbfcc3': \"In this paper, we present QUALEX, a system and algorithm for generating first-order qualitative causal graphs for tutorial purposes based on de Kleer and Brown's qualitative modeling theory. QUALEX is embedded in a constructive simulation environment called Heart Works which teaches hydraulics principles about the cardio-vascular system to first year college biology students.\",\n",
       " '53e997ccb7602d9701fbfccb': 'Histories often marvel at the impressive mathematicians who were part of the World War II team at Bletchley Park, but what often goes unrecognized is that the United States Navy assembled a similarly impressive team. This paper recognizes some of the mathematicians who served at the Washington, D.C. Naval Communications Annex during World War II.',\n",
       " '53e997ccb7602d9701fbfcd1': 'Using activity recognition for cognitive tasks can provide new insights about reading and learning habits.',\n",
       " '53e997ccb7602d9701fbfce0': 'A fast algorithm of finding the diagonal elements of the covariance matrix of the two-dimensional Walsh-Hadamard (WH) transform of data is described. Its usefulness and other interesting properties of the WH transform are discussed. The performance of the WH transform is compared with the Karhunen-Loeve transform for a first-order stationary Markov process.',\n",
       " '53e997ccb7602d9701fbfce3': 'A characterization of n-vertex isolate-free connected graphs G whose domination number γ ( G ) satisfies γ(G)=⌊n/2⌋ is obtained. This result enables us to obtain extremal graphs of inequalities which bound the sum of two domination parameters in isolate-free graphs.',\n",
       " '53e997ccb7602d9701fbfa2f': 'Autonomic systems are needed to self-manage the increasing complexity of pervasive communications access and the ubiquitous computing services it offers to humans. Policy based governance is the means by which humans will impose their will upon how autonomic systems manage themselves. Ultimately, these systems mediate information and services between people, in a rich tapestry of human associations formed though personal, commercial, and civic relationships. Therefore, if policies are to accurately reflect the wishes of their human authors, the process of authoring or engineering them must closely reflect the intricacies and fluidity of human relationships. We see this as the route to providing intuitive use of policy systems and thus to autonomic systems in empowering their users. We have previously presented a scheme for community-based management that addressed the dynamic organizational aspects of human relations in policy authoring. In this paper we review developments with this approach and describe its integration with a trust management system designed to flexibly regulate access to resources according to the trust relationships between participants. This approach promises to enable autonomic organisations, with structures that automatically adapt to the collective needs of the users.',\n",
       " '53e997ccb7602d9701fbfd01': 'We present a general technique for dynamizing a significant class of problems whose underlying structure is a computation graph embedded in a tree. This class of problems includes the evaluation of linear expressions over k-tuples from a semiring with binary and unary operators, attribute grammars with linear dependencies, point location in binary space partitions, compaction of slicing floorplans, graph drawing, generalized heaps, and a variety of optimization problems in bounded tree-width graphs. For problems in this class, we support a complete repertory of dynamic operations in logarithmic time using linear space.',\n",
       " '53e997ccb7602d9701fbfa45': 'Content aware image re-targeting methods aim to arbitrarily change image aspect ratios while preserving visually prominent features. To determine visual importance of pixels, existing re-targeting schemes mostly rely on grayscale intensity gradient maps. These maps show higher energy only at edges of objects, are sensitive to noise, and may result in deforming salient objects. In this paper, we present a computationally efficient, noise robust re-targeting scheme based on seam carving by using saliency maps that assign higher importance to visually prominent whole regions (and not just edges). This is achieved by computing global saliency of pixels using intensity as well as color features. Our saliency maps easily avoid artifacts that conventional seam carving generates and are more robust in the presence of noise. Also, unlike gradient maps, which may have to be recomputed several times during a seam carving based re-targeting operation, our saliency maps are computed only once independent of the number of seams added or removed.',\n",
       " '53e997ccb7602d9701fbfd0d': 'Dipoles represent long distance connections between the pressure anomalies of two distant regions that are negatively correlated with each other. Such dipoles have proven important for understanding and explaining the variability in climate in many regions of the world, e.g., the El Nino climate phenomenon is known to be responsible for precipitation and temperature anomalies over large parts of the world. Systematic approaches for dipole detection generate a large number of candidate dipoles, but there exists no method to evaluate the significance of the candidate teleconnections. In this paper, we present a novel method for testing the statistical significance of the class of spatio-temporal teleconnection patterns called as dipoles. One of the most important challenges in addressing significance testing in a spatio-temporal context is how to address the spatial and temporal dependencies that show up as high autocorrelation. We present a novel approach that uses the wild bootstrap to capture the spatio-temporal dependencies, in the special use case of teleconnections in climate data. Our approach to find the statistical significance takes into account the autocorrelation, the seasonality and the trend in the time series over a period of time. This framework is applicable to other problems in spatio-temporal data mining to assess the significance of the patterns.',\n",
       " '53e997ccb7602d9701fbfa53': 'In this paper we discuss an approach to organizing the integration of 110,000 lines of C, SQL, Assembler, and microcode distributed over a network of 36 processors of four types so that it could be accomplished in six months. The software runs on a test system architecture consisting of a LAN-based workstation group and a set of VME-based embedded processors. By using structured methodology, parallelism in the integration process was achieved. The necessary stub tools were identified and developed before integration began. The principles followed and experiences of integrating the system are discussed.',\n",
       " '53e997ccb7602d9701fbfd21': 'A DNA sequence can be regarded as a discrete-time Markov chain. Based on k-step transition probabilities, we construct a series of 4 x 4 k-step transition matrices to characterize the DNA primary sequences. According to the properties of Markov chains, we obtain distributions of A, T, C and G, and analyze the changes among them from yesterday to tomorrow. We can calculate the probabilities of nucleotide triples of DNA primary sequences. Finally, we introduce a correlation of this kind of transition matrices and consider it as an invariant to analyze the similarities/dissimilarities of DNA sequences. (C) 2006 Wiley Periodicals, Inc.',\n",
       " '53e997ccb7602d9701fbfa5a': 'Consider a bipartite graph with a set of left-vertices and a set of right-vertices. All the edges adjacent to the same left-vertex have the same weight. We present an algorithm that, given the set of right-vertices and the number of left-vertices, processes a uniformly random permutation of the left-vertices, one left-vertex at a time. In processing a particular left-vertex, the algorithm either permanently matches the left-vertex to a thus-far unmatched right-vertex, or decides never to match the left-vertex. The weight of the matching returned by our algorithm is within a constant factor of that of a maximum weight matching.',\n",
       " '53e997ccb7602d9701fbfd49': 'We propose a system to solve a multi-class produce categorization problem. For that, we use statistical color, texture, and structural appearance descriptors (bag-of-features). As the best combination setup is not known for our problem, we combine several ...',\n",
       " '53e997ccb7602d9701fbfa98': 'A Wireless Mesh Network (WMN) is composed of multiple Access Points (APs) that are connected together using the radio channel and by a limited number of gateway APs connected to the Internet. In this paper, we address the problem of gateway placement that consists of minimizing the number of gateways while satisfying system performance requirements. Along with the placement problem, the formulation includes joint routing and scheduling to account for the problem of interference and to enable spacial reuse. The problem, which we coined GPSRP (Gateway Placement and Spatial Reuse Problem), allows a much more efficient use of the available resources and reduces overall gateway costs. This article presents for the first time a mathematical formulation of the problem and discusses its advantages and limitations with respect to other approaches.',\n",
       " '53e997ccb7602d9701fbfbb8': 'To integrate a software component into a system, it must interact properly with the systemu0027s other components. Unfortunately, the decisions about how a component is to interact with other component...',\n",
       " '53e997ccb7602d9701fbfdb4': 'Distributed Network Management is gaining importance due to the e xplosive growth of the size of computer networks. New management paradigms are being proposed as an alternative to the centralised one, and new technologies and programm ing languages are making them feasible. The use of Mobile Agents (MAs) to distribute and delegate manage- ment tasks is a particularly promising approach to dealing with the limitations of current cen- tralised management systems which appear to be lacking flexibility and scalability. This paper is focused on the impact that mobile code paradigms can have on distributed net- work and system monitoring. A dynamic, hierarchical management model based on a delega- tion paradigm is adopted and an MA-architecture for monitoring operations is proposed. Fi- nally, possible uses of the proposed model and architecture for pursuing se amless and timely monitoring are discussed.',\n",
       " '53e997ccb7602d9701fbfdcc': 'This paper derives a lower bound of the form n 1 to the per-node throughput achievable by a wireless net- work when n source-destination pairs are randomly distributed throughout a disk of radius n , 0 < < 1=2 and propagation is modeled by an attenuation of the form 1=(1 + d) , > 2.',\n",
       " '53e997ccb7602d9701fbfdd1': \"This paper presents an innovative technique based on the joint approximation capabilities of radial basis function (RBF) networks and the estimation capability of the multivariate iterated Hilbert transform (IHT) for the statistical demodulation of pathological tremor from electromyography (EMG) signals in patients with Parkinson's disease. We define a stochastic model of the multichannel high-density surface EMG by means of the RBF networks applied to the reconstruction of the stochastic process (characterizing the disease) modeled by the multivariate relationships generated by the Karhunen-Loéve transform in Hilbert spaces. Next, we perform a demodulation of the entire random field by means of the estimation capability of the multivariate IHT in a statistical setting. The proposed method is applied to both simulated signals and data recorded from three Parkinsonian patients and the results show that the amplitude modulation components of the tremor oscillation can be estimated with signal-to-noise ratio close to 30 dB with root-mean-square error for the estimates of the tremor instantaneous frequency. Additionally, the comparisons with a large number of techniques based on all the combinations of the RBF, extreme learning machine, backpropagation, support vector machine used in the first step of the algorithm; and IHT, empirical mode decomposition, multiband energy separation algorithm, periodic algebraic separation and energy demodulation used in the second step of the algorithm, clearly show the effectiveness of our technique. These results show that the proposed approach is a potential useful tool for advanced neurorehabilitation technologies that aim at tremor characterization and suppression.\",\n",
       " '53e997ccb7602d9701fbfdd6': 'We argue that there are some special situations where it can be useful to repair well-formedness violations occurring in XML-like input, giving examples from our own work. We analyze the types of errors that can occur in XML-like input and present a shallow algorithm that fixes most of these errors, without requiring knowledge of a DTD or XML Schema.',\n",
       " '53e997ccb7602d9701fbfddc': 'RunMC is an object-oriented framework aimed to generate and to analyse high-energy collisions of elementary particles using Monte Carlo simulations. This package, being based on C++ adopted by CERN as the main programming language for the LHC experiments, provides a common interface to different Monte Carlo models using modern physics libraries. Physics calculations (projects) can easily be loaded and saved as external modules. This simplifies the development of complicated calculations for high-energy physics in large collaborations. This desktop program is open-source licensed and is available on the LINUX and Windows/Cygwin platforms.',\n",
       " '53e997ccb7602d9701fbfc40': \"The leaky integrate-and-fire (LIF) model of neuronal spiking (Stein 1967) provides an analytically tractable formalism of neuronal firing rate in terms of a neuron's membrane time constant, threshold, and refractory period. LIF neurons have mainly been used to model physiologically realistic spike trains, but little application of the LIF model appears to have been made in explicitly computational contexts. In this article, we show that the transfer function of a LIF neuron provides, over a wide-parameter range, a compressive nonlinearity sufficiently close to that of the logarithm so that LIF neurons can be used to multiply neural signals by mere addition of their outputs yielding the logarithm of the product. A simulation of the LIF multiplier shows that under a wide choice of parameters, a LIF neuron can log-multiply its inputs to within a 5% relative error.\",\n",
       " '53e997ccb7602d9701fbfbeb': 'Information summarization and retrieval are significant research topics associated with recent advancements in sensor devices, data compression and storage techniques, and high-speed internet. As a result of these advances, it is possible for people to collect huge life-logs. Video is one of the most important life information sources. This paper describes a method of summarizing video life-logs in an office environment with a multi-camera system. Previously, multi-camera systems have been used to track moving objects or to cover a wide area. This paper focuses on capturing diverse views of each office event using a multi-camera system with several cameras observing the same area. The summarization process includes camera view selection and event sequence summarization. View selection produces a single event sequence from multiple event sequences by selecting an optimal view at each time, for which domain knowledge based on the elements of the office environment and rules from questionnaire surveys have been used. Summarization creates a summary sequence from whole sequences by using a fuzzy rule-based system to approximate human decision making. The user-entered degrees of interest in objects, persons, and events are used for a personalized summarization. We confirmed experimentally that the proposed method provides promising results.',\n",
       " '53e997ccb7602d9701fbfc47': 'This paper presents data characterizing the household phone line in the 1-60 MHz band. Two types of measurements were performed: transmission and noise sampling. The transmission measurements were done by using the impulse channel sounding method, so both the line attenuation and the delay spread were obtained. The noise measurements include: phone line background noise, phone noise, and noise sampled over a 24 hour period. Statistical characteristics of the delay spread, frequency response and noise can be extracted from the data and used in the design of phone line based communications systems',\n",
       " '53e997ccb7602d9701fbfdf6': 'In this paper, we introduce a new high-resolution reflectometry technique that operates simultaneously in both the time and frequency domains. The approach rests upon time-frequency signal analysis and utilizes a chirp signal multiplied by a Gaussian time envelope. The Gaussian envelope provides time localization, while the chirp allows one to excite the system under test with a swept sinewave covering a frequency band of interest. This latter capability is of particular interest when testing communication cables and systems. Sensitivity in detecting the reflected signal is provided by a time-frequency cross-correlation function. The approach is verified by experimentally locating various types of faults, located at various distances, in RG 142 and RG 400 coaxial cables.',\n",
       " '53e997ccb7602d9701fbfc58': 'A constraint of a linear program is called a generalized variable upper bound (GVUB) constraint, if the right-hand is nonnegative\\n and each variable with a positive coefficient in the constraint does not have a nonzero coefficient in any other GVUB constraint.\\n Schrage has shown how to handle GVUB constraints implicitly in the simplex-method. It is demonstrated in this paper that the\\n Forrest-Tomlin data structure may be used for the inverse of the working basis, and it is discussed how to update this representation\\n from iteration to iteration.',\n",
       " '53e997ccb7602d9701fbfc5a': 'Two-sided assembly line is a set of sequential workstations where task operations can be performed in two sides of the line. The line is important for large-sized products, such as trucks, buses and cars. In this paper, we proposed a mathematical model for two-sided assembly line type II (TALBP-II) with assignment restrictions. The aim of the model is minimizing the cycle time for a given number of mated-workstations and balancing the workstation simultaneously. The model provides a more realistic situation of the two-sided assembly line problems. Genetic algorithm and iterative first-fit rule are used to solve the problem. The performances of both methods are compared using six numerical examples. Based on the experiments, the iterative first-fit rule can take the advantage of finding the best position over many workstations and the genetic algorithm provides more flexible task assignment and is significantly faster than the iterative first-fit rule.',\n",
       " '53e997ccb7602d9701fbfc5e': 'We propose an automatic instrumentation method for embedded software annotation to enable performance modeling in high level hardware/software co-simulation environments. The proposed \"cross-annotation\" technique consists of extending a retargetable compiler infrastructure to allow the automatic instrumentation of embedded software at the basic block level. Thus, target and annotated native binaries are guaranteed to have isomorphic control flow graphs (CFG). The proposed method takes into account the processor-specific optimizations at the compiler level and proves to be accurate with low simulation overhead.',\n",
       " '53e997ccb7602d9701fbfcb5': 'The MathLang project aims at computerizing mathematical texts according to various degrees of formalisations, and without any prior commitment to a particular logical framework (e.g., having to choose either set theory or category theory or type theory, ...',\n",
       " '53e997ccb7602d9701fbfe4b': 'The problem of estimating a linear functional in a linear Gaussian model is considered. For the estimation, the class of projection estimators is used. The problem is to choose the optimal estimate from this class on the basis of observations. The solution of this problem is based on the principle of risk envelope minimization.',\n",
       " '53e997ccb7602d9701fbfeb5': 'The complex semantics of Ada tasking cause excessive run-time overhead that cannot be avoided even when using the best compiler technology available. It has been demonstrated that special-purpose hardware can reduce the rendezvous latency with 90–99 % depending on the case [Roo89]. All the necessary extra hardware is contained in a single chip coprocessor which easily can be integrated into standard computer hardware. Multiprocessor tasking involves the additional problem of latency and limited bandwidth of the interprocessor communication system. By restructuring the run-time system the communication overhead can be reduced by 90–95 %, which has been demonstrated in a previous project [Lun90]. The success of both these projects has been due to a set of operations or a protocol cleverly tuned to the specific requirements in each case. In the present paper the coprocessor approach will be used and it will be extended to cover also the distributed tasking protocol developed in the previous project.',\n",
       " '53e997ccb7602d9701fbfec0': 'This paper presents a reduced-complexity approximate density evolution (DE) scheme for low-density parity-check (LDPC) codes in channels with memory in the form of a hidden Markov chain. This approximation is used to design degree sequences representing some of the best known LDPC code ensembles for the Gilbert-Elliott channel, and example optimizations are also given for other Markov channels. The problem of approximating the channel estimation is addressed by obtaining a specially constructed message-passing schedule in which the channel messages all approach their stable densities. It is shown that this new schedule is much easier to approximate than the standard schedule, but has the same ultimate performance in the limits of long block length and many decoding iterations. This result is extended to show that all message-passing schedules that satisfy mild conditions will have the same threshold under density evolution',\n",
       " '53e997ccb7602d9701fbff16': 'In this paper, we present the results of an empirical analysis regarding the application and realization of established design patterns in existing peer-to-peer system implementations. Among the goals of our research were the identification of relevant design patterns that are particularly helpful and well suited for the implementation of peer-to-peer architectures and the discovery of potential shortfalls with respect to the capitalization on design patterns in that domain. We describe which design pattern realizations were encountered in the analyzed system implementations and how frequently they were applied, and finally we draw some conclusions from our findings.',\n",
       " '53e997ccb7602d9701fbff30': 'The electronic/ionic mixed conduction is examined in solid ionic memory devices by numerically solving the Poisson–Nernst–Planck equations using the computational platform PROPHET. The boundary conditions for the Poisson–Nernst–Planck system are determined based on the theoretical treatments as a Dirichlet type. The chemical composition of the mixed conductor under the reference electrode and the magnitude of applied biases are considered as important parameters in the simulation. The results show that the deviation of carrier distribution increases from the analytical solutions with the increase of applied biases and the decrease of the partial pressure of the non-metallic component near the reference electrode in solid ionic memory devices.',\n",
       " '53e997ccb7602d9701fbff3f': 'In this paper, sufficient conditions for the approximate controllability of a class of second-order nonlinear stochastic functional differential equations of McKean-Vlasov type are derived. The nonlinearities at a given time t considered depend not only on the state of the solution at time t, but also on the corresponding probability distribution at time t. An example is given to illustrate the theory.',\n",
       " '53e997ccb7602d9701fbff44': \"A large part of a modern SOC's debug complexity resides in the interaction between the main system components. Transaction-level debug moves the abstraction level of the debug process up from the bit and cycle level to the transactions between IP blocks. In this paper we raise the debug abstraction level further, by utilising structural and temporal abstraction techniques, combined with debug data interpretation and logical communication views. The combination of these techniques and views allow us, among others, to single-step and observe the operation of the network on a per-connection basis. As an example, we show how these higher-level abstractions have been implemented in the debug environment for the Æthereal NOC architecture and present a generic debug API, which can be used to visualise an SOC's state at the logical communication level.\",\n",
       " '53e997ccb7602d9701fbff6e': 'Accurate inference of genealogical relationships between pairs of individuals is paramount in association studies, forensics and evolutionary analyses of wildlife populations. Current methods for relationship inference consider only a small set of close relationships and have limited to no power to distinguish between relationships with the same number of meioses separating the individuals under consideration (e.g. aunt-niece versus niece-aunt or first cousins versus great aunt-niece).We present CARROT (ClAssification of Relationships with ROTations), a novel framework for relationship inference that leverages linkage information to differentiate between rotated relationships, that is, between relationships with the same number of common ancestors and the same number of meioses separating the individuals under consideration. We demonstrate that CARROT clearly outperforms existing methods on simulated data. We also applied CARROT on four populations from Phase III of the HapMap Project and detected previously unreported pairs of third- and fourth-degree relatives.Source code for CARROT is freely available at http://carrot.stanford.edu.sofiakp@stanford.edu.',\n",
       " '53e997ccb7602d9701fbff7c': 'A custom designed markerless tracking system was demonstrated to be applicable for positron emission tomography (PET) brain imaging. Precise head motion registration is crucial for accurate motion correction (MC) in PET imaging. State-of-the-art tracking systems applied with PET brain imaging rely on markers attached to the patient&#39;s head. The marker attachment is the main weakness of these system...',\n",
       " '53e997ccb7602d9701fbffd0': 'Lexical ambiguity and especially part-of-speech ambiguity is the source of much non-determinism in parsing. As a result, the resolution of lexical ambiguity presents deterministic parsing with a major test. If deterministic parsing is to be viable, it must be shown that lexical ambiguity can be resolved easily deterministically. In this paper, it is shown that Marcus\\'s \"diagnostics\" can be handled without any mechanisms beyond what is required to parse grammatical sentences and reject ungrammatical sentences. It is also shown that many other classes of ambiguity can be easily resolved as well.',\n",
       " '53e997ccb7602d9701fbffe0': \"Current implementations of Backus' FP do not explore the parallelism in FP programs fully. Thus, the efficiency techniques is limited. This paper presents a new approach, namely, data flow, to evaluate FP programs. The data flow approach is more effective than the conventional approaches both since it adopts an eager evaluation and since the parallelism can be used fully. An FP to data flow graph translation algorithm is presented. Some optimization techniques are also presented. The implementations based on this approach are also discussed.\",\n",
       " '53e997ccb7602d9701fc000f': 'This monograph reviews both the theory and practice of the numerical computation of geodesic distances on Riemannian manifolds. The notion of Riemannian manifold allows one to define a local metric (a symmetric positive tensor field) that encodes the information about the problem one wishes to solve. This takes into account a local isotropic cost (whether some point should be avoided or not) and a local anisotropy (which direction should be preferred). Using this local tensor field, the geodesic distance is used to solve many problems of practical interest such as segmentation using geodesic balls and Voronoi regions, sampling points at regular geodesic distance or meshing a domain with geodesic Delaunay triangles. The shortest paths for this Riemannian distance, the so-called geodesics, are also important because they follow salient curvilinear structures in the domain. We show several applications of the numerical computation of geodesic distances and shortest paths to problems in surface and shape processing, in particular segmentation, sampling, meshing and comparison of shapes. All the figures from this review paper can be reproduced by following the Numerical Tours of Signal Processing. http://www.ceremade.dauphine.fr/~peyre/numerical-tour/ Several textbooks exist that include description of several manifold methods for image processing, shape and surface representation and computer graphics. In particular, the reader should refer to [42, 147, 208, 209, 213, 255] for fascinating applications of these methods to many important problems in vision and graphics. This review paper is intended to give an updated tour of both foundations and trends in the area of geodesic methods in vision and graphics.',\n",
       " '53e997ccb7602d9701fc01d1': 'Our aim is to build a set of rules, such that reasoning over temporal dependencies within gene regulatory networks is possible. The underlying transitions may be obtained by discretizing observed time series, or they are generated based on existing knowledge, e.g. by Boolean networks or their nondeterministic generalization. We use the mathematical discipline of formal concept analysis (FCA), which has been applied successfully in domains as knowledge representation, data mining or software engineering. By the attribute explorationalgorithm, an expert or a supporting computer program is enabled to decide about the validity of a minimal set of implications and thus to construct a sound and complete knowledge base. From this all valid implications are derivable that relate to the selected properties of a set of genes. We present results of our method for the initiation of sporulation in Bacillus subtilis. However the formal structures are exhibited in a most general manner. Therefore the approach may be adapted to signal transduction or metabolic networks, as well as to discrete temporal transitions in many biological and nonbiological areas.',\n",
       " '53e997ccb7602d9701fc13a2': 'The automated directory assistance system (ADAS) is traditionally formulated as an automatic speech recognition (ASR) problem. Recently, it has been formulated as a voice search problem, where a spoken utterance is firstly converted into text, which in turn is used to search for the listing. In this paper, we focus on the design and development of the utterance-to-listing component of ADAS. We show that many theoretical and practical issues need to be resolved when applying the basic idea of voice search to the development of ADAS. We share our experiences in addressing these issues, especially in pre-processing the listing database, generating a high performance LM, and developing efficient, accurate, and robust search algorithms. Field tests of our prototype system indicate that an 81% task completion rate can be achieved. Index Terms: speech recognition, directory assistance, voice search, TFIDF, spoken dialog system, vector space model',\n",
       " '53e997ccb7602d9701fc18f1': \"Several data mining methods require data that are discrete, and other methods often perform better with discrete data. We introduce an efficient Bayesian discretization (EBD) method for optimal discretization of variables that runs efficiently on high-dimensional biomedical datasets. The EBD method consists of two components, namely, a Bayesian score to evaluate discretizations and a dynamic programming search procedure to efficiently search the space of possible discretizations. We compared the performance of EBD to Fayyad and Irani's (FI) discretization method, which is commonly used for discretization.\",\n",
       " '53e997d1b7602d9701fc1ef7': ' The precise docking of a truck at a loading dockhas been proposed in [Nguyen &amp; Widrow 90] as a benchmarkproblem for non-linear control by neural-nets. Themain difficulty is that back-propagation is not a priori suitableas a learning paradigm, because no set of training vectorsis available: It is non-trivial to find solution trajectoriesthat dock the truck from anywhere in the loading yard.In this paper we show how a genetic algorithm can evolvethe weights of a feedforward 3-layer neural ... ',\n",
       " '53e997d1b7602d9701fc1efd': 'In an indoor environment, where GPS signal is not available, localization typically relies on triangulation or fingerprint (or radio map) algorithms. These algorithms require the availability of three or more anchor points (or reference points). In this paper, we propose a novel localization algorithm, which can derive the location of moving users with high accuracy based on measuring the technologically feasible displacement vector in an environment where only one anchor point is needed. The proposed localization algorithm can locate a user in a 3dimensional space and has been tested on a smartphone to verify its feasibility and accuracy.',\n",
       " '53e997ccb7602d9701fc1cd6': 'In the recent years, usage of the third-person perspective (3PP) in virtual training methods has become increasingly viable and despite the growing interest in virtual reality and graphics underlying third-person perspective usage, not many studies have systematically looked at the dynamics and differences between the third and first-person perspectives (1PPs). The current study was designed to quantify the differences between the effects induced by training participants to the third-person and first-person perspectives in a ball catching task. Our results show that for a certain trajectory of the stimulus, the performance of the participants post3PP training is similar to their performance postnormal perspective training. Performance post1PP training varies significantly from both 3PP and the normal perspective.',\n",
       " '53e997d1b7602d9701fc1f53': \"The most popular data mining techniques consist in searching databases for frequently occurring patterns, e.g. association rules, sequential patterns. We argue that in contrast to today's loosely-coupled tools, data mining should be regarded as advanced database querying and supported by Database Management Systems (DBMSs). In this paper we descirbe our research prototype system, which logically extends DBMS functionality, offering extensive support for pattern discovery, storage and management. We focus on the system architecture and novel SQL-based data mining query language, which serves as the user interface to the system.\",\n",
       " '53e997d1b7602d9701fc1f55': 'The increasing body of distributed and heterogeneous information and the autonomous, heterogeneous and dynamic nature of information resources are important issues hindering effective and efficient data access, retrieval and knowledge sharing. The importance ...',\n",
       " '53e997ccb7602d9701fc1d94': 'One of the main reasons for the difficulty of hardware verification is that hardware platforms are typically nondeterministic at clock-cycle granularity. Uninitialized state elements, I/O, and timing variations on high-speed buses all introduce nondeterminism that causes different behavior on different runs starting from the same initial state. To improve our ability to debug hardware, we would like to completely eliminate nondeterminism. This paper introduces the Cycle-Accurate Deterministic REplay (CADRE) architecture, which cost-effectively makes a boardlevel computer cycle-accurate deterministic. We characterize the sources of nondeterminism in computers and show how to address them. In particular, we introduce a novel scheme to ensure deterministic communication on source-synchronous buses that cross clock-domain boundaries. Experiments show that CADRE on a 4-way multiprocessor server enables cycle-accurate deterministic execution of one-second intervals with modest buffering requirements (around 200MB) and minimal performance loss (around 1%). Moreover, CADRE has modest hardware requirements.',\n",
       " '53e997ccb7602d9701fc1db8': 'This paper generalizes Wyner\\'s definition of common information of a pair of\\nrandom variables to that of $N$ random variables. We prove coding theorems that\\nshow the same operational meanings for the common information of two random\\nvariables generalize to that of $N$ random variables. As a byproduct of our\\nproof, we show that the Gray-Wyner source coding network can be generalized to\\n$N$ source squences with $N$ decoders. We also establish a monotone property of\\nWyner\\'s common information which is in contrast to other notions of the common\\ninformation, specifically Shannon\\'s mutual information and G\\\\\\'{a}cs and\\nK\\\\\"{o}rner\\'s common randomness. Examples about the computation of Wyner\\'s\\ncommon information of $N$ random variables are also given.',\n",
       " '53e997ccb7602d9701fc1dcc': \"We provide a combinatorial proof of a symplectic character identity relating the sum of a product of symplectic Schur functions to the product Pi(m)(i-1) Pi(n)(j-1) (x(i) + x(i)(-1) + y(j)(-1)). This formula owes its origin to the existence of a dual pair of symplectic groups acting on spinors, as pointed out by Hasegawa. The first combinatorial proof, based on symplectic tableaux and a variation of the Robinson-Schensted-Knuth correspondence, was due to Terada. Here we use Schutzenberger's jeu de taquin, augmented by two simple zero weight transformations. The identity itself generalizes a well-known identity expressing Pi(m)(i=1) Pi(n)(j=1) (x(i) + y(j)) as a sum of products of Schur functions that was due to Littlewood and proved combinatorially by Remmel. We offer an alternative combinatorial proof of this identity by means of the jeu de taquin, as a precursor to the proof of the symplectic identity.\",\n",
       " '53e997ccb7602d9701fc1de7': 'This paper introduces a new technique in the investigation of limited-dependent variable models. This paper illustrates that variable precision rough set theory (VPRS), allied with the use of a modern method of classification, or discretisation of data, can out-perform the more standard approaches that are employed in economics, such as a probit model. These approaches and certain inductive decision tree methods are compared (through a Monte Carlo simulation approach) in the analysis of the decisions reached by the UK Monopolies and Mergers Committee. We show that, particularly in small samples, the VPRS model can improve on more traditional models, both in-sample, and particularly in out-of-sample prediction. A similar improvement in out-of-sample prediction over the decision tree methods is also shown.',\n",
       " '53e997ccb7602d9701fc1e3f': 'The correct interpretation of tandem mass spectra is a difficult problem, even when it is limited to scoring peptides against a database. De novo sequencing is considerably harder, but critical when sequence databases are incomplete or not available. In this paper we build upon earlier work due to Dancik et al., and Chen et al. to provide a dynamic programming algorithm for interpreting de novo spectra. Our method can handle most of the commonly occurring ions, including a; b; y, and their neutral losses. Additionally, we shift the emphasis away from sequencing to assigning ion types to peaks. In particular, we introduce the notion of core interpretations, which allow us to give confidence values to individual peak assignments, even in the absence of a strong interpretation. Finally, we introduce a systematic approach to evaluating de novo algorithms as a function of spectral quality. We show that our algorithm, in particular the core-interpretation, is robust in the presence of measurement error, and low fragmentation probability.',\n",
       " '53e997d1b7602d9701fc204c': 'Many methods have been developed to utilize topic analysis models to deal with the noises and sparseness of the text. However, the use of a topic model solely sometimes unable to achieve the expected high performance, it is very necessary to improve the current topic model to cope with the characteristic of texts and specific requirements. In this paper, we focus on two tasks. One is to make use of different external corpus to identify topics from texts for better categorization. The other is to add the weight of a few features in texts to get some other topics from those of topic model. We further evaluate the performance of the two tasks with baseline results. The experiments show that our proposed method can achieve a higher accuracy in text classification. The approach can find truly representative words which may contribute to wide acceptance of topic models in micro-blog analysis. © 2013 Springer-Verlag Berlin Heidelberg.',\n",
       " '53e997ccb7602d9701fc1e57': 'A universal model-learning-competing one is established for solving task assignment problem in National Economy Mobilization(NEM), in which local search was focused in learning model and global search in competing. Thereafter the strong points of the two models were amalgamated in the algorithm. A team of parameters was used to coordinating the correlation of the two models. Applying learning-competing model to an actual case, using greed algorithm in learning model, genetic algorithm in competing model, the results obtained were in coincidence with the analysis.',\n",
       " '53e997ccb7602d9701fc1e96': 'A hardware and software solution is proposed for the development of a DSP-operated cell/particle counting and sizing system. LabVIEW, which is a graphical programming language, was adopted as the DSP programming platform. Special attention was paid to data reduction, noise elimination, baseline restoration, and pulsewidth discrimination. Global accuracy is verified based on externally generated te...',\n",
       " '53e997ccb7602d9701fc1e9e': 'Passage time densities are useful performance measurements in stochastic systems. With them the modeller can extract probabilistic quality-of-service guarantees such as: the probability that the time taken for a network header packet to travel across a heterogeneous network is less than 10ms must be at least 0.95. In this paper, we show how new tools can extract passage time densities and distributions from stochastic models defined in PEPA, a stochastic process algebra. In stochastic process algebras, the synchronisation policy is important for defining how different system components interact. We also show how these passage time results can vary according to which synchronisation strategy is used. We compare results from two popular strategies.',\n",
       " '53e997d1b7602d9701fc1ea5': 'Determining the shape of a point pattern is a problem of considerable practical interest and has applications in many branches of science related to Pattern Recognition. Set estimators of a nonparametric nature which may be used as shape descriptors should have several desirable properties. The estimators should be (a) consistent, i.e. Lebesgue measure of the symmetric difference of the actual region and the estimated should go to zero in probability; (b) computationally efficient; and (c) automatic, in the sense that the method should be able to detect the number of independent disjoint components in the region even when this number is unknown. None of the currently known estimators combine all these properties. A new shape descriptor called s-shape in the context of perceived border extraction of dot patterns in 2-D has been recently proposed. Here, a class of set estimators based on the s-shape is developed in k-dimensions, which combine all the above properties. These estimators are consistent not just under the uniform distribution, but also when samples are drawn under any continuous distribution. The order of error in estimation is independent of the dimensionality k. To illustrate the effectiveness of the proposed approach, a linear order algorithm readily derived from the definition is applied in digital domain. The role of  that controls the structure of the estimator, is analyzed.',\n",
       " '53e997d1b7602d9701fc20de': \" This paper is concerned with the problem of computingspanning tree (MST) for n points in a p-dimensionalspace where the &quot;distance&quot; between each pairof points i and j satisfies the relationship'dq max {Ixti - xtql} ,where xki is the coordinate of object i along the kttidimension. This relationship is clearly satisfied by allMinkowski metricsdq = [ Ixki - xnjl r] x/r, r &gt; 1  \",\n",
       " '53e997d1b7602d9701fc20c6': 'Agent-mediated electronic commerce has recently commanded much attention. Bidding support agents have been studied very extensively. We envision a future in which many people can trade their goods by using a bidding support agent on Internet auctions. In this paper, we formalize a situation in which people are trading their goods on Internet auctions and employing bidding support agents. Then, we prove that people who use a bidding support agent can successively win trades. Also, we prove that the situation in which every people use a bidding support agent can satisfied strategy proofness and Pareto optimality. Further, we present in the situation, unsupported bidders do not make a positive benefit.',\n",
       " '53e997d1b7602d9701fc20e2': 'The move towards integrated international Digital Libraries offers the opportunity of creating comprehensive data on citation networks. These data are not only invaluable pointers to related research, but also the basis for evaluations such as impact factors, and the foundation of smart search engines. However, creating correct citation-network data remains a hard problem, and data are often incomplete and noisy. The only viable solution appear to be systems that help authors create correct, complete, and annotated bibliographies, thus enabling autonomous citation indexing to create correct and complete citation networks. In this paper, we describe a general system architecture and two concrete components for supporting authors in this task. The system takes the author from literature search through domain-model creation and bibliography construction, to the semantic markup of bibliographic metadata. The system rests on a modular and extensible architecture: VBA Macros that integrate seamlessly into the user’s familiar working environment, the use of existing databases and information-retrieval tools, and a Web Service layer that connects them.',\n",
       " '53e997d1b7602d9701fc2111': ' . Skeletons offer the opportunity to improve parallel software developmentby providing a template-based approach to program design. However,due to the large number of architectural models available and the lackof adequate performance prediction models, such templates have to be optimisedfor each architecture separately. This paper proposes a programmingenvironment based on the Bulk Synchronous Parallel(BSP) model for multigridmethods, where key implementation decisions are made... ',\n",
       " '53e997d1b7602d9701fc239b': 'The theoretical problems of demand-side management are examined in without regard to the type of resource whose demand is to be managed, and the Maximum Demand problem is identified and addressed in a system consisting of independent processes that consume the resource. It is shown that the Maximum Demand problem generalizes the Santa Fe Bar Problem of game theory, and the basic properties of Maintenance and Strong and Weak Recovery that are desirable of systems of processes where demand management is practiced are defined. Basic algorithms are given for solving the Maximum Demand problem in a general context, and in a system where the processes have priorities.',\n",
       " '53e997d1b7602d9701fc217b': 'A fundamental challenge in data center networking is how to efficiently interconnect an exponentially increasing number of servers. This paper presents DCell, a novel network structure that has many desirable features for data center networking. DCell is a recursively defined structure, in which a high-level DCell is constructed from many low-level DCells and DCells at the same level are fully connected with one another. DCell scales doubly exponentially as the node degree increases. DCell is fault tolerant since it does not have single point of failure and its distributed fault-tolerant routing protocol performs near shortest-path routing even in the presence of severe link or node failures. DCell also provides higher network capacity than the traditional tree-based structure for various types of services. Furthermore, DCell can be incrementally expanded and a partial DCell provides the same appealing features. Results from theoretical analysis, simulations, and experiments show that DCell is a viable interconnection structure for data centers.',\n",
       " '53e997d1b7602d9701fc219c': \"Recent advances in device miniaturization have clearly contributed to make Weiser's vision of ubiquitous computing closer to reality. However, such vision is still far from being accomplished because some difficult problems remain to be solved. It is still hard to develop applications that take advantage of the disappearance of the real/virtual barrier, and to support such applications in a way that can be easily integrated with current environments. We propose a middleware approach (OSMOSIS) that employs pragmatic approach to solve the previous drawbacks so that ubiquitous computing can become a reality. It provides a file-system abstraction in order to make real objects virtual so that each real-world object has a virtual counterpart in the form of a file. Thus, the topology of a workplace, or a house, is reflected in the directories and subdirectories organization. The OSMOSIS middleware combines the use of RFID, Location and Context Management and a Policy Engine in order to provide to applications a context-aware file-system enriched with semantic-information.\",\n",
       " '53e997d1b7602d9701fc221f': 'The current status of Chinese question answering system (QA) was introduced in the paper firstly. QA is the study on the methodology that returns exact answers to natural language questions. This paper attempts to increase the speed of the system responding to the users and accuracy of QA systems. To achieve this objective, the processing includes: Firstly, it introduces the conceptual theory and parts of intelligent question-answering system in detail. Secondly, it researches the Chinese word segmentation algorithm and its relevant technology. Thirdly, it puts forward the method of calculating sum of word frequency based on the whole sentence. It makes a better improvement for processing the problems. Fourthly, it brings forward the classification problem of specific fields, which is convenient for result matching according to the kind of problem of the system. In the end, this system completes the results extraction by calculating the weight. On the basis of theory and algorithm mentioned above, a question-answering system in the course ASP.Net field is implemented.',\n",
       " '53e997d1b7602d9701fc243f': 'Correct ordering of timing quantities is essential for both timing analysis and design optimization in the presence of process variation, because timing quantities are no longer a deterministic value, but a distribution. This paper proposes a novel metric, called tiered criticalities, which guarantees to provide a unique order for a set of correlated timing quantities while properly taking into account full process space coverage. Efficient algorithms are developed to compute this metric, and its effectiveness on path ranking for at-speed testing is also demonstrated.',\n",
       " '53e997d1b7602d9701fc2452': 'The current study investigates a case where the online learning procedure in three-dimensional (3D) technologically-advanced environments of the Web 2.0 is growing at an exponential rate. In this occasion it is highly imperative need to understand students\\' interactions in this innovative mode of e-Education that requires from educators and scholars not only analysis conceptually, but also an empirically-driven optimization. The community of inquiry (CoI) model (or framework) consists to be as one of the most prominent multi-dimensional constructs that it is widely used to represent several distinct dimensions of social presence, teaching presence and cognitive presence, as a unique and fundamental theoretical concept to measure students\\' interactions in contemporary electronic environments. Although, the effectiveness of these multi-dimensional constructs creates a dilemma to researchers who want the breadth and comprehensiveness of this model for the precision and clarity of users\\' (instructors and students) dimensions with other motivational and learning variables. To address this dilemma, the current empirical study presents statistical analyses from the \\\\\"trinity\\\\\" constructs of the CoI model by utilizing correlation and hierarchical regression analyses with two fundamental motivational (computer self-efficacy and situational interest) and another one learning (academic self-concept) variables. This study goes one step further and introduces the conspicuously indisputable intervention of a virtual (V)CoI and its utilization in multi-user virtual worlds, like Second Life (SL). The study findings of one hundred thirty-five (135) participants who enrolled in several online sessions unveiled that the situational interest was the only significant predictor of social presence. The computer self-efficacy was not a significant predictor of the CoI model, while on the other hand academic self-concept was a significant predictor in a revamped attempt to validate the strong relationship among constructs within it. According to the aforementioned reasons, it can be surmised that the successful combination of the VCoI in Second Life, surpassing irrefutable and inherent shortcomings to a future-driven sustainable use and growth.',\n",
       " '53e997d1b7602d9701fc2279': '  Objective: To present an overview on the current state of the art concerning metrics-based quality evaluation of software components and component assemblies. Method: Comparison of several approaches available in the literature, using a framework comprising several aspects, such as scope, intent, definition technique, and maturity. Results: The identification of common shortcomings of current approaches, such as ambiguity in definition, lack of adequacy of the specifying formalisms and insufficient validation of current quality models and metrics for software components. Conclusions: Quality evaluation of components and component-based infrastructures presents new challenges to the Experimental Software Engineering community. ',\n",
       " '53e997d1b7602d9701fc2288': 'This paper introduces experiments of visual attention and learning of a mobile robot. The recurrent neural network (RNN) learns the sequence of events encountered during navigation incrementally as episodic memories so that the RNN can make prediction based on such sequences in the future. The visual module has two task processes to execute, namely object recognition and wall-following. Attention between these two tasks is switched by means of the top-down prediction made by the RNN. The strength of the top-down prediction acting on the vision processes is modulated dynamically using the measurement of learning status of the RNN. Our experimental results showed that the robot adapts to the environment in the course of dynamical interactions between its learning, attention and behavioral functions.',\n",
       " '53e997d1b7602d9701fc227b': \"Understanding what drives members of online content-sharing communities to contribute is key to understanding why some communities flourish while others fizzle out. Both cognitive and social explanations have been proposed with varying degrees of support from data collected from existing online communities. However, because most studies to date have tended to focus only on a whole community, it is not always clear how important that community is to its members relative to the other online communities in which they are engaged. We study the associations between member-group interactions relative members' other Flickr activity and their contributions group content (photos and comments). We introduce two sets of group quality measures. The measures in the first set indicate the level of relative engagement members have with the group while those in the second indicate the extent of interest in the group's content (photos). We find support for two theories of member-group interaction. Firstly, consistent with the idea that each member has an absolute limit to their capacity to contribute or engage, we found that members who were members of fewer other groups tended to contribute more. Secondly, we found support for the hypothesis that individuals are more likely to contribute when they believe that their contributions will be identifiable. These findings provide the basis for studying the differences between groups that arise due to different member compositions.\",\n",
       " '53e997d1b7602d9701fc2296': 'In this paper, we give learning algorithms for two new subclass of DNF formulas: poly-disjoint One-read-once Monotone DNF; and Read-once Factorable Monotone DNF, which is a generalization of Read-once Monotone DNF formulas. Our result uses Fourier analysis to construct the terms of the target formula based on the Fourier coefficients corresponding to these terms. To facilitate this result, we give a novel theorem on the approximation of Read-once Factorable Monotone DNF formulas, in which we show that if a set of terms of the target formula have polynomially small mutually disjoint satisfying sets, then the set of terms can be approximated with small error by the greatest common factor of the set of terms. This approximation theorem may be of independent interest.',\n",
       " '53e997d1b7602d9701fc253f': 'Currently, PVM constitutes a widely used software for developing parallel applications in workstation and parallel environments. In this paper we propose a processors management system for PVM which allows to assign the PVM tasks over a computers system. The Processors Management System uses two task assignment heuristics. These heuristics are based on Neural Networks and Genetic Algorithms.',\n",
       " '53e997d1b7602d9701fc2569': 'Module systems for proof assistants provide administrative support for large developments when mechanizing the meta-theory of programming languages and logics. We describe a module system for the logical framework LF that is based on two main primitives: signatures and signature morphisms. Signatures are defined as collections of constant declarations, and signature morphisms as homo-morphism in between them. Our design is semantically transparent in the sense that it is always possible to elaborate modules into the module free version of LF. We have implemented our design as part of the Twelf system and rewritten parts of the Twelf example library to take advantage of the module system.',\n",
       " '53e997d1b7602d9701fc2543': 'A fully automated 3D centerline modeling algorithm for coronary arteries is presented. It utilizes a subset of standard rotational X-ray angiography projections that correspond to one single cardiac phase. The algorithm is based on a fast marching approach, which selects voxels in 3D space that belong to the vascular structure and introduces a hierarchical order. The local 3D propagation speed is determined by a combination of corresponding 2D projections filtered with a vessel enhancing kernel.',\n",
       " '53e997d1b7602d9701fc2ee0': 'Various instance weighting methods have been proposed for instance-based transfer learning. Kernel Mean Matching (KMM) is one of the typical instance weighting approaches which estimates the instance importance by matching the two distributions in the universal reproducing kernel Hilbert space (RKHS). However, KMM is an unsupervised learning approach which does not utilize the class label knowledge of the source data. In this paper, we extended KMM by leveraging the class label knowledge and integrated KMM and SVM into an unified optimization framework called KMM-LM (Large Margin). The objective of KMM-LM is to maximize the geometric soft margin, and minimize the empirical classification error together with the domain discrepancy based on KMM simultaneously. KMM-LM utilizes an iterative minimization algorithm to find the optimal weight vector of the classification decision hyperplane and the importance weight vector of the instances in the source domain. The experiments show that KMM-LM outperforms the state-of-the-art baselines. © Springer-Verlag 2012.',\n",
       " '53e997d1b7602d9701fc2ee1': 'This paper presents a design and implementation of a ATM multicast service based on programmable and active network concepts. It aims to address the design and implementation issues of creating new network services - multicast in this case - through a set of Corba-based network interfaces, and with a java based user codes injection mechanism for supporting customization of network services. We demonstrate the feasibility of our prototype through the implementation of a wavelet video multicast application with active filters implanted at intermediate nodes for supporting heterogeneous receivers. The performance of the prototype over an ATM test-bed is measured and evaluated.',\n",
       " '53e997d1b7602d9701fc2eb6': 'With the number of high-density servers in data centers rapidly increasing, power control with performance optimization has become a key challenge to gain a high return on investment, by safely accommodating the maximized number of servers allowed by the limited power supply and cooling facilities in a data center. Various power control solutions have been recently proposed for high-density servers and different components in a server to avoid system failures due to power overload or overheating. Existing solutions, unfortunately, either rely only on the processor for server power control, with the assumption that it is the only major power consumer, or limit power only for a single component, such as main memory. As a result, the synergy between the processor and main memory is impaired by uncoordinated power adaptations, resulting in degraded overall system performance. In this paper, we propose a novel power control solution that can precisely limit the peak power consumption of a server below a desired budget. Our solution adapts the power states of both the processor and memory in a coordinated manner, based on their power demands, to achieve optimized system performance. Our solution also features a control algorithm that is designed rigorously based on advanced feedback control theory for guaranteed control accuracy and system stability. Compared with two state-of-the-art server power control solutions, experimental results show that our solution, on average, achieves up to 23% better performance than one baseline for CPU-intensive benchmarks and doubles the performance of the other baseline when the power budget is tight.',\n",
       " '53e997d1b7602d9701fc2ef7': 'Without adding any primitives to the language, we define a concurrency monad transformer in Haskell. This allows us to add a limited form of concurrency to any existing monad. The atomic actions of the new monad are lifted actions of the underlying monad. Some extra operations, such as fork, to initiate new processes, are provided. We discuss the implementation, and use some examples to illustrate the usefulness of this construction.',\n",
       " '53e997d1b7602d9701fc2f20': 'Considering the grid manager′s point of view, needs in terms of prediction of intermittent energy like the photovoltaic resource can be distinguished according to the considered horizon: following days (d+1, d+2 and d+3), next day by hourly step (h+24), next hour (h+1) and next few minutes (m+5 e.g.). Through this work, we have identified methodologies using time series models for the prediction horizon of global radiation and photovoltaic power. What we present here is a comparison of different predictors developed and tested to propose a hierarchy. For horizons d+1 and h+1, without advanced ad hoc time series pre-processing (stationarity) we find it is not easy to differentiate between autoregressive moving average (ARMA) and multilayer perceptron (MLP). However we observed that using exogenous variables improves significantly the results for MLP. We have shown that the MLP were more adapted for horizons h+24 and m+5. In summary, our results are complementary and improve the existing prediction techniques with innovative tools: stationarity, numerical weather prediction combination, MLP and ARMA hybridization, multivariate analysis, time index, etc.',\n",
       " '53e997d1b7602d9701fc3169': 'The paper presents a Coarse-Grained Multicomputer algorithm that solves the Longest Common Subsequence Problem. This algorithm can be implemented in the CGM with processors in O( ) in time and O( ) communica- tion steps. It is the first CGM algorithm for this problem. We present also experimental results showing that the CGM algorithm is very efficient.',\n",
       " '53e997d1b7602d9701fc349e': 'We study a generalization of the Muddy Children puzzle by allowing public announcements with arbitrary generalized quantifiers. We propose a new concise logical modeling of the puzzle based on the number triangle representation of quantifiers. Our general aim is to discuss the possibility of epistemic modeling that is cut for specific informational dynamics. Moreover, we show that the puzzle is solvable for any number of agents if and only if the quantifier in the announcement is positively active (satisfies a form of variety).',\n",
       " '53e997d1b7602d9701fc348c': 'We propose a new approach for automatic road extraction from aerial imagery with a model and a strategy mainly based on the multi-scale detection of roads in combination with geometry-constrained edge extraction using snakes. A main advantage of our approach is, that it allows for the first time a bridging of shadows and partially occluded areas using the heavily disturbed evidence in the image. Additionally, it has only few parameters to be adjusted. The road network is constructed after extracting crossings with varying shape and topology. We show the feasibility of the approach not only by presenting reasonable results but also by evaluating them quantitatively based on ground truth.',\n",
       " '53e997d1b7602d9701fc34c4': 'In statistical analysis of measurement results, it is often necessary to compute the range (V ;V ) of the population variance V = 1 n ¢ n X i=1 (xi ¡ E) 2 ˆ n ¢ n X i=1 xi ! when we only know the intervals (e xi ¡¢i;e xi +¢i) of possible values of the xi. While V can be computed e-ciently, the problem of computing V is, in general, NP-hard. In our previous paper \\\\Population Variance under Interval Uncertainty: A New Algorithm\" (Reliable Computing, 2006, Vol. 12, No. 4, pp. 273{280), we showed that in a practically important case, we can use constraints techniques to compute V in time O(n¢log(n)). In this paper, we provide new algorithms that compute V and, for the above case, V in linear time O(n). Similar linear-time algorithms are described for computing the range of the entropy S = ¡ n P i=1 pi ¢ log(pi) when we only know the intervals pi = (pi;pi) of possible values of probabilities pi.',\n",
       " '53e997d1b7602d9701fc349f': 'All clustering methods have to assume some cluster relationship among the data objects that they are applied on. Similarity between a pair of objects can be defined either explicitly or implicitly. In this paper, we introduce a novel multiviewpoint-based similarity measure and two related clustering methods. The major difference between a traditional dissimilarity/similarity measure and ours is that the former uses only a single viewpoint, which is the origin, while the latter utilizes many different viewpoints, which are objects assumed to not be in the same cluster with the two objects being measured. Using multiple viewpoints, more informative assessment of similarity could be achieved. Theoretical analysis and empirical study are conducted to support this claim. Two criterion functions for document clustering are proposed based on this new measure. We compare them with several well-known clustering algorithms that use other popular similarity measures on various document collections to verify the advantages of our proposal.',\n",
       " '53e997d1b7602d9701fc34cc': 'We have developed a software package that simulates the operation of a silicon micromachined CMOS thermal conductivity gas pressure gauge. The performance of actual devices was compared against the simulated operation and was found to be in good agreement. The 3-dimensional simulation was reduced to two 2-dimensional simulations to reduce complexity. The two equations resulting from steady state energy balance considerations were discretized and an iterative nonlinear Gauss-Seidel procedure applied to solve the system of equations. Temperature profiles and contours were calculated and the effect of geometric and materials modifications was demonstrated',\n",
       " '53e997d1b7602d9701fc3691': 'Manufacturing process planning plays a significant role by providing collaborative integration capabilities, from design to manufacturing. This situation is more acute in the tooling production industry, which is characterised by short lead-time and frequent dynamic customer requirements. A framework for collaborative integration between computerised design and manufacturing has been developed to support the integration of product design with manufacturing process planning. A prototype system based on this framework has been developed and tested in a tooling job shop with satisfactory results. The integration framework is also expected to be effectively applied in other similar enterprise application domains.',\n",
       " '53e997d1b7602d9701fc3694': 'Golomb and Gong (1997 and 1999) considered binary sequences with the trinomial property. In this correspondence we shall show that the sets of those sequences are (quite trivially) closely connected with binary cyclic codes with codewords of weight three. This approach gives us another way to deal with trinomial property problems. After disproving one conjecture formulated by Golomb and Gong, we exhibit an infinite class of sequences which do not have the trinomial property, corresponding to binary cyclic codes of length 2m-1 with minimum distance exactly four',\n",
       " '53e997d1b7602d9701fc3662': \"Much has been written in recent years about the changes in corporate strategies and industry structures associated with electronic coordination of market activities. This paper considers the advent of electronic market coordination in the home mortgage industry, focusing on Computerized Loan Origination (CLO) systems. Case studies of five CLOs (First Boston's Shelternet, PRC's Loan Express, American Financial Network's Rennie Mae, Prudential's CLOS, and Citicorp's Mortgage Power Plus) reveal a range of system functionalities. Predictions from the Electronic Markets Hypothesis (EMH) are tested against the empirical results of the five case studies. As suggested by the EMH, financial intermediaries have been threatened by the introduction of CLOs, and in some cases, opposition has been mounted against the systems. On the other hand, despite the availability of the technology and mortgages' seemingly favorable characteristics as an electronically mediated market product, the industry has not been fundamentally changed by the introduction of these systems, despite more than a decade of experience with them. Of the two case studies that could be characterized as electronic markets, neither continues to exist in that form today. And the system with the largest dollar volume of mortgages of the five is best characterized as an electronic hierarchy. These results suggest that either the full results predicted by the EMH require a longer gestation period or that the underlying hypothesis will require augmentation in order to fully explain the results in the home mortgage market.\",\n",
       " '53e997d1b7602d9701fc371f': 'FlySPEC is a video camera system designed for real-time remote operation. A hybrid design combines the high resolution of an optomechanical video camera with the wide field of view always available from a panoramic camera. The control system integrates requests from multiple users so that each controls a virtual camera. The control system seamlessly integrates manual and fully automatic control. It supports a range of options from untended automatic to full manual control. The system can also learn control strategies from user requests. Additionally, the panoramic view is always available for an intuitive interface, and objects are never out of view regardless of the zoom factor. We present the system architecture, an information-theoretic approach to combining panoramic and zoomed images to optimally satisfy user requests, and experimental results that show the FlySPEC system significantly assists users in a remote inspection tasks.',\n",
       " '53e997d1b7602d9701fc3752': 'The increasing availability of Web Services asked for investigating ways to automate the discovery process. Discovery processes enhanced with semantics can be recognize to be general, but often they lack the flexibility needed in specific domains. In ...',\n",
       " '53e997d1b7602d9701fc3751': 'False data filtering is an important issue in wireless sensor networks. In this paper, we consider a new type of false data injection attacks called collaborative false data injection, and propose two schemes to defend such attacks. In collaborative false data injection attacks, multiple compromised nodes collaboratively forge a fake report and inject the report into the network. This type of attacks is hard to defend with existing approaches, because they only verify a fixed number of message authentication codes (MACs) carried in the data report but the adversary can easily obtain enough compromised nodes from different geographical areas of the network to break their security. Our novel solution is to bind the keys of sensor nodes to their geographical locations, and verify the legitimacy of a data report by checking whether the locations of the sensors endorsing the report are logical (e.g., the sensors should be close enough to each other to sense the same event). We propose two filtering schemes: The geographical information based false data filtering scheme (GFFS) which utilizes the absolute positions of sensors in the verification, and the neighbor information based false data filtering scheme (NFFS) which utilizes relative positions of sensors when absolute positions cannot be obtained. We theoretically analyze the filtering probability of the two proposed schemes, and evaluate their performance through extensive simulations. Simulation results show that, when there are totally ten nodes compromised in a 400 nodes network, the detection probability of collaborative false data injection attacks is higher than 97% in GFFS and NFFS, but is less than 7% in traditional false data filtering approaches such as SEF.',\n",
       " '53e997d1b7602d9701fc3f08': 'TCP based attack is a well known security problem that leads to consumption of mobile devices resources such as bandwidths, batteries as well as memory. The attack is common in new environments providing TCP-based network services (web service, email service) such as peer to peer networks and scenarios where wireless terminals act as servers. Verifying sources sending synchronize (SYN), acknowledge (ACK) or reset (RST) has been a great challenge. The existing solutions have focused much on verifying sources sending SYN requests and therefore encouraging attackers to use invalid RSTs and ACKs thus rendering the TCP servers ineffective. This paper describes two mechanisms that verify the sources sending SYN requests, ACK and RST in order to distinguish invalid requests and responses from legitimate ones. The solution requires minimum modifications to the existing firewalls and reduces attackers’ effective rate significantly.',\n",
       " '53e997d1b7602d9701fc37bf': 'We describe work on automatically assigning classification labels to books using the Library of Congress Classification scheme. This task is non-trivial due to the volume and variety of books that exist. We explore the utility of Information Extraction (IE) techniques within this text categorisation (TC) task, automatically extracting structured information from the full text of books. Experimental evaluation of performance involves a corpus of books from Project Gutenberg. Results indicate that a classifier which combines methods and tools from IE and TC significantly improves over a state-of-the-art text classifier, achieving a classification performance of Fβ=1 = 0.8099.',\n",
       " '53e997d1b7602d9701fc3f10': 'Domain scientists synthesize different data and computing resources to solve their scientific problems. Making use of distributed execution within scientific workflows is a growing and promising way to achieve better execution performance and efficiency. This paper presents a high-level distributed execution framework, which is designed based on the distributed execution requirements identified within the Kepler community. It also discusses mechanisms to make the presented distributed execution framework easy-to-use, comprehensive, adaptable, extensible and efficient.',\n",
       " '53e997d1b7602d9701fc3f34': 'In this paper, the average successful throughput, i.e., goodput, of a coded 3-node cooperative network is studied in a Rayleigh fading environment. It is assumed that a simple automatic repeat request (ARQ) technique is employed in the network so that erroneously received packets are retransmitted until successful delivery. The relay is assumed to operate in either amplify-and-forward (AF) or decode-and-forward (DF) mode. Under these assumptions, retransmission mechanisms and protocols are described, and the average time required to send information successfully is determined. Subsequently, the goodput for both AF and DF relaying is formulated. The tradeoffs and interactions between the goodput, transmission rates, and relay location are investigated and optimal strategies are identified.',\n",
       " '53e997d1b7602d9701fc3f38': 'We show that the fixed power, synchronous Interference Avoidance (IA) scheme of (3) employing the (greedy) eigen-iteration can be modeled as the recently developed potential game of (10). Motivated by the fact that receivers can make small mistakes, we consider the convergence of the eigen- iteration when noise is added in a manner similar to (2). Further, we restrict ourselves to a class of signal environments that we call levelable environments. Applying game-theory, we obtain a convergence result similar to that of (2) for levelable environments: arbitrarily small noise assures that the eigen- iteration almost surely converges to a neighborhood of the optimum signature set.',\n",
       " '53e997d1b7602d9701fc3f44': 'Ecotones are zones of transition between two adjacent ecological systems and are characterized by a high rate of change compared to these adjacent areas. They are dynamic entities with both a spatial and temporal property, reflected in an ecotone width and location, which vary across time during succession or environmental change on both a local or global scale. Various techniques have been proposed to characterize ecotones, one of them being a sigmoid wave curve fit on the transects across the ecotone. In this paper, we test the robustness of a sigmoid wave model approach on simulated ecotone data with a varying degree of steepness, patchiness and transect length. An analysis of variance (ANOVA) provided us details on the sensitivity of the estimated ecotone width for the steepness, the transect length as well as for the patchiness of the ecotone. The statistics also allowed us to investigate the interaction between the different parameters on the resulting ecotone width. We conclude that the sigmoid wave curve-fitting algorithm provides a robust way to describe ecotones with various degrees of steepness and patchiness. Depending on the transect window size used, a sigmoid wave curve-fitting algorithm will pick up variations in ecotone steepness or in ecotone steepness and patchiness.',\n",
       " '53e997d1b7602d9701fc3f4e': 'Many disciplines have been proposed for scheduling and processor al- location in multiprogrammed multiprocessors for parallel processing. These have been, for the most part, designed and evaluated for workloads having relatively low variability in service demand. But with reports that var iability in service de- mands at high performance computing centers can actually be quite high, these disciplines must be reevaluated. In this paper, we examine the performance of two well-known static scheduling disciplines, and propose preemptive versions of these that offer much better mean response times when the variability in ser- vice demand is high. We argue that, in systems in which dynamic repartitioning in applications is expensive or impossible, these preempti ve disciplines are well suited for handling high variability in service demand.',\n",
       " '53e997d1b7602d9701fc4170': \"Memory activation has been modeled in symbolic architectures in the past, but usually at the level of individual chunks or productions in long term memory. Recent research (Chong 2003) has demonstrated activation at the lev el of individual elements of working memory. In this paper, we present a comprehensive implementation of working memory activation in Soar that takes advantage of the uniq ue characteristic of Soar's working memory structure, namely persistence. We also explore modifications to activ ation so that the activation of new working memory elements is not a fixed level, but is based on the activation of the working memory elements tested in its creation. We demonstrate our model in terms of how it aids the selection of feat ures relevant to learning.\",\n",
       " '53e997d1b7602d9701fc404c': \"This paper deals with the efficiency of systems consisting of multiple power sources. The operating point ensuring maximum efficiency is defined and solved analytically. Moreover, it is shown that, in the case of linear sources, the maximum efficiency is determined solely by the load power and the network's maximum power, independently of the sources' internal construction and their connectivity. The generic circuit theory treatment that is proposed applies to distributed power generation (smart grid, photovoltaic, fuel cells, etc.) and to distributed load systems (LED arrays, microprocessors).\",\n",
       " '53e997d1b7602d9701fc411a': '  As an alternative view to the graph formation models in the statistical physics community, we introduce graph formation models using \\\\textit{network formation} through selfish competition as an approach to modeling graphs with particular topologies. We further investigate a specific application of our results to collaborative oligopolies. We extend the results of Goyal and Joshi (S. Goyal and S. Joshi. Networks of collaboration in oligopoly. Games and Economic behavior, 43(1):57-85, 2003), who first considered the problem of collaboration networks of oligopolies and showed that under certain linear assumptions network collaboration produced a stable complete graph through selfish competition. We show with nonlinear cost functions and player payoff alteration that stable collaboration graphs with an arbitrary degree sequence can result. ',\n",
       " '53e997d1b7602d9701fc4192': 'Asharov, Canetti, and Hazay (Eurocrypt 2011) studied how game-theoretic concepts can be used to capture the cryptographic properties of correctness, privacy, and fairness in two-party protocols in the presence of fail-stop adversaries. Based on their work, we characterize the properties of \"two-message\" oblivious transfer protocols by using a game-theoretic concept. Specifically, we present a single two-player game for two-message oblivious transfer in the game-theoretic framework, where it captures the cryptographic properties of correctness and privacy in the presence of malicious adversaries.',\n",
       " '53e997d1b7602d9701fc4341': 'In this paper we thoroughly analyze a distributed procedure for the problem of local database update in a network of database peers, useful for data exchange scenarios The algorithm supports dynamic networks: even if nodes and coordination rules appear or disappear during the computation, the proposed algorithm will eventually terminate with a sound and complete result.',\n",
       " '53e997d1b7602d9701fc4431': 'As a superior approach for peer-to-peer (P2P) streaming live media, mesh-based overlay has made much success due to its potential scalability and ease of deployment. Peers in mesh streaming systems employ a swarming content delivery mechanism to exchange resources. However, the limited availability of new content heavily affects the quality of delivered stream. Aiming at leveraging and optimizing the resources of participating peers to distribute contents, we propose ORSP, an efficient resource acquisition policy for mesh content distribution services. Instead of choosing coming stream randomly with limited bandwidth, ORSP (i) instructs each peer to derive a combination of stream from a gathered subset of peers that maximizes the delivered stream, and (ii) guarantees that peers with more available stream are firstly selected to provide stream in order to balance the whole network resources. Simulation results show that ORSP optimizes stream selection manner that enables peers to obtain more available resources, distribution of system resources is prone to be relatively average that helps to decrease the node failure rate for node join in and drop out, accompanied with a little increased source-to-end delay to some degree.',\n",
       " '53e997d1b7602d9701fc4450': \"Dushnik and Miller defined the dimension of a partially ordered setX, DimX, as the minimum number of linear extensions ofX whose intersection is the partial ordering onX. The concept of dimension for a partially ordered set has applications to preference structures and the theory of measurement. Hiraguchi proved that DimX = [|X|/2] when |X| = 4. Bogart, Trotter, and Kimble gave a forbidden subposet characterization of Hiraguchi's inequality by constructing for eachn = 2 the minimum collectionBn of posets such that if [|X|/2] =n = 2, then DimX  unlessX contains one of the posets inBn. Recently Trotter gave a simple proof of Hiraguchi's inequality based on the following theorem. IfA is an antichain ofX and |X - A| =n = 2, then DimX = n. In this paper we give a forbidden subposet characterization of this last inequality.\",\n",
       " '53e997d1b7602d9701fc43ce': 'Lately, we have been proposing a novel concept for a wide area haptic feedback device. It is based on a deformable mechanical structure capable of morphing its shape in order to imitate a desired object. Due to the physical presence of the resulting shape, the latter can intuitively be touched and explored with the whole hand. The prototype has been called SmartMesh [1]. After a short review of the SmartMesh mechanism, this paper focuses on the controllability issue of such a multi-degree of freedom structure and introduces the concept and the implementation of the control algorithm responsible for providing the required parameters for the actuation of the mechanical structure. The results emphasize the amazing deformation capability and expressiveness of a future haptic feedback enabled user interface device based on the SmartMesh concept.',\n",
       " '53e997d1b7602d9701fc4433': \"The basic algorithmic structures are fundamental to systems development and understanding them is an essential step towards the learning process. In order to become a good developer the student needs to refine his learning outcomes to foresee beyond syntax and semantics of a programming language. However, we found that many students face difficulties in that learning process. Thus, it is important to adopt strategies that best fits this situation in order to provide the necessary conditions to achieve success. For that purpose a tool provided with resources for the teachers to plan group problem solving exercises would help students keeping themselves engaged on the programming activities. In this regard we are developing scenarios using virtual worlds. Such environments have been used on many educational contexts, and we believe that if we include Polya's problem solving method as a means for start solving the collaborative tasks described within the scenarios students will propose strategies for solving the tasks, formalize them later on as algorithms. Those activities will promote integration between teacher and students at the same time as students practice their algorithm building skills.\",\n",
       " '53e997d1b7602d9701fc44b1': 'We consider the effects of temporal delay in a neural feedback system with excitation and inhibition. The topology of our model system reflects the anatomy of the avian isthmic circuitry, a feedback structure found in all classes of vertebrates. We show that the system is capable of performing a ‘winner-take-all’ selection rule for certain combinations of excitatory and inhibitory feedback. In particular, we show that when the time delays are sufficiently large a system with local inhibition and global excitation can function as a ‘winner-take-all’ network and exhibit oscillatory dynamics. We demonstrate how the origin of the oscillations can be attributed to the finite delays through a linear stability analysis.',\n",
       " '53e997d1b7602d9701fc4910': 'We propose an adaptive redundancy control scheme for a systematic erasure coding scheme to transmit real time data in the Internet. Since the recovery rate of packet loss depends on the amount of redundancy data, we use the redundancy estimation algorithm which considers the loss burstiness. The Gilbert model is used for modeling the loss process and adjusting the number of redundant packets. Analysis and experimental results show that the proposed scheme is efficient for loss recovery in a consecutive loss environments like the Internet',\n",
       " '53e997d1b7602d9701fc48e1': \"We describe a user interface for wireless information devices, specifically designed to facilitate learning about users' individual interests in daily news stories. User feedback is collected unobtrusively to form the basis for a content-based machine learning algorithm. As a result, the described system can adapt to users' individual interests, reduce the amount of information that needs to be transmitted, and help users access relevant information with minimal effort.\",\n",
       " '53e997d1b7602d9701fc4ba1': 'Most of current research in wireless networked embedded sensing approaches the problem of application design as one of manually customizing network protocols. The design complexity and required expertise make this unsuitable for increasingly complex sensor network systems. We address this problem from a parallel and distributed systems perspective and propose a methodology that enables domain experts to design, analyze, and synthesize sensor network applications without requiring a knowledge of implementation details. At the core of our methodology is a virtual architecture for a class of sensor networks that hides enough system details to relieve programmers of the burden of managing low-level control and coordination, and provides algorithm designers with a clean topology and cost model.We illustrate this methodology using a real-world topographic querying application as a case study.',\n",
       " '53e997d1b7602d9701fc4a94': 'The University of Chicago participated in the Cross-Language Evaluation Forum 2004 (CLEF2004) cross-language multilingual, bilingual, and spoken language tracks. Cross-language experiments focused on meeting the challenges of new languages with freely available resources. We found that modest effectiveness could be achieved with the additional application of pseudo-relevance feedback to overcome some gaps in impoverished lexical resources. Experiments with a new dimensionality reduction approach for re-ranking of retrieved results yielded no improvement, however. Finally, spoken document retrieval experiments aimed to meet the challenges of unknown story boundary conditions and noisy retrieval through query-based merger of fine-grained overlapping windows and pseudo-feedback query expansion to enhance retrieval.',\n",
       " '53e997d1b7602d9701fc4e36': 'In this paper, we established travelling wave solutions of the nonlinear equation. The first integral method was used to construct travelling wave solutions of the Cahn–Allen equation. The obtained results include periodic and solitary wave solutions. The power of this manageable method is confirmed.',\n",
       " '53e997d1b7602d9701fc4f9c': '  Non-negative matrix factorization (NMF) has proved effective in many clustering and classification tasks. The classic ways to measure the errors between the original and the reconstructed matrix are $l_2$ distance or Kullback-Leibler (KL) divergence. However, nonlinear cases are not properly handled when we use these error measures. As a consequence, alternative measures based on nonlinear kernels, such as correntropy, are proposed. However, the current correntropy-based NMF only targets on the low-level features without considering the intrinsic geometrical distribution of data. In this paper, we propose a new NMF algorithm that preserves local invariance by adding graph regularization into the process of max-correntropy-based matrix factorization. Meanwhile, each feature can learn corresponding kernel from the data. The experiment results of Caltech101 and Caltech256 show the benefits of such combination against other NMF algorithms for the unsupervised image clustering. ',\n",
       " '53e997d1b7602d9701fc4fc8': 'Source model extraction---the automated extraction of information from system artifacts---is a common phase in reverse engineering tools. One of the major challenges of this phase is creating extractors that can deal with irregularities in the artifacts ...',\n",
       " '53e997d1b7602d9701fc4fcb': 'In this paper, two anticontrol algorithms for synthesis of discrete chaos are introduced. In these algorithms, the control parameter of a discrete dynamical system is switched, either randomly or in a deterministic way, between two or more values corresponding to periodic motions, the result being chaotic behavior. These algorithms require no knowledge of specific mathematical properties of the underlying map modeling the system. The existence of chaos is demonstrated using various tools including graphical iteration, histogram, Lyapunov exponent and surrogate tests. In this paper, these very simple and implementable chaotifiers are applied to the logistic map.',\n",
       " '53e997d1b7602d9701fc4fd2': 'Kernpunkte\\xa0\\xa0Die Prozesslandschaft als Menge aller in einer Organisation ausgeführten Geschäftsprozesse ist das neue Handlungsfeld der\\n Prozessgestaltung in öffentlichen Verwaltungen. Zur Modellierung und Analyse einer Prozesslandschaft ist ein neuer methodischer\\n Ansatz erforderlich:\\n \\n \\n \\n –\\xa0\\n \\n Die PICTURE-Methode dient zur modellierung der Prozesslandschaft. Sie besteht aus 24 domänenspezifischen Prozessbausteinen\\n mit festem Abstraktionsniveau, die durch Attribute näher spezifiziert und zu einzelnen Prozessen zusammengesetzt werden können.\\n \\n \\n \\n \\n –\\xa0\\n \\n Mit der PICTURE-Methode lässt sich die Prozesslandschaft deutlich effizienter modellieren, als dies klassische Modellierungsmethoden\\n leisten können.\\n \\n \\n \\n \\n –\\xa0\\n \\n Die PICTURE-Methode wurde im Praxiseinsatz in einem Modellierungsprojekt der Universität Münster evaluiert und hat die an\\n sie gestellten Anforderungen erfüllt.\\n \\n \\n \\n \\n \\n \\n ',\n",
       " '53e997d1b7602d9701fc4ff8': 'In this paper, we present an improved region-based active contour/surface model for 2D/3D brain MR image segmentation. Our model combines the advantages of both local and global intensity information, which enable the model to cope with intensity inhomogeneity. We define an energy functional with a local intensity fitting term and an auxiliary global intensity fitting term. In the associated curve evolution, the motion of the contour is driven by a local intensity fitting force and a global intensity fitting force, induced by the local and global terms in the proposed energy functional, respectively. The influence of these two forces on the curve evolution is complementary. When the contour is close to object boundaries, the local intensity fitting force became dominant, which attracts the contour toward object boundaries and finally stops the contour there. The global intensity fitting force is dominant when the contour is far away from object boundaries, and it allows more flexible initialization of contours by using global image information. The proposed model has been applied to both 2D and 3D brain MR image segmentation with promising results.',\n",
       " '53e997d1b7602d9701fc500a': 'We have used a bioinformatics approach for the identification and reconstruction of metabolic pathways associated with amino acid metabolism in human mitochondria. Human mitochondrial proteins determined by experimental and computational methods have been superposed on the reference pathways from the KEGG database to identify mitochondrial pathways. Enzymes at the entry and exit points for each reconstructed pathway were identified, and mitochondrial solute carrier proteins were determined where applicable. Intermediate enzymes in the mitochondrial pathways were identified based on the annotations available from public databases, evidence in current literature, or our MITOPRED program, which predicts the mitochondrial localization of proteins. Through integration of the data derived from experimental, bibliographical, and computational sources, we reconstructed the amino acid metabolic pathways in human mitochondria, which could help better understand the mitochondrial metabolism and its role in human health.',\n",
       " '53e997d1b7602d9701fc4ffb': 'Agile methods emerge as an alternative to improve quality and performance in software development processes. However, as agile methods are essentially focused on human aspects, their application in companies depends mostly on their adequacy to the current organizational culture. This study explores the view of the organizational culture in three levels as a theoretical framework to allow early detection of problems, which could jeopardize the adoption of agile methods by a company. This article points out that many facilitators or obstacles to the adoption of an agile method can be hidden in the lower levels of the organizational culture. Additionally, the article shows that a superficial analysis of those issues can lead to a miscomprehension about the possibility of applying an agile method in a software company. This article also evidences that the interpretation of the levels of organizational culture improves the understanding of how an agile culture should be established. Copyright © 2009 John Wiley & Sons, Ltd.',\n",
       " '53e997d1b7602d9701fc5015': 'Several French national projects have been achieved last years, allowing linguist and computer scientists working on French Sign Language (FSL) to establish a permanent collaboration. During these projects, several video FSL corpora have been realised. One of the research orientations induced by these projects relates to multi-disciplinary annotation. From the computer scientists side, the aim is to develop an annotation software integrating different kind of tools based on image processing and 3d modelling that will be used to automatically annotate both lexical and syntactic information. This article presents several of these components. A first version of the annotation software integrating these components is under development.',\n",
       " '53e997d1b7602d9701fc5014': 'This paper deals with the design aspect of a software aiming at modeling the anatomical and pathological structures of patients from medical images, for diagnosis purposes. In terms of functionalities, it allows to combine image processing algorithms, and to visualize and manipulate 3D models and images. The proposed software uses specific extensible and reusable components and a system managing their combination, thanks to a formal XML-based description of their interfaces. This architecture facilitates the dynamic integration of new functionalities, in particular in terms of image processing algorithms. We describe the structural and behavioral aspects of the proposed reusable component-based architecture. We also discuss the potential of this work for developing other softwares in the field of computer aided surgery.',\n",
       " '53e997d1b7602d9701fc5024': 'In this paper we study the use of precomputed fault dictionaries to diagnose stuck-at and bridging defects in the UltraSPARC/sup TM/-I processor. In constructing the dictionary we analyze the effect of the dictionary format on parameters such as memory size, computational effort, and diagnostic resolution. The dictionary is built based on modeled stuck-at faults. However to effectively diagnose both stuck-at and bridging faults, we employ a novel procedure that combines dictionary information with potential bridge defects extracted from layout. Experiments with failing devices show excellent correlation of predicted errors with actual defects.',\n",
       " '53e997d1b7602d9701fc5357': 'In this paper, we extend the machine reassignment model proposed by Google for the ROADEF/EURO Challenge. The aim of the challenge is to develop algorithms for the efficient solutions of data-center consolidation problems. The problem stated in the challenge mainly focus on dependability requirements and does not take into account performance requirements (end-to-end response times). We extend the Google problem definition by modeling and constraining end-to-end response times. We provide experimental results to show the effectiveness of this extension.',\n",
       " '53e997d1b7602d9701fc5363': 'Robotics, artificial intelligence and, in general, any activity involving computer simulation and engineering relies, in a fundamental way, on mathematics. These fields constitute excellent examples of how mathematics can be applied to some area of investigation with enormous success. This, of course, includes embodied oriented approaches in these fields, such as Embodied Artificial Intelligence and Cognitive Robotics. In this chapter, while fully endorsing an embodied oriented approach to cognition, I will address the question of the nature of mathematics itself, that is, mathematics not as an application to some area of investigation, but as a human conceptual system with a precise inferential organization that can be investigated in detail in cognitive science. The main goal of this piece is to show, using techniques in cognitive science such as cognitive semantics and gestures studies, that concepts and human abstraction in general (as it is exemplified in a sublime form by mathematics) is ultimately embodied in nature.',\n",
       " '53e997d1b7602d9701fc548c': 'A truly personal and reactive computer system should have access to the same information as its user, including the ambient sights and sounds. To this end, we have developed a system for extracting events and scenes from natural audio/visual input. We find our system can (without any prior labeling of data) cluster the audio/visual data into events, such as passing through doors and crossing the street. Also, we hierarchically cluster these events into scenes and get clusters that correlate with visiting the supermarket, or walking down a busy street.',\n",
       " '53e997d1b7602d9701fc531d': 'This paper presents a maximum power point tracking (MPPT) control method for high efficient direct methanol fuel cell (DMFC) generation systems via T-S fuzzy model. In detail, we consider a DC/DC boost converter and incremental resistance method to regulate the output power of the DMFC to increase the output power under various conditions. First, the nonlinear boost converter system is represented by the T-S fuzzy model based control. Then, a fuzzy MPPT controller is proposed to achieve the MPPT control, in which the controller gains are obtained by solving linear matrix inequalities (LMIs). Finally, the satisfactory performance is shown from the simulations even under varying fuel cell conditions.',\n",
       " '53e997d1b7602d9701fc533f': 'System level power management must consider the uncertainty and variability that comes from the environment, the application and the hardware. A robust power management technique must be able to learn the optimal decision from past history and improve itself as the environment changes. This paper presents a novel online power management technique based on model-free constrained reinforcement learning (RL). It learns the best power management policy that gives the minimum power consumption for a given performance constraint without any prior information of workload. Compared with existing machine learning based power management techniques, the RL based learning is capable of exploring the trade-off in the power-performance design space and converging to a better power management policy. Experimental results show that the proposed RL based power management achieves 24% and 3% reduction in power and latency respectively comparing to the existing expert based power management.',\n",
       " '53e997d1b7602d9701fc54ef': 'Intelligent Automation, Inc. has performed a study of a six-degree-of-freedom (dof) active vibration isolation system based on a Stewart platform mechanism to be used for precision control of a wide range of space-based structures as well as earth-based systems. This article presents part of the study results, which includes a new Terfenol-D actuator design and analysis, a design of a Stewart platform as a vibration isolation device, and robust adaptive filter algorithms for active vibration control. Prototype hardware of a six-dof active vibration isolation system has been implemented and tested. About 30 dB of vibration attenuation is achieved in real-time experiments.',\n",
       " '53e997d1b7602d9701fc5878': 'Key frames are the subset of still images which best represent the content of a video sequence in an abstracted manner. In other words, video abstraction transforms an entire video clip to a small number of representative images. We present a scheme for object-based video abstraction facilitated by an efficient video-object segmentation (VOS) system. In such a framework, the concept of a \"key frame\" is replaced by that of a \"key video-object plane (VOP).\" In order to achieve an online object-based framework such as an object-based video surveillance system, it becomes essential that semantically meaningful video objects are directly accessed from video sequences. Moreover, the extraction of key VOPs needs to be automated and context dependent so that they maintain the important contents of the video while removing all redundancies. Once a VOP is extracted, the shape of the VOP needs to be well described. To this end, both region-based and contour-based shape descriptors are investigated, and the region-based descriptor is selected for the proposed system. The key VOPs are extracted in a sequential manner by successive comparison with the previously declared key VOP. Experimental results on the proposed online processing scheme combined with efficient VOS show the proposed integrated scheme generates desirable summarizations of surveillance videos.',\n",
       " '53e997d1b7602d9701fc59e1': 'In this paper, the method of well-combined semantics and syntax proposed by Pavelka is applied to the research of the propositional calculus formal system L*. The partial constant values are taken as formulas, formulas are fuzzifled in two manners of semantics and syntax, and inferring processes are fuzzified. A sequence of new extensions {L-n*} of the system L* is proposed, and the completeness of L-n* is proved.',\n",
       " '53e997d1b7602d9701fc5aec': 'Wireless mobile grids are one of the emerging grid types, which help to pool\\nthe resources of several willing and cooperative mobile devices to resolve a\\ncomputationally intensive task. The mobile grids exhibit stronger challenges\\nlike mobility management of devices, providing transparent access to grid\\nresources, task management and handling of limited resources so that resources\\nare shared efficiently. Task execution on these devices should not be affected\\nby their mobility. The proposed work presents performance evaluation of\\nwireless mobile grid using normal walk mobility model. The normal walk model\\nrepresents daily motion of users and the direction of motion is mostly\\nsymmetric in a real life environment, thus it is effective in location updating\\nof a mobile station and in turn helps task distribution among these available\\nmobile stations. Some of the performance parameters such as Task Execution\\nTime, task failure rate, communication overhead on Brokering Server and\\nMonitoring Cost are discussed.',\n",
       " '53e997d1b7602d9701fc5b57': 'Enhancement of security using hypervisors is an effective approach that has been extensively studied. This paper is concerned with hypervisors using the parapass-through architecture, in which most of the I/O accesses from the operating system are passed through the hypervisor, while the minimum accesses necessary to implement security functionality are mediated by the hypervisor. Parapass-through hypervisors can provide various security functionalities such as encryption of storage data and creation of virtual private networks. Although a previous study has detailed a method for protecting privacy with a parapass-through hypervisor, it has not yet clarified a method for detecting malware. In this paper, we propose a scheme for incorporating malware detection functionality into a parapass-through hypervisor. Using this scheme, we implemented BVMD, an extension of a parapass-through hypervisor BitVisor, for malware detection. BVMD detects malware by comparing the contents of the data I/O with the malware signatures. A major advantage of BVMD is that its detection depends only slightly on the guest operating system. We confirmed through experiments that BVMD could detect many in-the-wild malware.',\n",
       " '53e997d1b7602d9701fc5b67': 'We propose two extensions of the Local Similarity Pattern/Weighted Local Similarity Pattern (LSP/WLSP) image similarity models, proposed by Stejić (2002). The objective is to improve the retrieval precision, by increasing the expressive power of the LSP/WLSP models. We formalize LSP and WLSP as special cases of a general similarity model, which defines image similarity as a weighted mean of the corresponding region similarities, and, in turn, each region similarity as a weighted mean of the corresponding feature similarities. In the case of LSP/WLSP models, region and feature weights have discrete non-negative values. The proposed two extensions are: (1) LSP-C+/WLSP-C+ models, with continuous non-negative weight values; and (2) LSP-C±/WLSP-C± models, with continuous positive and negative weight values. Similar to the LSP/WLSP, the proposed four models are incorporated in the relevance feedback mechanism, using genetic algorithm (GA) to infer the (sub-)optimal assignment of region and feature weights, which maximizes the similarity between the query image and the set of relevant images, chosen by the user. Accordingly, for the weight inference, GAs used for the LSP/WLSP models are extended from the integer-coded to the real-coded ones, and, in addition, new chromosome coding, two crossover modifications, six new mutation types, and a weights normalization operator are proposed. The proposed four models are evaluated on five test databases, with around 2500 images, covering 62 semantic categories. Compared with the existing image similarity models, including LSP/WLSP, and other models based on relevance feedback, proposed LSP-C±/WLSP-C± models bring in average over 10% increase in the retrieval precision. The main contributions, not limited to the LSP/WLSP models and the image retrieval, are: (1) the introduction of negative weights into the weighted mean of similarities, which increases the expressive power of the similarity model, and results in a significantly higher retrieval precision; and (2) new real-coded genetic operators suitable for the weight inference in general.',\n",
       " '53e997d1b7602d9701fc5c45': 'When countries move from manufacture-oriented to service-oriented societies, the service sector becomes ever more important. Through this development, coalitions of industry and service companies play a crucial role to respond to individualized customer demands. Those coalitions are referred to as service networks. A vivid IS community in Europe and the United States emerged, to explore and advance the role of IS in service networks. On their way towards global economic leadership, service networks also play a crucial role for the BRIC countries (Brazil, Russia, India, and China). However, so far research on that topic is at an early development stage in those emerging markets. A Brazilian-German workshop held in 2011 in Florianopolis brought design-oriented researchers from both countries together. In this paper we present the identified research challenges for IS research on service networks, potential contributions to meet these challenges and conclude with a research agenda for the forthcoming years.',\n",
       " '53e997d1b7602d9701fc5e70': 'Software specification and implementation techniques based on state machines simplify design, coding, and validation. However, large systems require complex state machines. Incremental construction techniques can control this complexity. In this paper, we present a construction technique that permits derivation of complex state machines from simpler state machines. The technique uses subclassing, composition, delegation, and genericity to incrementally modify and combine simpler machines.In addition, we present a novel implementation technique that uses exactly one table-lookup and one addition to dispatch events on derived state machines, no matter the depth of the derivation. As an example, we describe the derivation of a complicated distributed virtual memory scheme from a simple paging virtual memory scheme.',\n",
       " '53e997d1b7602d9701fc5ebe': 'In this paper we model a scenario where a buyer reserves capacity from one or more suppliers in the presence of demand uncertainty. We explicitly derive suppliers’ capacity reservation price, which is a function of their capacity, amount of capacity reserved by the buyer and other parameters. The buyer operates in a “built-to-order” environment and needs to decide how much capacity to reserve and from how many suppliers. For a strategy of equal allocation of capacity among the selected suppliers we develop closed form solutions and show that the model is robust to the number of suppliers from whom capacity is procured through reservation. When the parameters of demand distribution changes the supply base is likely to remain more or less the same. Our analysis further shows that increasing the number of pre-qualified suppliers does not provide significant advantages to the buyer. On the other hand, a pre-qualified supply base with greater capacity heterogeneity will benefit the buyer.',\n",
       " '53e997d1b7602d9701fc5ec0': 'Mobile communication networks experience a tremendous growth. According to the vision of Wireless World Research Forum (WWRF), there will be 7 trillion wireless devices serving 7 billion people by the year 2017. Ubiquitous access to information anywhere, anytime and anyhow at low cost is one of the essential features of future mobile communication networks, which will interconnect a heterogeneity of various systems and be much more dynamic and flexible in terms of changes in access technology, topology, services, etc. Moreover, there will be a need to match resources supply with application demands as these demands are expected to fluctuate over time. This makes an adequate service placement in such networks of a major importance. In this paper we survey the existing service placement mechanisms and present a qualitative comparison of existing mechanisms. Moreover we also highlight the short comings of existing approaches so that the new approaches can remove the short comings of existing approaches.',\n",
       " '53e997d1b7602d9701fc67f6': 'We study load balancing problems of temporary jobs (i.e., jobs that arrive and depart at unpredictable time) in two different contexts, namely, machines and network paths. Such problems are known as machine load balancing and virtual circuit routing in the literature. We present new on-line algorithms and improved lower bounds.',\n",
       " '53e997d1b7602d9701fc67f7': 'We study load balancing problems with temporary jobs (i.e., jobs that arrive and depart at unpredictable time) in two different\\n con- texts, namely, machines and network paths. Such problems are known as machine load balancing and virtual circuit routing\\n in the literature. We present new on-line algorithms and improved lower bounds.\\n ',\n",
       " '53e997d1b7602d9701fc681c': \"A well-known construction associates to each rational subset of N-k a rational function in k commuting variables. We extend this construction to rational subsets of Z(k). As a consequence, we derive a multivariate generalization of Popoviciu's theorem and a classical valuation on rational polyhedra. (C) 2002 Elsevier Science B.V. All rights reserved.\",\n",
       " '53e997d1b7602d9701fc6832': 'It has been shown that during arm movement, humans selectively change the endpoint stiffness of their arm to compensate for the instability in an unstable environment. When the direction of the instability is rotated with respect to the direction of movement, it was found that humans modify the antisymmetric component of their endpoint stiffness. The antisymmetric component of stiffness arises due to reflex responses suggesting that the subjects may have tuned their reflex responses as part of the feedforward adaptive control. The goal of this study was to examine whether the CNS modulates the gain of the reflex response for selective tuning of endpoint impedance. Subjects performed reaching movements in three unstable force fields produced by a robotic manipulandum, each field differing only in the rotational component. After subjects had learned to compensate for the field, allowing them to make unperturbed movements to the target, the endpoint stiffness of the arm was estimated in the middle of the movements. At the same time electromyographic activity (EMG) of six arm muscles was recorded. Analysis of the EMG revealed differences across force fields in the reflex gain of these muscles consistent with stiffness changes. This study suggests that the CNS modulates the reflex gain as part of the adaptive feedforward command in which the endpoint impedance is selectively tuned to overcome environmental instability.',\n",
       " '53e997d1b7602d9701fc6837': 'Version management is a key part of software configuration management. A big variety of version models has been realized in both commercial systems and research prototypes. These version models differ with respect to the objects put under version control (files, directories, entities, objects), the organization of versions (version graphs versus multidimensional version spaces), the granularity of versioning (whole software products versus individual components), emphasis on states versus emphasis on changes (state- versus change-based versioning), rules for version selection, etc. We present a uniform version model驴and its support architecture驴for software configuration management. Unlike other unification approaches, such as UML for object-oriented modeling, we do not assemble all the concepts having been introduced in previous systems. Instead, we define a base model that is built on a small number of concepts. Specific version models may be expressed in terms of this base model. Our approach to uniform version management is distinguished by its underlying layered architecture. Unlike the main stream of software configuration management systems, our instrumentable version engine is completely orthogonal to the data model used for representing software objects and their relationships. In addition, we introduce version rules at the bottom of the layered architecture and employ them as a uniform mechanism for expressing different version models. This contrasts to the main stream solution, where a specific version model驴usually version graphs驴is deeply built into the system and version rules are dependent on this model.',\n",
       " '53e997d1b7602d9701fc689f': 'Many concurrent data-structure implementations use the well-known compare-and-swap (CAS) operation, supported in hardware by most modern multiprocessor architectures, for inter-thread synchronization. A key weakness of the CAS operation is the degradation in its performance in the presence of memory contention. In this work we study the following question: can software-based contention management improve the efficiency of hardware-provided CAS operations? Our performance evaluation establishes that lightweight contention management support can greatly improve performance under medium and high contention levels while typically incurring only small overhead when contention is low.',\n",
       " '53e997d1b7602d9701fc688c': \"In this paper, we present a multiagent model in which agents have a perception upon their shared environment, a measure is associated to the agents' perception field. We apply the model on the Vehicle Routing Problem with Time Windows (VRPTW). The overall process adopts the general schema of parallel insertion methods and it uses the contracting of perception's field of the vehicle agents as a new distance between them. This new measure expresses the feasibility universe of the vehicles and is used as a criterion of choice between candidates vehicles for the insertion of a customer in their plan. Our approach provides a new method to tackle the Time constrained VRP in which the solving process is focused on the future and constitutes an alternative for handling the dynamic version of the problem.\",\n",
       " '53e997d1b7602d9701fc693a': \"Hybrid architectures have been used in several recent knowledge representation systems. This paper explores some distinctions between various hybrid representation architectures, focusing in particular on systems built around restricted representation languages This restricted language architecture is illustrated by describing KL-TWO, a hybrid reasoner based on the restricted representation facility RUP. The bulk of this paper discusses KL- TWO, its subcomponents, and the techniques used to interface them. Many recent knowledge representation systems have been based on hybrid architectures. These systems are hybrids in that they do not attempt the entire knowledge representation task with a single inferential component. Instead, they combine several reasoners into a complex whole. The advantages to be gained by this hybrid approach vary from system to system, but often include increases in the system's computational efficiency, the coverage of its representation language, or the ease of expressing knowledge with the system. The topic of this paper is a new logic-based hybrid representation system. This system, called KL-TWO, is one which I have been developing with my colleagues over the past two years. Although KL-TWO has features in common with other hybrid reasoners, it differs very significantly from other systems in the way the common features are used. The purpose of this paper is thus twofold, to describe the features that KL-TWO possesses, and to stress the differences in approach between KL-TWO and other hybrid reasoners.\",\n",
       " '53e997d1b7602d9701fc7193': 'In Organization the data is very important that increase the volume of information that is available on the web and that leads to the design of efficient and accurate web data classification systems. In this paper, we define a framework to improve the performance of a base classifier, by clustering the unlabeled data with labeled data using clustering algorithm (training of samples) labeling of clusters (majority voting for each Hyperspheres) and final generated classified data. We have used construction of BNN based semi-supervised classifier while training and testing of the classifier is performed. We have studied and customized a supervised classification algorithm to form out semi-supervised classification that leads to design a multiclass semi-supervised classifier using geometrical expansion. The experimental result shows provision for the classifier designer followed by training and testing medical disease dataset using pre-decided samples. Our classification model consists of training phase that covers two process clustering and labeling to perform classification task of medical data and the binary neural network is trained. In this we used two techniques normalization and quantization for pre-processing the datasets. Pre-processing impart various outcomes after applying the classification model like number of hypersphere, confusing samples that cannot be learned, training time and label of hypersphere. Comparison has been done for implementation and design of Binary Neural Network Classifier Algorithm with the other existing traditional algorithms. Our classifier evaluates performance in terms of generalization, number of hidden neuron and accuracy etc. The BNN-CA construct three-layered binary neural network (BNN) and can solve any semi-labeled multi-class problem.',\n",
       " '53e997d1b7602d9701fc71ac': 'The problem of assigning a probability to each word of a language is considered. Two methods are discussed. One method assigns a probability to a word on the basis of particular measurable features of the language. The second method is applied to languages L(G) generated by a grammar G. A probability is associated with each production of G. These in turn define the word probabilities of each word in the language. The conditions for this assignment to be a probabilistic measure are derived.',\n",
       " '53e997d1b7602d9701fc7220': 'We present a set of static techniques that reduce runtime overheads in task-parallel programs with implicit synchronization. We use a static dependence analysis to detect non-conflicting tasks and remove unnecessary runtime checks. We further reduce overheads by statically optimizing task creation and management of runtime metadata. We implemented these optimizations in SCOOP, a source-to-source compiler for such a programming model and runtime system. We evaluate SCOOP on 10 representative benchmarks and show that our approach can improve performance by 12% on average.',\n",
       " '53e997d1b7602d9701fc723f': 'Using organizational information processing theory (OIPT), we suggest several factors that influence some of the enterprise resource planning (ERP) costs and benefits that organizations are experiencing. Though we do not attempt to address all important factors that contribute to an ERPs impact, we suggest two organizational characteristics that may have received insufficient attention in other ERP literature: interdependence and differentiation. High interdependence among organizational sub-units, contributes to the positive ERP-related effects because of ERPs ability to coordinate activities and facilitate information flows. However, when differentiation among sub-units is high, organizations may incur ERP-related compromise or design costs. We provide a case study that explores the viability of this framework. The case describes some local-level impacts of ERP and provides some evidence of the validity of the model. Unexpected findings are also presented.',\n",
       " '53e997d1b7602d9701fc728c': \"Most present-day operational document retrieval systems require two distinct types of behavioral response from their users. First, the user's knowledge-perception governing the search is required to be expressed semantically (i.e., as a set of search terms). Second, the user is required to link such terms together syntactically, using the apparatus of Boolean logic. Given the reasonable expectation that some system users will perform better at the first task than the second, it should be useful to provide a system enhancement that allows Boolean syntax to be imposed automatically (and invisibly to the user) on an input set of search terms. Discussion of such an enhancement is offered, along with an outline specification for it, for users of MEDLINE, based on experimental results.\",\n",
       " '53e997d1b7602d9701fc7267': 'In this paper, we present a framework, called OrbGOP, to support the architecting and high-level programming of CORBA-based distributed applications. OrbGOP makes two contributions to the development of CORBA applications: (1) it provides higher-level abstractions for programming CORBA applications and frees the programmer from the underlying irrelevant details; (2) it facilitates the architecture description and dynamic reconfiguration of CORBA applications. OrbGOP is based on the graph-oriented programming (GOP) model, where the configuration of a distributed program is described as a logical graph separated from the programming of the constituent components of the program. Component interactions and dynamic reconfiguration are implemented by executing a set of operations that are defined over the graph. OrbGOP extends the application of GOP to the CORBA environment and provides more powerful support for distributed software architecture. Through a sample example, we show that OrbGOP provides a reflective, architectural approach to high-level programming support for the development of CORBA-based distributed applications. The system architecture, the design of runtime support and functional library support, as well as the preliminary evaluation of a working prototype of OrbGOP are also presented.',\n",
       " '53e997d1b7602d9701fc6d7d': 'Large, slow moving landslides in the Berkeley Hills cause many damage and pose a potential threat to public safety due to the close proximity of the Hayward Fault. We have been using Differential SAR interferometry (DInSAR) and time-series analysis of SAR data to resolve the rates of the landslide motion. In this paper, we aim to interpret the new satellite TerraSAR-X data in this small landslides area. We have acquired both Stripmap data and Spotlight data. Standard InSAR method as well as Persistent Scatterer InSAR is utilized in the processing.',\n",
       " '53e997d1b7602d9701fc72f7': 'Originating from a viewpoint that complex/chaotic dynamics would play an important role in biological system including brains, chaotic dynamics introduced in a recurrent neural network was applied to control. The results of computer experiment was successfully implemented into a novel autonomous roving robot, which can only catch rough target information with uncertainty by a few sensors. It was employed to solve practical two-dimensional mazes using adaptive neural dynamics generated by the recurrent neural network in which four prototype simple motions are embedded. Adaptive switching of a system parameter in the neural network results in stationary motion or chaotic motion depending on dynamical situations. The results of hardware implementation and practical experiment using it show that, in given two-dimensional mazes, the robot can successfully avoid obstacles and reach the target. Therefore, we believe that chaotic dynamics has novel potential capability in controlling, and could be utilized to practical engineering application.',\n",
       " '53e997d1b7602d9701fc7307': 'Digital pulse position modulation (PPM) is a preferred modulation format for the ideal photon counting channel and optical intersatellite links. Here we examine its potential for the coherent optical fibre communications channel. We present a thorough performance and optimisation analysis. Comparisons, at a wavelength of 1.5-mu-m, are made with shot-noise limited coherent PCM (homodyne and heterodyne ASK, FSK and PSK) over a range of fibre bandwidths and varying PPM word size. We conclude that for moderate to high fibre bandwidths homodyne digital PPM should achieve an improvement in sensitivity of typically 5 dB over homodyne PSK PCM.',\n",
       " '53e997d1b7602d9701fc70d9': 'We present advances in the software engineering design and implementation of the multi-tier run-time system for the General Intensional Programming System (GIPSY) by further unifying the distributed technologies used to implement the Demand Migration Framework (DMF) in order to streamline distributed execution of hybrid intensional-imperative programs using Java.',\n",
       " '53e997d1b7602d9701fc70ec': 'Many videotexts exist in TV programs. Some videotexts provide valuable information. Thus, an efficient design to extract these videotexts is requested. Existing videotext extractors work on the PC platform and they are difficult to achieve real-time extraction and integration. Therefore, this work designs a videotext extractor on a dual-core platform. A distributed design framework for a dual-core platform is proposed. The extraction task is dispatched to the ARM and the DSP. The ARM core executes capture, display, control, and extraction threads. The DSP core performs algorithms. The ARM and the DSP communicate by buffers and solid channels. On the DSP side, some techniques are manipulated to optimize the videotext extractor. They include software pipeline, internal memory, adjusted program, assembly optimization, and DMA. To achieve high performance, two transferred schemes of DMA are proposed. This system is implemented on the TI Davinci DM6446 platform. All input videos are 720 x 480 with 30 fps captured from real-time DVB-T system. The simulation result shows that this extractor can process the large-size frames, and all the videotext can be extracted. With this novel architecture, the extraction speed can be enhanced to 23 frames per second.',\n",
       " '53e997d1b7602d9701fc70ee': 'This paper presents an iterative receiver for multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) systems over time-variant wireless channels. The receiver performs joint decoding, channel estimation, and multiuser detection, with soft information iteratively provided by the single-user decoders. Time-variance is effectively taken into account exploiting the properties of the discrete prolate spheroidal (DPS) sequences, being the bandlimited sequences with maximum energy concentration in time. Turbo codes are used for each transmit antenna, thus the receiver presents an iterative structure also in the single-user case. Simulation results for the performance are presented in terms of bit error rate (BER) and normalized mean square error (NMSE) vs signal-to-noise ratio (SNR). The effects of the number of external and internal iterations as well as the number of pilots on the performance of the system are investigated.',\n",
       " '53e997d1b7602d9701fc7116': 'We present an algorithm for the layout of undirected compound graphs, relaxing restrictions of previously known algorithms in regards to topology and geometry. The algorithm is based on the traditional force-directed layout scheme with extensions to handle multi-level nesting, edges between nodes of arbitrary nesting levels, varying node sizes, and other possible application-specific constraints. Experimental results show that the execution time and quality of the produced drawings with respect to commonly accepted layout criteria are quite satisfactory. The algorithm has also been successfully implemented as part of a pathway integration and analysis toolkit named PATIKA, for drawing complicated biological pathways with compartmental constraints and arbitrary nesting relations to represent molecular complexes and various types of pathway abstractions.',\n",
       " '53e997d1b7602d9701fc73f1': 'We present the results from a Comparative Molecular Field Analysis (CoMFA) and docking study of a diverse set of 36 estrogen receptor ligands whose relative binding affinities (RBA) with respect to 17beta-Estradiol were available in both isoforms of the nuclear estrogen receptors (ER alpha, ER beta). Initial CoMFA models exhibited a correlation between the experimental relative binding affinities and the molecular steric and electrostatic fields; ER alpha: r2 = 0.79, q2 = 0.44 ER beta: r2 = 0.93, q2 = 0.63. Addition of the solvation energy of the isolated ligand improved the predictive nature of the ER beta model initially; r2 = 0.96, q2 = 0.70 but upon rescrambling of the data-set and reselecting the training set at random, inclusion of the ligand solvation energy was found to have little effect on the predictive nature of the CoMFA models. The ligands were then docked inside the ligand binding domain (LBD) of both ER alpha and ER beta utilizing the docking program Gold, after-which the program CScore was used to rank the resulting poses. Inclusion of both the Gold and CScore scoring parameters failed to improve the predictive ability of the original CoMFA models. The subtype selectivity expressed as RBA(ER alpha/ER beta) of the test sets was predicted using the most predictive CoMFA models, as illustrated by the cross-validated r2. In each case the most selective ligands were ranked correctly illustrating the utility of this method as a prescreening tool in the development of novel estrogen receptor subtype selective ligands.',\n",
       " '53e997d1b7602d9701fc73fd': 'Di- and tri-phosphate nucleotides are essential cofactors for many proteins, usually in an Mg(2+) -bound form. Proteins like GTPases often detect the difference between NDP and NTP and respond by changing conformations. To study such complexes, simple, fixed charge force fields have been used, which allow long simulations and precise free energy calculations. The preference for NTP or NDP binding depends on many factors, including ligand structure and Mg(2+) coordination and the changes they undergo upon binding. Here, we use a simple force field to examine two Mg(2+) coordination modes for the unbound GDP and GTP: direct, or \"Inner Sphere\" (IS) coordination by one or more phosphate oxygens and indirect, \"Outer Sphere\" (OS) coordination involving one or more bridging waters. We compare GTP: and GDP:Mg binding with OS and IS coordination; combining the results with experimental data then indicates that GTP prefers the latter. We also examine different kinds of IS coordination and their sensitivity to a key force field parameter: the optimal Mg:oxygen van der Waals distance Rmin . Increasing Rmin improves the Mg:oxygen distances, the GTP: and GDP:Mg binding affinities, and the fraction of GTP:Mg with β + γ phosphate coordination, but does not improve or change the GTP/GDP affinity difference, which remains much larger than experiment. It has no effect on the free energy of GDP binding to a GTPase.',\n",
       " '53e997d1b7602d9701fc7173': 'In this paper, we consider the single machine weighted tardiness scheduling problem with sequence-dependent setups. We present heuristic algorithms based on the beam search technique. These algorithms include classic beam search procedures, as well as the filtered and recovering variants. Previous beam search implementations use fixed beam and filter widths. We consider the usual fixed width algorithms, and develop new versions that use variable beam and filter widths. The computational results show that the beam search versions with a variable width are marginally superior to their fixed value counterparts, even when a lower average number of beam and filter nodes is used. The best results are given by the recovering beam search algorithms. For large problems, however, these procedures require excessive computation times. The priority beam search algorithms are much faster, and can therefore be used for the largest instances. Scope and purpose: We consider the single machine weighted tardiness scheduling problem with sequence-dependent setups. In the current competitive environment, it is important that companies meet the shipping dates, as failure to do so can result in a significant loss of goodwill. The weighted tardiness criterion is a standard way of measuring compliance with the due dates. Also, the importance of sequence-dependent setups in practical applications has been established in several studies. In this paper, we present several heuristics based on the beam search technique. In previous beam search implementations, fixed beam and filter widths have been used. We consider the usual fixed width algorithms, and also develop new versions with variable beam and filter widths. The computational tests show that the beam search versions with a variable width are marginally superior to their fixed value counterparts. The recovering beam search procedures are the heuristic of choice for small and medium size instances, but require excessive computation times for large problems. The priority beam search algorithm is the fastest of the beam search heuristics, and can be used for the largest instances.',\n",
       " '53e997d1b7602d9701fc7181': 'Systems that support the co-authoring of web sites often allow users to freely edit pages. This can result in semantic inconsistencies within and between pages. We propose a change awareness mechanism that monitors intra- and inter-document edits, taking into account changes made to a page and pages connected to it through htmlor transclusion links. The effect of all the changes is computed based on various metrics and on different semantic levels according to user preferences. A visualisation tool indicates how much a document and documents linked to it have changed. An edit profile allows users to easily spot parts with \"interesting\" changes within web pages.',\n",
       " '53e997d1b7602d9701fc740f': 'Supporting large-scale scientific workflows in distributed network environments and optimizing their performances are crucial to the success of collaborative scientific discovery. We develop a generic scientific workflow platform, referred to as SciFlow, which constitutes a flexible framework to facilitate the distributed execution and management of scientific workflows and incorporates a class of workflow mapping schemes to achieve optimal end-to-end performances. The functionalities of SciFlow are provided and its interactions with other tools or systems are enabled through web services for easy access over standard Internet protocols while being independent of different platforms and programming languages. The performance superiority of SciFlow over existing workflow mapping schemes and management systems is illustrated by extensive simulations and is further verified by large-scale experiments on real-life scientific workflows through effective system implementation and deployment in distributed network environments.',\n",
       " '53e997d1b7602d9701fc7714': 'The development of optimized motions of humanoid robots that guarantee fast and also stable walking is an important task, especially in the context of autonomous soccer-playing robots in RoboCup. We present a walking motion optimization approach for the humanoid robot prototype HR18 which is equipped with a low-dimensional parameterized walking trajectory generator, joint motor controller and an internal stabilization. The robot is included as hardware-in-the-loop to define a low-dimensional black-box optimization problem. In contrast to previously performed walking optimization approaches, we apply a sequential surrogate optimization approach using stochastic approximation of the underlying objective function and sequential quadratic programming to search for a fast and stable walking motion. This is done under the conditions that only a small number of physical walking experiments should have to be carried out during the online optimization process. For the identified walking motion for the considered 55 cm tall humanoid robot, we measured a forward walking speed of more than 30 cm s-1 . With a modified version of the robot, even more than 40 cm s-1 could be achieved in permanent operation.',\n",
       " '53e997d1b7602d9701fc7743': 'Implementation of overlapped block filtering using Schedul- ing by Edge Reversal (SER) is proposed in this paper. S- ER is a very simple and powerful synchronizer. It allows more efficient implementation of parallel structures. This technique is applied for the first time to FIR filters using the overlapped block digital filtering, and implemented on a parallel computer plataform. The results confirm the ex- pected reduction in computation time.',\n",
       " '53e997d1b7602d9701fc7812': 'Abstract This paper presents algorithms developed for pixel merging phase of object space parallel polygon rendering on hypercube connected multicomputers These algorithms reduce volume of communication in pixel merging phase by only exchanging local foremost pixels In order to avoid message fragmentation local foremost pixels should be stored in consecutive memory locations An algorithm called modi ed scanline z bu er is proposed to store local foremost pixels e ciently This algo rithm also avoids the initialization of scanline z bu er for each scanline on the screen Good processor utilization is achieved by subdividing the image space among the processors in pixel merging phase E cient algo rithms for load balancing in the pixel merging phase are also proposed and presented Experimental results obtained on a processor Intel s iPSC hypercube multicomputer are presented ',\n",
       " '53e997d1b7602d9701fc781c': 'In the conventional JPEG algorithm, an image is divided into eight by eight blocks and then the 2-D DCT is applied to encode each block. In this paper, we find that, in addition to rectangular blocks, the 2-D DCT is also orthogonal in the trapezoid and triangular blocks. Therefore, instead of eight by eight blocks, we can generalize the JPEG algorithm and divide an image into trapezoid and triangular blocks according to the shapes of objects and achieve higher compression ratio. Compared with the existing shape adaptive compression algorithms, as we do not try to match the shape of each object exactly, the number of bytes used for encoding the edges can be less and the error caused from the high frequency component at the boundary can be avoided. The simulations show that, when the bit rate is fixed, our proposed algorithm can achieve higher PSNR than the JPEG algorithm and other shape adaptive algorithms. Furthermore, in addition to the 2-D DCT, we can also use our proposed method to generate the 2-D complete and orthogonal sine basis, Hartley basis, Walsh basis, and discrete polynomial basis in a trapezoid or a triangular block.',\n",
       " '53e997d1b7602d9701fc78da': 'Compound nouns such as example noun compound are becoming more common in\\nnatural language and pose a number of difficult problems for NLP systems,\\nnotably increasing the complexity of parsing. In this paper we develop a\\nprobabilistic model for syntactically analysing such compounds. The model\\npredicts compound noun structures based on knowledge of affinities between\\nnouns, which can be acquired from a corpus. Problems inherent in this\\ncorpus-based approach are addressed: data sparseness is overcome by the use of\\nsemantically motivated word classes and sense ambiguity is explicitly handled\\nin the model. An implementation based on this model is described in Lauer\\n(1994) and correctly parses 77% of the test set.',\n",
       " '53e997d1b7602d9701fc7834': 'There are two objectives of this chapter. One objective is to examine the basic principles and issues of granular computing. We focus on the tasks of granulation and computing with granules. From semantic and algorithmic perspectives, we study the construction, interpretation, and representation of granules, as well as principles and operations of computing and reasoning with granules. The other objective is to study a partition model of granular computing in a set-theoretic setting. The model is based on the assumption that a finite set of universe is granulated through a family of pairwise disjoint subsets. A hierarchy of granulations is modeled by the notion of the partition lattice. The model is developed by combining, reformulating, and reinterpreting notions and results from several related fields, including theories of granularity, abstraction and generalization (artificial intelligence), partition models of databases, coarsening and refining operations (evidential theory), set approximations (rough set theory), and the quotient space theory for problem solving.',\n",
       " '53e997d1b7602d9701fc78fa': 'Process capability indices CPU and CPL are developed in the manufacturing industry to provide quantitative measures on process potential and performance, for processes with one-sided specification limits. Statistical properties of the estimators of CPU and CPL have been investigated extensively, but only restricted to cases with single samples. In this paper, we consider the estimation and capability testing of CPU and CPL based on multiple samples. We show that the proposed estimator of CPU and CPL are indeed the uniformly minimum variance unbiased estimators (UMVUEs). A simple procedure based on a hypothesis testing using the UMVUE is developed for the practitioners to use for judging whether a stable process meets the preset capability requirement.',\n",
       " '53e997d1b7602d9701fc7841': 'In this paper, we present a new model for the assessment of Geographic Relevance. This model is drawn from Okapi BM25, thus it takes into account not only a score for each dimension of relevance but also the distribution of these scores within the collection. Preliminary results suggest that the relevance estimation of top-ranked objects is more sensitive to small changes in the user context.',\n",
       " '53e997d1b7602d9701fc7844': 'Describing the interactional behavior of rational agents and seeking equilibria are two main domains of game theory. The epistemic\\n foundation of the above two domains is based on the assumption that all players are rational. However, game theory itself\\n cannot precisely model the higher-order information changes of mutual knowledge among players, so in the current studies of\\n game theory the interpretations of rationality are vague. In this paper, a concept of the rationality is redefined through\\n incorporating an epistemic ingredient. Then a method is proposed to solve and refine Nash equilibria which is grounded on\\n public announcement logic, and it is proved that the iterated announcement of this rationality assertion characterizes the\\n iterated admissibility algorithm in game theory, which offers a dynamic epistemic foundation for this algorithm. Finally,\\n an implementation of this method, based on the extended DEMO, is shown to be correct.',\n",
       " '53e997d1b7602d9701fc788d': \"In order to examine the effect of an avatar in natural interaction with elderly users in ambient intelligent environments, we performed an empirical study with elderly people (normal aging, mild cognitive impairment and Alzheimer's patients) not only on subjective but also on objective measures. The data supports the following: 1) The subjects followed some instructions much better when interacting with the avatar. 2) The presence of the avatar has neither any positive nor negative effect on the recall of elderly people and it has a positive effect only on the subjective measures. 3) We found that elderly people both with and without cognitive impairment are capable of recognizing emotions in the facial expressions of the avatar and 4) they found the experience of having an emotional avatar in the interface a pleasant one. Thus, we conclude that virtual characters could improve the interaction between elderly people and machines, but this would depend greatly on the request task.\",\n",
       " '53e997d1b7602d9701fc78fc': 'Actin is the monomeric subunit of actin filaments which form one of the three major cytoskeletal networks in eukaryotic cells. Actin dynamics, be it the polymerisation of actin monomers into filaments or the reverse process, plays a key role in many cellular activities such as cell motility and phagocytosis. There is a growing number of experimental, theoretical and mathematical studies on the components of actin polymerisation and depolymerisation. However, it remains a challenge to develop compositional models of actin dynamics, e.g., by using differential equations. In this paper, we propose compositional process algebra models of actin polymerisation, and present a geometric representation of these models that allows to generate movies reflecting their dynamics.',\n",
       " '53e997d1b7602d9701fc790c': 'Pattern matching is an important operation in functional programs. So far, pattern matching has been investigated in the context of structured terms. This paper presents an approach to extend pattern matching to terms without (much of a) structure such as binaries which is the kind of data format that network applications typically manipulate. After introducing a notation for matching binary data against patterns, we present an algorithm that constructs a tree automaton from a set of binary patterns. We then show how the pattern matching can be made adaptive, how redundant tests can be avoided, and how we can further reduce the size of the resulting automaton by taking interferences between patterns into account. The effectiveness of our techniques is evaluated using implementations of network protocols taken from actual telecom applications.',\n",
       " '53e997d1b7602d9701fc7d0c': 'Plan-recognition requires the construction of possible plans which could explain a set of observed actions, and then selecting one or more of them as providing the belt explanation. In this paper we present a formal model of the latter process based upon probability theory. Our model consists of a knowledge-base of facts about the world expressed in a first-order language, and rules for using that knowledge-base to construct a Bayesian network. The network is then evaluated to find the plans with the highest probability.',\n",
       " '53e997d1b7602d9701fc7d39': 'The difficulty in processing long documents is due to the variety of topics they contain. In this paper we study the use of probabilistic passage analysis in text categorization, assigning predefined topics to long documents. Unlike conventional text categorization that assigns topics to a whole document, passage categorization assigns topics to each passage in a document. The advantage of passage categorization is verified through experiments on the Ziff data set. We also discuss possible applications of passage categorization such as text summarization, text tiling, and passage clustering.',\n",
       " '53e997d1b7602d9701fc8025': 'A probabilistic model for computer-based generation of a machine translation system on the basis of English-Russian parallel text corpora is suggested. The model is trained using parallel text corpora with pre- aligned source and target sentences. The training of the model results in a bilingual dictionary of words and \"word blocks\" with relevant translation probability The corpus-based statistical MT gains more popularity nowadays due to vastly increased capacity of modern computers. The works of P. Brown and collabo- rators (1, 2), may be regarded as a typical recent example. This paper suggests another approach to statistical MT different from that of Brown et al. The suggested model is trained on pre-aligned bilingual text corpora and the following approach to \\'tailor making\\' a computer dictionary and an MT system is taken. The translation of a source word combination by a target one is determined by the correlation with the neighboring word combinations both in the source and the target texts rather than only by the translation probabilities of the combinations themselves. The word order of the source and target sentences seldom coincide, however, the raw translation with the incorrect order of words may often be understood by a specialist. The translation quality will radically improve if instead of individual words one takes internally agreed word combinations with fixed order (blocks). In this model statistically stable source blocks are related to the most prob- able target ones using specially introduced function, called \"adhesion function\" since it is believed that this function indirectly reflects the grammatical and se- mantic \"adhesion\" of the words in a text. We believe that blocks with negative correlation having been excluded the remaining internally agreed blocks in a way will become a substitute of the proper word order in the target sentence. �Institute for Theoretical Physics, National Academy of Sciences of Ukraine',\n",
       " '53e997d1b7602d9701fc802b': \"Menus are a primary control in current interfaces, but there has been relatively little theoretical work to model their performance. We propose a model of menu performance that goes beyond previous work by incorporating components for Fitts' Law pointing time, visual search time when novice, Hick-Hyman Law decision time when expert, and for the transition from novice to expert behaviour. The model is able to predict performance for many different menu designs, including adaptive split menus, items with different frequencies and sizes, and multi-level menus. We tested the model by comparing predictions for four menu designs (traditional menus, recency and frequency based split menus, and an adaptive 'morphing' design) with empirical measures. The empirical data matched the predictions extremely well, suggesting that the model can be used to explore a wide range of menu possibilities before implementation. Author Keywords Menus, Hick-Hyman Law, Fitts' Law, performance modelling, adaptive behaviour.\",\n",
       " '53e997d1b7602d9701fc802c': 'Multithreaded programs express a complex non-linear dependency between their configuration and the performance. To better understand this dependency performance prediction models are used. However, building performance models manually is time-consuming and error-prone. We present a novel methodology for automatically building performance models of industrial multithreaded programs.',\n",
       " '53e997d1b7602d9701fc802d': 'This study presents a probabilistic model of melody perception, which infers the key of a melody and also judges the probability of the melody itself. The model uses Bayesian reasoning: For any \"surface\" pattern and underlying \"structure,\" we can infer the structure maximizing P(structure I surface) based on knowledge of P(surface, structure). The probability of the surface can then be calculated as Sigma P(surface, structure), summed over all structures. In this case, the surface is a pattern of notes; the structure is a key. A generative model is proposed, based on three principles: (a) melodies tend to remain within a narrow pitch range; (b) note-to-note intervals within a melody tend to be small; and (c) notes tend to conform to a distribution (or key profile) that depends on the key. The model is tested in three ways. First, it is tested on its ability to identify the keys of a set of folksong melodies. Second, it is tested on a melodic expectation task in which it must judge the probability of different notes occurring given a prior context; these judgments are compared with perception data from a melodic expectation experiment. Finally, the model is tested on its ability to detect incorrect notes in melodies by assigning them lower probabilities than the original versions.',\n",
       " '53e997d1b7602d9701fc8032': 'Humans can generate accurate and appropriate motor commands in various, and even uncertain, environments. MOSAIC (MOdular\\n Selection And Identification for Control) was originally proposed to describe this human ability, but this model is hard to\\n analyze mathematically because of its emphasis on biological plausibility. In this article, we present an alternative and\\n probabilistic model of MOSAIC (p-MOSAIC) as a mixture of normal distributions and an online EM-based learning method for its\\n predictors and controllers. A theoretical consideration shows that the learning rule of p-MOSAIC corresponds to that of MOSAIC\\n except for some points which are mostly related to the learning of controllers. The results of experiments using synthetic\\n datasets demonstrate some practical advantages of p-MOSAIC. One is that the learning rule of p-MOSAIC stabilizes the estimation\\n of “responsibility.” Another is that p-MOSAIC realizes more accurate control and robust parameter learning in comparison to\\n the original MOSAIC, especially in noisy environments, due to the direct incorporation of the noises into the model.',\n",
       " '53e997d1b7602d9701fc7f6d': \"A new conjugate gradient method is proposed by applying Powell's symmetrical technique to conjugate gradient methods in this paper. Using Wolfe line searches, the global convergence of the method is analyzed by using the spectral analysis of the conjugate gradient iteration matrix and Zoutendijk's condition. Based on this, some concrete descent algorithms are developed. 200s numerical experiments are presented to verify their performance and the numerical results show that these algorithms are competitive compared with the PRP+ algorithm. Finally, a brief discussion of the new proposed method is given.\",\n",
       " '53e997d1b7602d9701fc83d3': 'Reasoning systems have reached a high degree of maturity in the last decade. However, even the most successful systems are usually not general purpose problem solvers but are typically specialised on problems in a certain domain. The MathWeb Software Bus (MathWeb-SB) is a system for combining reasoning specialists via a common software bus. We describe the integration of the 驴Clam system, a reasoning specialist for proofs by induction, into the MathWeb-SB. Due to this integration, 驴Clam now offers its theorem proving expertise to other systems in the MathWeb-SB. On the other hand, 驴Clam can use the services of any reasoning specialist already integrated. We focus on the latter and describe first experiments on proving theorems by induction using the computational power of the Maple system within 驴Clam.',\n",
       " '53e997d1b7602d9701fc8239': 'The wireless sensor network (WSN) node is required to operate for several months with the limited system resource such as memory and power. A general WSN node operates in the active state during less than 1% of the several-month lifetime and waits an event in the inactive state during 99% of the same lifetime. This paper suggests a power adjust dual priority scheduler (PA-DPS) for low-power, which has a structure to meet the requirements for the WSN by estimating power consumption in the WSN node. PA-DPS has been designed based on an event-driven approach and is based on the dual-priority scheduling structure, which has been conventionally suggested in the real-time system field. From experimental results, PA-DPS reduced the inactive mode current up to 40% under the 1% duty cycle.',\n",
       " '53e997d1b7602d9701fc865a': 'The main objective of this work was to study the functional markers of the clinical response to cholecystokinin tetrapeptide (CCK-4). Twelve healthy male subjects were challenged with CCK-4 and simultaneously underwent functional magnetic resonance imaging (fMRI) recording. Since anticipatory anxiety (AA) is an intrinsic part of panic disorder, a behavioral paradigm, using the threat of being administered a second injection of CCK-4, has been developed to investigate induced AA. The study was composed of three fMRI scans according to an open design. During first and second scan, subjects were injected with placebo and CCK-4, respectively. The third scan was the AA challenge. CCK-4 administration induced physiological and psychological symptoms of anxiety that met the criteria for a panic attack in 8 subjects, as well as cerebral activation in anxiety-related brain regions. Clinical and physiological response intensity was consistent with cerebral activity extent and robustness. fMRI proved more sensitive than clinical assessment in evidencing the effects of the AA challenge. The latter induced brain activation, different from that obtained on CCK-4 and during placebo injection, that was likely related to anxiety. The method applied in this study is suitable for the study of anxiety using fMRI.',\n",
       " '53e997d1b7602d9701fc8d96': 'The development of patch test consistent quasi-continuum energies for multidimensional crystalline solids modeled by many-body potentials remains a challenge. The original quasi-continuum energy (QCE) [R. Miller and E. Tadmor, Model. Simul. Mater. Sci. Eng., 17 (2009), 053001] has been implemented for many-body potentials in two and three space dimensions, but it is not patch test consistent. We propose that by blending the atomistic and corresponding Cauchy-Born continuum models of QCE in an interfacial region with thickness of a small number $k$ of blended atoms, a general blended quasi-continuum energy (BQCE) can be developed with the potential to significantly improve the accuracy of QCE near lattice instabilities such as dislocation formation and motion. In this paper, we give an error analysis of the blended quasi-continuum energy (BQCE) for a periodic one-dimensional chain of atoms with next-nearest neighbor interactions. Our analysis includes the optimization of the blending function for an improved convergence rate. We show that the $\\\\ell^2$ strain error for the nonblended QCE energy, which has low order $O(\\\\varepsilon^{1/2})$, where $\\\\varepsilon$ is the atomistic length scale [M. Dobson and M. Luskin, SIAM J. Numer. Anal., 47 (2009), pp. 2455-2475, P. Ming and J. Z. Yang, Multiscale Model. Simul., 7 (2009), pp. 1838-1875], can be reduced by a factor of $k^{3/2}$ for an optimized blending function where $k$ is the number of atoms in the blending region. The QCE energy has been further shown to suffer from a $O(1)$ error in the critical strain at which the lattice loses stability [M. Dobson, M. Luskin, and C. Ortner, J. Mech. Phys. Solids, 58 (2010), pp. 1741-1757]. We prove that the error in the critical strain of BQCE can be reduced by a factor of $k^2$ for an optimized blending function, thus demonstrating that the BQCE energy for an optimized blending function has the potential to give an accurate approximation of the deformation near lattice instabilities such as crack growth.',\n",
       " '53e997d1b7602d9701fc8e4d': 'The current study tracked 80 participants who spent an average of six hours per week in Second Life over six consecutive weeks. Objective measures of movement and chat were automatically collected in real time when participants logged in to Second Life. Data regarding the number of groups and friends was self-reported through online questionnaires on a weekly basis. Results demonstrated that although the social networks of users continued to broaden over the course of the study, users became less inclined to explore regions, decreased their use of high-energy actions such as flying or running, and chatted less. We discuss implications for theories of virtual social interaction as well as the use of Second Life as a social science research platform.',\n",
       " '53e997d1b7602d9701fc90aa': 'Even though a very large number of solution methods has been developed for the job-shop scheduling problem, a majority has been designed for the makespan criterion. In this paper, we propose a general approach for optimizing any regular criterion in the job-shop scheduling problem. The approach is a local search method that uses a disjunctive graph model and neighborhoods generated by swapping critical arcs. The connectivity property of the neighborhood structure is proved and a novel efficient method for evaluating moves is presented. Besides its generality, another prominent advantage of the proposed approach is its simple implementation that only requires to tune the range of one parameter. Extensive computational experiments carried out on various criteria (makespan, total weighted flow time, total weighted tardiness, weighted sum of tardy jobs, maximum tardiness) show the efficiency of the proposed approach. Best results were obtained for some problem instances taken from the literature.',\n",
       " '53e997d1b7602d9701fc9102': 'When service-based applications implement business processes, it is important to monitor their performance in terms of Key Performance Indicators (KPIs). If monitoring results show that the KPIs do not reach target values, the influential factors have to be analyzed and corresponding adaptation actions have to be taken. In this paper we present a novel adaptation approach for servicebased applications (SBAs) based on a process quality factor analysis. This approach uses decision trees for showing the dependencies of KPIs on process quality factors from different functional levels of an SBA. We extend the monitoring and analysis approach and show how the analysis results may be used to come up with an adaptation strategy leading to an SBA that satisfies KPI values.',\n",
       " '53e997d1b7602d9701fc9105': 'We present an adaptive algorithm aimed at detecting multiple point-like radar targets embedded in correlated Gaussian noise. The proposed detector modifies and improves the adaptive beamformer orthogonal rejection test (ABORT) idea to address detection of multiple targets. More precisely, it relies on the so-called two-step generalized likelihood ratio test (GLRT) design procedure implemented without assignment of a distinct set of secondary data. The newly proposed detector can guarantee the constant false alarm rate (CFAR) property and the performance assessment, conducted resorting to simulated data, has shown that it exhibits better rejection capabilities of mismatched signals than previously proposed detectors, at the price of an acceptable performance loss for matched signals',\n",
       " '53e997d1b7602d9701fc8fc4': 'Cloud optical thickness (COT) has been retrieved using multiple optical instruments onboard ENVISAT and compared for consistency for a single cloud field over central Europe. To match the spatial resolution of the Scanning Imaging Absorption Spectrometer for Atmospheric Chartography (SCIAMACHY), the results of retrievals from higher resolution instruments have been averaged on the scale of 30 time...',\n",
       " '53e997d6b7602d9701fc958b': 'In this paper, we propose a new parallel sparse iterative method (PPSIA) for computing the stationary distribution of large-scale Markov chains. The PPSIA method is based on Markov chain state isolation and aggregation techniques. The parallel method conserves as much as possible the benefits of aggregation, and Gauss---Seidel effects contained in the sequential algorithm (SIA) using a pipelined technique. Both SIA and PPSIA exploit sparse matrix representation in order to solve large-scale Markov chains. Some Markov chains have been tested to compare the performance of SIA, PPSIA algorithms with other techniques such as the power method, and the generalized minimal residual GMRES method. In all the tested models, PPSIA outperforms the other methods and shows a super-linear speed-up.',\n",
       " '53e997d1b7602d9701fc90f8': 'A peak-to-average power ratio (PAPR) reduction based on adaptive all-pass filters (AAPFs) for multiple-input single-output orthogonal frequency division multiplexing systems with space-frequency block coding is proposed. The AAPF creates phase rotation to reduce the PAPR. With the scattered pilot pattern, the AAPF enables data recovery without side information (SI). The PAPR reduction performance and the BER performance with considered system parameters are comparable to those of the blind SLM (B-SLM) scheme and the low complexity SLM (LC-SLM) scheme with SI. The computational complexity is much lower than that of the B-SLM scheme and comparable to that of the LC-SLM scheme.',\n",
       " '53e997d1b7602d9701fc9173': 'In this paper, we present an improved method of anchor mod- els for speaker verification. Anchor model is the method that represent a speaker by his relativity of a set of other speakers, called anchor speakers. It was firstly introduced for speaker indexing in large audio database. We suggest a rank based metric for the measurement of speaker character vectors in anchor model. Different from conventional metric methods which consider each anchor speaker equally and compare the log likelihood scores directly, in our method the relative order of anchor speakers is exploited to characterize target speaker. We have taken experiments on the YOHO database. The re- sults show that EER of our method is 13.29% lower than that of conventional metric. Also, our method is more robust against the mismatching between test set and anchor set.',\n",
       " '53e997d6b7602d9701fc9620': 'A restricted Boltzmann machine consists of a layer of visible units and a layer of hidden units with no visible-visible or hidden-hidden connections. The restricted Boltzmann machine is the main component used in building up the deep belief network and has been studied by many researchers. However, the learning algorithm for the restricted Boltzmann machine is a NP-hard problem in general. In this paper we propose a new approximate learning algorithm for the restricted Boltzmann machines using the EM algorithm and the loopy belief propagation.',\n",
       " '53e997d6b7602d9701fc961e': 'With the tremendous increase in the use of mobile handheld and wearable devices, the pervasive computing arena is becoming stronger and powerful day by day. Despite having various physical constraints, many of the resourcerich functionalities enjoyed by other devices have been incorporated in these tiny devices. This is why different fields of research have developed in this area. There are still some unexplored but crucial features like Knowledge Usability, Resource Discovery, and Self-healing that deserve further attention. Besides exploration of these areas, implementation and evaluation of these features need to be considered from a security and privacy perspective. In this paper, we illustrate the design and implementation of a middleware MARKS, which incorporates these less-explored areas of pervasive computing, to guarantee optimum utilization of the physical capabilities and also to ensure security and privacy.',\n",
       " '53e997d1b7602d9701fc8fc8': 'This paper presents a reinforcement learning algorithm designed for solving optimal control problems for which the state space and the time are continuous variables. Like Dynamic Programming methods, reinforcement learning techniques generate an optimal feed-back policy by the mean of the value function which estimates the best expectation of cumulative reward as a function of initial state. The algorithm proposed here uses finite-elements methods for approximating this function. It is composed of two dynamics: the learning dynamics, called Finite-Element Reinforcement Learning, which estimates the values at the vertices of a triangulation defined upon the state space, and the structural dynamics, which refines the triangulation inside regions where the value function is irregular. This mesh refinement algorithm intends to solve the problem of the combinatorial explosion of the number of values to be estimated. A formalism for reinforcement learning in the continuous case is proposed, the Hamilton-Jacobi-Bellman equation is stated, then the algorithm is presented and applied to a simple two-dimensional target problem.',\n",
       " '53e997d6b7602d9701fc9639': \"Many real-time operating systems (RTOSes) offer very small interrupt latencies, in the order of tens or hundreds of cycles. They achieve this by making the RTOS kernel fully preemptible, permitting interrupts at almost any point in execution except for some small critical sections. One drawback of this approach is that it is difficult to reason about or formally model the kernel's behavior for verification, especially when written in a low-level language such as C. An alternate model for an RTOS kernel is to permit interrupts at specific preemption points only. This controls the possible interleavings and enables the use of techniques such as formal verification or model checking. Although this model cannot (yet) obtain the small interrupt latencies achievable with a fully-preemptible kernel, it can still achieve worst-case latencies in the range of 10,000s to 100,000s of cycles. As modern embedded CPUs enter the 1 GHz range, such latencies become acceptable for more applications, particularly when they come with the additional benefit of simplicity and formal models. This is particularly attractive for protected multitasking microkernels, where the (inherently non-preemptible) kernel entry and exit costs dominate the latencies of many system calls. This paper explores how to reduce the worst-case interrupt latency in a (mostly) non-preemptible protected kernel, and still maintain the ability to apply formal methods for analysis. We use the formally-verified seL4 microkernel as a case study and demonstrate that it is possible to achieve reasonable response-time guarantees. By combining short predictable interrupt latencies with formal verification, a design such as seL4's creates a compelling platform for building mixed-criticality real-time systems.\",\n",
       " '53e997d1b7602d9701fc9007': 'Cognitive Radio Networks (CRNs) have paved a road for Secondary Users (SUs) to opportunistically exploit unused spectrum without harming the communications among Primary Users (PUs). In this paper, practical unicast and convergecast schemes, which are overlooked by most of the existing works for CRNs, are proposed. We first construct a cell-based virtual backbone for CRNs. Then prove that SUs have positive probabilities to access the spectrum and the expected one hop delay is bounded by a constant, if the density of PUs is finite. According to this fact, we proposed a three-step unicast scheme and a two-phase convergecast scheme. We demonstrate that the induced delay from our proposed Unicast Scheduling (US) algorithm scales linearly with the transmission distance between the source and the destination. Furthermore, the expected delay of the proposed Convergecast Scheduling (CS) algorithm is proven to be upper bounded by $O(\\\\log n + \\\\sqrt{n/\\\\log n})$ . To the best of our knowledge, this is the first study of convergecast in CRNs. Finally, the performance of the proposed algorithms is validated through simulations.',\n",
       " '53e997d1b7602d9701fc931c': 'Performance optimization has become one of the most important problems at all design stages of today&#39;s highly integrated circuits. During logic synthesis performance optimization is guided by only rough net delay models. Therefore, net delays cannot be optimized effectively during logic synthesis. As device dimensions are shrinking in deep submicron designs, net delays tend to dominate performance...',\n",
       " '53e997d1b7602d9701fc934d': 'We design and implement a web service for LiteOS-based wireless sensor networks (WSNs). LiteOS is a newly developed operating system for the sensor networks. Taking advantage of UNIX-like shell commands and C programming language supported by LiteOS, we implement a web service middleware which interacts with the front-end web application as well as the WSNs such that users can remotely request and view the sensor readings. Our web service system is equipped with secure membership, a visualizer for sensor readings, and accepts parameterized queries from multiple users simultaneously. We also address the issues of improving user perceived response time and energy efficiency of sensor nodes in the web service.\\n\\n',\n",
       " '53e997d1b7602d9701fc9358': 'We introduce, in this paper, a variational framework for the construction of improved treatment plans in volumetric modulated arc therapy for cancer treatment. This framework consists of a shape optimization component, from the molding of beam shapes, as well as strong constraints, from the equipment involved, on both beam shapes and intensities. We apply the binary level-set method, to handle complex shape topologies, and the fast sweeping method, to handle beam intensity constraints. The result is a fast, flow-based algorithm, in a simplified setting, that guarantees energy decrease. We include numerical simulations of clinical cases to demonstrate the efficacy of the approach.',\n",
       " '53e997d6b7602d9701fc96f6': 'In this paper we demonstrate two new approaches to deriving three- dimensional surface orientation information (\"shape\") from two-dimensional image cues. The two approaches are the method of affine-transformable pat- terns and the shape-from-texture paradigm. They are introduced by a specific application common to both: the concept of skewed symmetry. Skewed sym- nietry is shown to constrain the relationship of observed distortions in a known object regularity to ;I sniall subset of possible underlying surface orientations. Iksidcs this constraint. valuable in its own right, the two methods are shown to generate other surface constraints as well. Some applications are presented of skewed symmetry to line drawing analysis, to the use of gravity in shape under- \\\\tdnding. and to glotxil hipc recovery.',\n",
       " '53e997d6b7602d9701fc96fe': 'Weighted vector directional filters are used to enhance multichannel image data and have attracted a lot of interest from researchers in the image processing community. This paper describes a novel method for deriving the weights of a vector directional filter that uses an interactive evolution strategy. We performed an empirical study in which 30 participants each developed two filters using our approach. Each participant compared the performance of his/her filters to the basic vector directional filter and a filter that had previously been developed using a genetic algorithm. Of the filters studied, our interactive approach was the most effective at removing salt and pepper noise for the case when the percentage of corrupt image pixels was low.',\n",
       " '53e997d1b7602d9701fc90ea': 'In this paper, we describe a 64-bit multi-round block cipher, suitable for software implementation, in which three different encryption methods are combined in a sequence determined by the user key. In this way, whilst the design is public knowledge, the actual encryption method selected by the user key is kept secret. This method has been implemented using the three block ciphers: Khufu, Loki, and a cipher by Lai and Massey. The performance and cryptanalysis results using the CRYPT-XB package for this example are provided.',\n",
       " '53e997d1b7602d9701fc9123': 'For those who want our discipline to be based on rigorous scientific foundations, this is confusing and disquieting. Information is a powerful metaphor, often compared with a fluid that can flow, have a source, and be extracted, transferred, acquired, and contained; data, symbols, signals, and messages are carriers of the fluid, Suppose that information is only a metaphor? Suppose that it has no measurable existence? Might it be to us what ether was to physicists in the last century? In the 1830s, Michael Faraday and others studying electricity and magnetism postulated “ether” as the medium through which electrical and magnetic action propagated. But every attempt to measure the speed of the earth’s passage through the ether—culminating with the famous Michelson–Morley experiment in 1887—detected nothing. In 1905, Einstein explained these baffling findings by proposing that the long-discussed ether did not exist, and did not need to because light travels at a fixed speed independent of the frame of reference. Einstein’s removal of the ether became part of the foundation of modern physics [Burke 1988]. In like manner, information as a measurable quantity does not appear to be needed to design computers and software.',\n",
       " '53e997d1b7602d9701fc90e8': 'Energy consumption in data centers can be reduced by efficient design of the data centers and efficient management of computing resources and cooling units. A major obstacle in the analysis of data centers is the lack of a holistic simulator, where the impact of new computing resource (or cooling) management techniques can be tested with different designs (i.e., layouts and configurations) of data centers. To fill this gap, this paper proposes Green Data Center Simulator (GDCSim) for studying the energy efficiency of data centers under various data center geometries, workload characteristics, platform power management schemes, and scheduling algorithms. GDCSim is used to iteratively design green data centers. Further, it is validated against established CFD simulators. GDCSim is developed as a part of the BlueTool infrastructure project at Impact Lab.',\n",
       " '53e997d6b7602d9701fc9715': \"Wireless router platforms based on the Linux operating system are becoming popular in consumers' home networks. The transmission of multimedia data or their use as media-aware network elements imposes high traffic and computational loads on these devices. Thus, it is interesting to evaluate the networking and processing capabilities of such home router platforms in order to assess their usefulness for improved multimedia services such as in-network H.264/SVC video stream adaptation. This paper presents a performance evaluation of three home router platforms representative for low-end, mid-range, and high-end devices. The scope of the evaluation is the performance of the Linux networking stack on these routers; results for both application-layer (TCP and UDP) transmission and kernel-level (UDP) traffic routing are given. The results show that both TCP and UDP throughputs are significantly below (less than half of) the outgoing (wired) links' nominal capacities and depend very much on the sizes of the transmitted data blocks. This clearly indicates that the networking performance is limited by the platforms' processing capabilities and the lack of mechanisms that offload networking tasks from the CPUs. This behaviour cannot be observed on today's PC systems and has to be considered when deploying multimedia services on these network devices. Furthermore, a detailed analysis of the Linux networking stack reveals that the performance is heavily impacted by the netfilter code, even when no packet filtering or network address translation is being performed. Considerable performance gains can be achieved when this netfilter code is bypassed.\",\n",
       " '53e997d1b7602d9701fc914c': 'Exact small-sample methods for discrete data use probability distributions that do not depend on unknown parameters. However, they are conservative inferentially: the actual error probabilities for tests and confidence intervals are bounded above by the nominal level. This article surveys ways of reducing or even eliminating the conservatism. Fuzzy inference is a recent innovation that enables one to achieve the error probability exactly. We present a simple way of conducting fuzzy inference for discrete one-parameter exponential family distributions. In practice, most scientists would find this approach unsuitable yet might be disappointed by the conservatism of ordinary exact methods. Thus, we recommend using exact small-sample distributions but with inferences based on the mid-P value. This approach can be motivated by fuzzy inference, it is less conservative than standard exact methods, yet usually it does well in terms of achieving desired error probabilities. We illustrate for inferences about the binomial parameter.',\n",
       " '53e997d1b7602d9701fc9130': \"This paper discusses our system's results at the Spanish Question Answering task of CLEF 2007. Our system is centered in a full data-driven approach that combines information retrieval and machine learning techniques. It mainly relies on the use of lexical information and avoids any complex language processing procedure. Evaluation results indicate that this approach is very effective for answering definition questions from Wikipedia. In contrast, they also reveal that it is very difficult to respond factoid questions from this resource solely based on the use of lexical overlaps and redundancy.\",\n",
       " '53e997d6b7602d9701fc9763': 'The Small Heterodimer Partner (SHP) is an orphan nuclear receptor and an atypical member of the nuclear receptor superfamily Since its discovery, a growing body of evidences have pointed out a pivotal role for SHP in the transcriptional regulation of a variety of target genes involved in diverse metabolic pathways. While we have previously developed a homology model of the structure of SHP that was instrumental to identify a putative ligand binding pocket and suggest the possibility of the development of synthetic modulators, others reported that some atypical retinoids may represent the first synthetic ligands for this receptor. In this work, we report a combined computational approach aimed at shedding further lights on the binding mode and mechanism of action of some atypical retinoids as ligands of SHP. The results have been instrumental to design mutagenesis experiments whose preliminary data suggest the presence of a functional site in SHP as defined by residues Phe96, Arg138 and Arg238. While further experimental studies are ongoing, these findings constitute the basis for the design and identification of novel synthetic modulators of SHP functions.',\n",
       " '53e997d6b7602d9701fc9797': 'The aim of this paper is to show how the Model Driven Architecture (MDA) can be used in relation with component based software engineering. A software component only exhibits its provided or required interfaces, hence defining basic contracts between components allowing one to properly wire them. These contractually specified interfaces should go well beyond mere syntactic aspects: they should also involve functional, synchronization and Quality of Service (QoS) aspects. In large, mission-critical component based systems, it is also particularly important to be able to explicitly relate the QoS contracts attached to provided interfaces with the QoS contracts obtained from required interfaces. We thus introduce a QoS contract model (called QoSCL for QoS Constraint Language), allowing QoS contracts and their dependencies to be modeled in a UML2.0 modeling environment. Building on Model Driven Engineering techniques, we then show how the very same QoSCL contracts can be exploited for (1) validation of individual components, by automatically weaving contract monitoring code into the components; and (2) validation of a component assembly, including getting end-to-end QoS information inferred from individual component contracts, by automatic translation to a Constraint Logic Programming language. We illustrate our approach with the example of a GPS (Global Positioning System) software component, from its functional and contractual specifications to its implementation in a .Net framework.',\n",
       " '53e997d1b7602d9701fc9398': 'We consider a generalized model of neural network with a fuzziness and chaos. The origin of the fuzzy signals lies in complex biochemical and electrical processes of the synapse and dendrite membrane excitation and the inhibition mechanism. The mathematical operations included into fuzzy neural network modeling are: the scalar product between inputs of layers and synaptic weights is replaced by a fuzzy logic multiplication, the sum of products changes to the fuzzy logic sums, and the operators such as supremum, maximum, and minimum are presented for a fuzzy description. The algorithm of varying membership functions, built basing on a backpropagation paradigm and a method of fuzzy neural optimization, has been considered. Both fuzzy properties and a chaos phenomenon are analyzed basing upon experimental computations.',\n",
       " '53e997d6b7602d9701fc950c': 'The problem of determining maximal safe sets and hybrid controllers is computationally intractable because of the mathematical generality of hybrid system models. Given the practical and theoretical relevance of the problem, finding implementable procedures that could at least approximate the maximal safe set is important. To this end, we begin by restricting our attention to a special class of hybrid systems: switching systems. We exploit the structural properties of the graph describing the discrete part of a switching system to develop an efficient procedure for the computation of the safe set. This procedure requires the computation of a maximal controlled invariant set. We then restrict our attention to linear discrete-time systems for which there is a wealth of results available in the literature for the determination of maximal controlled invariant sets. However, even for this class of systems, the computation may not converge in a finite number of steps. We then propose to compute inner approximations that are controlled invariant and for which a procedure that terminates in a finite number of steps can be obtained. A tight bound on the error can be given by comparing the inner approximation with the classical outer approximation of the maximal controlled invariant set. Our procedure is applied to the idle-speed regulation problem in engine control to demonstrate its efficiency.',\n",
       " '53e997d6b7602d9701fc966c': 'Among all integration rules with n points, it is well-known that n-point Gauss-Legendre quadrature rule@!\\\\\"-\\\\\"1^1f(x)dx~@?\\\\\"i\\\\\"=\\\\\"1^nw\\\\\"if(x\\\\\"i)has the highest possible precision degree and is analytically exact for polynomials of degree at most 2n-1, where nodes x\\\\\"i are zeros of Legendre polynomial P\\\\\"n(x), and w\\\\\"i\\'s are corresponding weights. In this paper we are going to estimate numerical values of nodes x\\\\\"i and weights w\\\\\"i so that the absolute error of introduced quadrature rule is less than a preassigned tolerance @e\\\\\"0, say @e\\\\\"0=10^-^8, for monomial functionsf(x)=x^j,j=0,1,...,2n+1. (Two monomials more than precision degree of Gauss-Legendre quadrature rules.) We also consider some conditions under which the new rules act, numerically, more accurate than the corresponding Gauss-Legendre rules. Some examples are given to show the numerical superiority of presented rules.',\n",
       " '53e997d6b7602d9701fc9679': 'Registration of histopathology to in vivo magnetic resonance imaging (MRI) of the prostate is an important task that can be used to optimize in vivo imaging for cancer detection. Such registration is challenging due to the change in volume and deformation of the prostate during excision and fixation. One approach towards this problem involves the use of an ex vivo MRI of the excised prostate specimen, followed by in vivo to ex vivo MRI registration of the prostate. We propose a novel registration method that uses a patient-specific biomechanical model acquired using magnetic resonance elastography to deform the in vivo volume and match it to the surface of the ex vivo specimen. The forces that drive the deformations are derived from a region-based energy, with the elastic potential used for regularization. The incorporation of elastography data into the registration framework allows inhomogeneous elasticity to be assigned to the in vivo volume. We show that such inhomogeneity improves the registration results by providing a physical regularization of the deformation map. The method is demonstrated and evaluated on six clinical cases.',\n",
       " '53e997d6b7602d9701fc96d2': 'The inadequacies of the current Internet data and control planes to adapt to changes have given rise to several new proposed Internet architectures. In many cases, these new architectures enable a variety of \"services\" in the network that can be used to customize network functionality and to add new features after deployment. One key challenge to such network services is that they need to be composed from various networking resources at runtime. Therefore, it is critical to develop methods for accurately specifying the semantics of network services and composing them dynamically. In this article, we propose a service description semantic, discuss how to formalize the service composition problem and use the formalization in conjunction with a logic planner to provide an efficient solution.',\n",
       " '53e997d6b7602d9701fc9b66': 'Pauling and Corey proposed a pleated-sheet configuration, now called alpha-sheet, as one of the protein secondary structures in addition to alpha-helix and beta-sheet. Recently, it has been suggested that alpha-sheet is a common feature of amyloidogenic intermediates. We have investigated the stability of antiparallel beta-sheet and two conformations of alpha-sheet in solution phase using the density functional theoretical method. The peptides are modeled as two-strand acetyl-(Ala)(2)-N-methylamine. Using stages of geometry optimization and single point energy calculation at B3LYP/cc-pVTZ//B3LYP/6-31G* level and including zero-point energies, thermal, and entropic contribution, we have found that beta-sheet is the most stable conformation, while the alpha-sheet proposed by Pauling and Corey has 13.6 kcal/mol higher free energy than the beta-sheet. The alpha-sheet that resembles the structure observed in molecular dynamics simulations of amyloidogenic proteins at low pH becomes distorted after stages of geometry optimization in solution. Whether the alpha-sheets with longer chains would be increasingly favorable in water relative to the increase in internal energy of the chain needs further investigation. Different from the quantum mechanics results. AMBER parm94 force field gives small difference in solution phase energy between alpha-sheet and beta-sheet. The predicted amide I IR spectra of alpha-sheet shows the main band at higher frequency than beta-sheet. (C) 2009 Wiley Periodicals, Inc. J Comput Chem 31: 1216-1223, 2010',\n",
       " '53e997d6b7602d9701fc9b7f': 'This paper argues that real-time control systems should be structured in abstraction layers in provide adequate instruction formalisms for both task specification and for low level control, and that the design information used when specifying the system should be included in the encoding of the abstraction barrier. When this design infor- mation is available, describing the semantics of the abstraction, the problems with interdependencies between real-time control and task level operation can be handled. An example of a system encompassing this principle — Aramis — is presented. The paper discusses how this prin- ciple can be utilised to analyse the system and for error recovery support.',\n",
       " '53e997d6b7602d9701fc999f': 'During the last three decades, a considerable amount of software has been developed based on obsolete technologies (such as using procedural languages). This type of systems has undergone severe code revisions during a long time period. As a consequence, the high level of entropy combined with imprecise documentation about the design and architecture make the maintenance more difficult, time consuming, and costly. On the other hand, these systems have important economical value; many of them are crucial to their owners [4]. For the high cost of lost former investment and business knowledge that embedded in those systems, in many cases, simply abandon legacy systems and re-develop new systems based on new technology is not the choice. Migrating legacy system toward new emerging technology is an appropriate solution. However, migrating legacy system towards new technology is a complex system engineering work. In this paper, we propose a novel approach to reduce the migration complexity. We apply dynamic program analysis, software visualization, knowledge recovery, and divide-and-conquer techniques to cope with the complexity issue in legacy software migration project.',\n",
       " '53e997d6b7602d9701fc9a82': 'Configuration programming is the process whereby components written in any conventional programming language can be bound together to form a dynamic system, often suitable for execution on distributed hardware. Among the specialised languages that exist for configuration programming there is currently a debate over the importance of recognising the connections between components as being as important as the components themselves. This paper lays out the pros and cons of the debate, outlining in the process the properties and roles of connectors. By means of experiments we show how connectors influence the way configurations are programmed and also how some of the effects can be simulated. The examples are given in Darwin, UNICON and WRIGHT and reference is also made lo the status of other current configuration languages.',\n",
       " '53e997d6b7602d9701fc9dbe': 'The aim of the WebGen system is to allow visually impaired users to create web presentations in a simple and natural way by means of dialogue. This paper describes the basic methods and principles used in the system Web-Gen, especially the system structure and the dialogue interface. An illustrative example of a dialogue is included as well as the resulting web page.',\n",
       " '53e997d6b7602d9701fc9dca': 'IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and a major enhancement to the standard is the addition of decimal format. This paper proposes two novel BCD adders called carry skip and carry look-ahead BCD adders respectively. Furthermore, in the recent years, reversible logic has emerged as a promising technology having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. It is not possible to realize quantum computing without reversible logic. Thus, this paper also paper provides the reversible logic implementation of the conventional BCD adder as the well as the proposed Carry Skip BCD adder using a recently proposed TSG gate. Furthermore, a new reversible gate called TS-3 is also being proposed and it has been shown that the proposed reversible logic implementation of the BCD Adders is much better compared to recently proposed one, in terms of number of reversible gates used and garbage outputs produced. The reversible BCD circuits designed and proposed here form the basis of the decimal ALU of a primitive quantum CPU.',\n",
       " '53e997d6b7602d9701fc9b32': \"A teleconference system via mobile Internet connections is developed for homecare services. An important purpose of such a system is the realization of real time instruction from a service station (doctors) to service providers (nurses) in mobile conditions. In this paper, we present a whiteboard system as an interactive communication platform between the station site and home site. The following four features of this system are summarized: 1) two distinct layers including image layer and memo layer are used to save mouse motions; 2) a snapshot from USB camera presents medical images or illustrations to all participants' computers in a teleconference; 3) the synchronization of any data and operations (such as displaying an image, drawing a line, and so on) within all participants' computers; 4) a realization via mobile Internet connections, where communication abilities are limited.\",\n",
       " '53e997d6b7602d9701fc9e52': 'UP is the class of languages accepted by polynomial-time nondeterministic Turing machines that have at most one accepting path. We show that the quadratic residue problem belongs to UP ∩ coUP. This answers affirmatively an open problem, discussed in Theory of Computational Complexity (Du and Ko, 2000), of whether the quadratic nonresidue problem is in NP. We generalize to higher powers and show the higher power residue problem also belongs to UP ∩ coUP.',\n",
       " '53e997d6b7602d9701fc9b9a': 'This paper investigates the necessary features of an effective clause weighting local search algorithm for propositional satisfiability testing. Using the recent history of clause weighting as evidence, we suggest that the best current algorithms have each discovered the same basic framework, that is, to increase weights on false clauses in local minima and then to periodically normalize these weights using a decay mechanism. Within this framework, we identify two basic classes of algorithm according to whether clause weight updates are performed additively or multiplicatively. Using a state-of-the-art multiplicative algorithm (SAPS) and our own pure additive weighting scheme (PAWS), we constructed an experimental study to isolate the effects of multiplicative in comparison to additive weighting, while controlling other key features of the two approaches, namely, the use of pure versus flat random moves, deterministic versus probabilistic weight smoothing and multiple versus single inclusion of literals in the local search neighbourhood. In addition, we examined the effects of adding a threshold feature to multiplicative weighting that makes it indifferent to similar cost moves. As a result of this investigation, we show that additive weighting can outperform multiplicative weighting on a range of difficult problems, while requiring considerably less effort in terms of parameter tuning. Our examination of the differences between SAPS and PAWS suggests that additive weighting does benefit from the random flat move and deterministic smoothing heuristics, whereas multiplicative weighting would benefit from a deterministic/probabilistic smoothing switch parameter that is set according to the problem instance. We further show that adding a threshold to multiplicative weighting produces a general deterioration in performance, contradicting our earlier conjecture that additive weighting has better performance due to having a larger selection of possible moves. This leads us to explain differences in performance as being mainly caused by the greater emphasis of additive weighting on penalizing clauses with relatively less weight.',\n",
       " '53e997d6b7602d9701fc9e54': 'The deformable sheet, a physical model that provides a natural framework for addressing many vision problems that can be solved by smoothness-constrained optimization, is described. Deformable sheets are characterized by a global energy functional, and the smoothness constraint is represented by a linear internal energy term. Analogous to physical sheets, the model sheets are deformed by problem-specific external forces and, in turn, impose smoothness on the applied forces. The model unifies the properties of scale and smoothness into a single parameter that makes it possible to perform scale space tracking by properly controlling the smoothness constraint. Specifically, the desired scale space trajectory is found by solving a differential equation in scale. The simple analytic dependence on scale also provides a mechanism for adaptive step size control. Results from application of the deformable sheet model to various problems in computational vision are presented.',\n",
       " '53e997d6b7602d9701fc9d88': 'A linear function approximation-based reinforcement learning algorithm is proposed for Markov decision processes with infinite horizon risk-sensitive cost. Its convergence is proved using the \"o.d.e. method\" for stochastic approximation. The scheme is also extended to continuous state space processes.',\n",
       " '53e997d7b7602d9701fcabdd': \"It's a hotspot to expend the research on support vector machine from a two-class issue to a multi-class one. Among all kinds of methods, Bintree multi-class text categorization algorithm based on support vector machine is more effective in training and sorting then others, and it works out the impartibility problem. So it is a good method. The dissertation systematically researches and analyses Bintree multi-class text categorization algorithm based on support vector machine, and improves it. That is, assemble first, and then sort them when the size of testing texts is too large. The aim is that after improvement the judgment of the testing text does not have to begin from the base crunode of Bintree, instead the testing text can be put into category function to be computed. The improvement can enhance the efficiency of text categorization and the probability of accurate categorization when the size of testing texts is too big and the quantity of sorted functions is too large.\",\n",
       " '53e997d7b7602d9701fcab58': 'This paper presents a language framework for a concurrent description and an implementation of a synchroniza-tion mechanism in distributed object-oriented computation.Some concurrent object-oriented languages are considered to be constructed from a user-defined function and a synchronization constraint. We propose the language for the synchronization constraint, which allows to define mutual exclusion and conditional synchronization by using declarative notations. The synchronization constraint is separated from user-defined functions, and the relation between them is described in the binding code. The binding mechanism reduces the dependency between them and contributes to code reuse. This framework makes it is easy for programmers to understand and describe the use-defined functions and synchronization constraints independently.The implementation of the synchronization mechanism is based on reconfigurable object model that we are developing. The user-defined function and synchronization mechanism are implemented as its own separated module in the single object. Then the synchronization mechanism has flexibility and preserves the concept of object-orientation.',\n",
       " '53e997d7b7602d9701fcac50': \"Markov random field models are powerful tools for the study of complex systems. However, little is known about how the interactions between the elements of such systems are encoded, especially from an information-theoretic perspective. In this paper, our goal is to enlighten the connection between Fisher information, Shannon entropy, information geometry and the behavior of complex systems modeled by isotropic pairwise Gaussian Markov random fields. We propose analytical expressions to compute local and global versions of these measures using Besag's pseudo-likelihood function, characterizing the system's behavior through its Fisher curve, a parametric trajectory across the information space that provides a geometric representation for the study of complex systems in which temperature deviates from infinity. Computational experiments show how the proposed tools can be useful in extracting relevant information from complex patterns. The obtained results quantify and support our main conclusion, which is: in terms of information, moving towards higher entropy states (A -> B) is different from moving towards lower entropy states (B -> A), since the Fisher curves are not the same, given a natural orientation (the direction of time).\",\n",
       " '53e997d7b7602d9701fcac52': 'It has been a challenging problem to determine the smallest graph class where a problem is proved to be hard. In the literature, this has been pointed out to be very important in order to establish the real nature of a combinatorial problem.',\n",
       " '53e997d7b7602d9701fcade5': 'Determination of appropriate neural-network (NN) structure is an important issue for a given learning or training task since the NN performance depends much on it. To remedy the weakness of conventional BP neural networks and learning algorithms, a new Laguerre orthogonal basis neural network is constructed. Based on this special structure, a weights-direct-determination method is derived, which could obtain the optimal weights of such a neural network directly (or to say, just in one step). Furthermore, a growing algorithm is presented for determining immediately the smallest number of hidden-layer neurons. Theoretical analysis and simulation results substantiate the efficacy of such a Laguerre-orthogonal-basis neural network and its growing algorithm based on the weights-direct-determination method.',\n",
       " '53e997d7b7602d9701fcad90': 'The random oracle model is an idealized theoretical model that has been successfully used for designing many cryptographic algorithms and protocols. Unfortunately, a series of results has shown that proofs of security in the idealized random oracle model do not translate into security in the standard model (basically synonymous with \"real systems\"), so the reasoning that protocols designed using random oracles are secure on real systems is heuristic at best, and fundamentally flawed at worst. In this paper, we consider how architectural changes taking place in real systems today, specifically the introduction of the trusted platform module, affect the realizability of random oracles. In particular, we show how a TPM that is only trivially enhanced from real, standard TPMs can leverage one of its most powerful capabilities -- the capability of keeping secrets from the host in which it resides -- in order to provide functionality that is indistinguishable from a true random oracle to any polynomial time adversary. In addition to a careful description of how this works, we provide security proofs based on assumptions of TPM security, and provide concrete performance estimates through benchmarks using a current TPM. To prove the security of our TPM-based scheme, we formally define and prove properties about a cryptographic primitive which we call a \"hybrid pseudo-random function\" that may be of independent interest.',\n",
       " '53e997d7b7602d9701fcae57': ' Drawing on our previous work [4], this paper presents an interthreadedmotif-based behavioral organization architecture anddemonstrates its application in organizing the behaviors of a syntheticagent. The developed architecture enables agent behaviorsto be developed as a result of the concurrent self-organization inindividual motifs as well as their crisscrossing interactions. Theself-organizing motifs contribute to the global organization of agentbehaviors through data exchange and... ',\n",
       " '53e997d7b7602d9701fcaf3c': \"The generalized Nash equilibrium problem (GNEP) is a generalization of the standard Nash equilibrium problem (NEP), in which each player's strategy set may depend on the rival players' strategies. The GNEP has recently drawn much attention because of its capability of modeling a number of interesting conflict situations in, for example, an electricity market and an international pollution control. However, a GNEP usually has multiple or even infinitely many solutions, and it is not a trivial matter to choose a meaningful solution from those equilibria. The purpose of this paper is two-fold. First we present an incremental penalty method for the broad class of GNEPs and show that it can find a GNE under suitable conditions. Next, we formally define the restricted GNE for the GNEPs with shared constraints and propose a controlled penalty method, which includes the incremental penalty method as a subprocedure, to compute a restricted GNE. Numerical examples are provided to illustrate the proposed approach.\",\n",
       " '53e997d7b7602d9701fcaf4c': 'We present an object-oriented framework for constructing parallel implementations of stencil algorithms. This framework simplifies the development process by encapsulating the common aspects of stencil algorithms in a base stencil class so that application-specific derived classes can be easily defined via inheritance and overloading. In addition, the stencil base class contains mechanisms for parallel execution. The result is a high-performance, parallel, application-specific stencil class. We present the design rationale for the base class and illustrate the derivation process by defining two sub-classes, an image convolution class and a PDE solver. The classes have been implemented in Mentat, an object-oriented parallel programming system that is available on a variety of platforms. Performance results are given for a network of Sun SPARCstation IPCs.',\n",
       " '53e997d7b7602d9701fcae6f': 'The power saving mechanisms of the current IEEE 802.16e system are designed to take no consideration of the harmonization between power saving classes (PSCs) I and II. Using the IEEE 802.16e standard as a basis, we propose a dynamic power saving mechanism that increases unavailability interval when a mobile station (MS) uses PSCs. The proposed mechanism adjusts the timing of the sleep window of PSC I to maximize the unavailability interval of the MSs. As a result, the proposed scheme achieves power saving of the MSs. Through numerical analysis and simulations, we show that the proposed mechanism can reduce the power consumption of the MSs considerably compared with conventional mechanisms.',\n",
       " '53e997d7b7602d9701fcae7d': 'Using an approach developed in physics, we propose a new framework for the\\nstudy of cellular networks. The key idea of the physical network model we\\npropose is to replace the discrete base stations (BS) entities by a continuum\\nof transmitters which are spatially distributed in the network. This allows us\\nto establish a closed form formula of the other-cell downlink interference\\nfactor f, as a function of the location of the mobile. We define here f as the\\nratio of outer cell received power (i.e. the power received from other cells)\\nto the inner cell received power. This physical model allows calculating the\\ninfluence of interference on any mobile in a cell, whatever its position.\\nResults obtained with that closed-form formula are close to the ones obtained\\nby simulations using a traditional hexagonal network model. Since the physical\\nmodel allows to establish a closed form formula of the interference factor, it\\nallows to do analytical studies of wireless networks such as outage\\nprobability, quality of service, capacity.',\n",
       " '53e997d7b7602d9701fcae94': 'We study the existence of solutions to a class of first-order linear fuzzy differential equations subject to periodic boundary conditions from the point of view of generalized differentiability. The objective of this paper is to show that fuzzy differential equations under generalized differentiability can be used in the study of periodic phenomena, by considering a combination of two types of derivatives with a switching point. We provide sufficient conditions which guarantee that the piecewise-defined solutions match adequately and illustrate, through some examples, the process of construction and calculation of solutions.',\n",
       " '53e997d7b7602d9701fcae8f': 'Cartesian stiffness of a 7-DOF(degree-of-freedom) hybrid cable-driven anthropomorphic-arm manipulator(CDAM) was analyzed,in which the shoulder module and wrist module were both 3-DOF cable-driven spherical joint(CDSJ).The calculation process of Cartesian stiffness matrix of the manipulator was demonstrated in detail through the analysis of the stiffness of CDSJ combined with the solution for the stiffness of serial manipulators.The calculation process could be divided into three steps: firstly,the analysis of CDSJ stiffness was introduced to solve the stiffness of shoulder and wrist;secondly,the stiffness of elbow,a cable-driven revolute joint,was calculated;lastly,by merging the stiffness of the wrist,elbow and shoulder module to the joint stiffness matrix,the modified conservative congruence transformation(CCT) was used to get the solution of Cartesian stiffness matrix of CDAM.Based on the stiffness analysis of CDAM,a stiffness optimal algorithm was proposed to enhance the stiffness in motion.The simulation result proves the effectiveness of the algorithm.',\n",
       " '53e997d7b7602d9701fcaea3': \"Because modern GPGPU can provide significant computing power and has very high memory bandwidth, and also, developer-friendly programming interfaces such as CUDA have been introduced, GPGPU becomes more and more accepted in the HPC research area. Much research has been done to help developers to better optimize GPU applications. But to fully understand GPU performance behavior remains a hot research topic. We developed an analytical tool called TEG (Timing Estimation tool for GPU) to estimate GPU performance. Previous work shows that TEG has good approximation and can help us to quantify bottlenecks' performance effects. We have made some improvement to the tool and in this paper, we use TEG to analyze the GPU performance scaling behavior. TEG takes the dis-assembly output of CUDA kernel binary code and instruction trace as input. It does not execute the codes, but try to model the execution of CUDA codes with timing information. Because TEG takes the native GPU assembly code as input, it can estimate the execution time with a small error and it allows us to get more insight into GPU performance result.\",\n",
       " '53e997d7b7602d9701fcaea4': 'Conversations in poster sessions in academic events, referred to as poster conversations, pose interesting and challenging topics on multi-modal analysis of multi-party dialogue. This article gives an overview of our project on multi-modal sensing, analysis and \"understanding\" of poster conversations. We focus on the audience\\'s feedback behaviors such as non-lexical backchannels (reactive tokens) and noddings as well as joint eye-gaze events by the presenter and the audience. We investigate whether we can predict when and who will ask what kind of questions, and also interest level of the audience. Based on these analyses, we design a smart posterboard which can sense human behaviors and annotate interactions and interest level during poster sessions.',\n",
       " '53e997d7b7602d9701fcaeb5': 'A new hybrid fuzzy clustering algorithm that incorporates the Fuzzy C-means (FCM) into the Quantum-behaved Particle Swarm Optimization (QPSO) algorithm is proposed in this paper (QPSO+FCM). The QPSO has less parameters and higher convergent capability of the global optimizing than Particle Swarm Optimization algorithm (PSO). So the iteration algorithm is replaced by the QPSO based on the gradient descent of FCM, which makes the algorithm have a strong global searching capacity and avoids the local minimum problems of FCM and in a large degree avoids depending on the initialization values. This paper also investigates the ability of FCM algorithm, PSO+FCM algorithm and GA+FCM algorithm with Iris testing data and Wine testing data. The simulation result proves that compared with other algorithms, the new algorithm not only has the favorable convergence but also has been obviously improved the clustering effect.',\n",
       " '53e997d7b7602d9701fcafcd': 'Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations.In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data distributions give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using them in an on-line fashion for selectivity estimation. Our histograms also provide quick approximate answers to OLAP queries when the exact answers are not required. Our method captures the joint distribution of multiple attributes effectively, even when the attributes are correlated. Experiments confirm that our histograms offer substantial improvements in accuracy over random sampling and other previous approaches.',\n",
       " '53e997d7b7602d9701fcaf27': 'An area-efficient 1-D median filter based on the sorting network is presented in this brief. It is a word-level filter, storing the samples in the window in descending order according to their values. When a sample enters the window, the oldest sample is removed, and the new sample is inserted in an appropriate position to preserve the sorting of samples. To increase the throughput, the deletion and insertion of samples are performed in one clock cycle, so that the median output is generated at each cycle. The experimental results have shown the improved area efficiency of our design in comparison with previous work.',\n",
       " '53e997d7b7602d9701fcafea': 'This paper explores three theoretical approaches for estimating the degree of correctness to which the accuracy figures of a gridded Digital Elevation Model (DEM) have been estimated depending on the number of checkpoints involved in the assessment process. The widely used average-error statistic Mean Square Error (MSE) was selected for measuring the DEM accuracy. The work was focused on DEM uncertainty assessment using approximate confidence intervals. Those confidence intervals were constructed both from classical methods which assume a normal distribution of the error and from a new method based on a non-parametric approach. The first two approaches studied, called Chi-squared and Asymptotic Student t, consider a normal distribution of the residuals. That is especially true in the first case. The second case, due to the asymptotic properties of the t distribution, can perform reasonably well with even slightly non-normal residuals if the sample size is large enough. The third approach developed in this article is a new method based on the theory of estimating functions which could be considered much more general than the previous two cases. It is based on a non-parametric approach where no particular distribution is assumed. Thus, we can avoid the strong assumption of distribution normality accepted in previous work and in the majority of current standards of positional accuracy. The three approaches were tested using Monte Carlo simulation for several populations of residuals generated from originally sampled data. Those original grid DEMs, considered as ground data, were collected by means of digital photogrammetric methods from seven areas displaying differing morphology employing a 2 by 2\\xa0m sampling interval. The original grid DEMs were subsampled to generate new lower-resolution DEMs. Each of these new DEMs was then interpolated to retrieve its original resolution using two different procedures. Height differences between original and interpolated grid DEMs were calculated to obtain residual populations. One interpolation procedure resulted in slightly non-normal residual populations, whereas the other produced very non-normal residuals with frequent outliers. Monte Carlo simulations allow us to report that the estimating function approach was the most robust and general of those tested. In fact, the other two approaches, especially the Chi-squared method, were clearly affected by the degree of normality of the residual population distribution, producing less reliable results than the estimating functions approach. This last method shows good results when applied to the different datasets, even in the case of more leptokurtic populations. In the worst cases, no more than 64-128 checkpoints were required to construct an estimate of the global error of the DEM with 95% confidence. The approach therefore is an important step towards saving time and money in the evaluation of DEM accuracy using a single average-error statistic. Nevertheless, we must take into account that MSE is essentially a single global measure of deviations, and thus incapable of characterizing the spatial variations of errors over the interpolated surface.',\n",
       " '53e997d7b7602d9701fcb0c2': 'Language understanding is a long-standing problem in computer science. However, the human brain is capable of processing complex languages with seemingly no difficulties. This paper shows a model for language understanding using biologically plausible neural networks composed of associative memories. The model is able to deal with ambiguities on the single word and grammatical level. The language system is embedded into a robot in order to demonstrate the correct semantical understanding of the input sentences by letting the robot perform corresponding actions. For that purpose, a simple neural action planning system has been combined with neural networks for visual object recognition and visual attention control mechanisms.',\n",
       " '53e997d7b7602d9701fcb0dc': 'Curve discrimination is an important task in engineering and other sciences. We propose several shape descriptors for classifying functional data, inspired by form analysis from the image analysis field: statistical moments, coefficients of the components of independent component analysis, and two mathematical morphology descriptors (morphological covariance and spatial size distributions). These are applied to three problems: an artificial problem, a speech recognition problem, and a biomechanical application. Shape descriptors are compared with other methods in the literature, with better or similar performance obtaining.',\n",
       " '53e997d7b7602d9701fcb0e3': 'An edge-sensitive variational approach for the restoration of optical flow fields is presented. Real world optical flow fields are frequently corrupted by noise, reflection artifacts or missing local information. Still, applications may require dense motion fields. In this paper, we pick up image inpainting methodology to restore motion fields, which have been extracted from image sequences based on a statistical hypothesis test on neighboring flow vectors. A motion field inpainting model is presented, which takes into account additional information from the image sequence to improve the reconstruction result. The underlying functional directly combines motion and image information and allows to control the impact of image edges on the motion field reconstruction. In fact, in case of jumps of the motion field, where the jump set coincides with an edge set of the underlying image intensity, an anisotropic TV-type functional acts as a prior in the inpainting model. We compare the resulting image guided motion inpainting algorithm to diffusion and standard TV inpainting methods.',\n",
       " '53e997d7b7602d9701fcb141': 'This paper presents a speech enhancement method for noise ro- bust front-end and speech reconstruction at the back-end of Dis- tributed Speech Recognition (DSR). The speech noise removal algorithm is based on a two stage noise filtering LSAHT by log spectral amplitude speech estimator (LSA) and harmonic tun- neling (HT) prior to feature extraction. The noise reduced fea- tures are transmitted with some parameters, viz., pitch period, the number of harmonic peaks from the mobile terminal to the server along noise-robust mel-frequency cepstral coefficients. Speech reconstruction at the back end is achieved by sinusoidal speech representation. Finally, the performance of the system is measured by the segmental signal-noise ratio, MOS tests, and the recognition accuracy of an Automatic Speech Recognition (ASR) in comparison to other noise reduction methods.',\n",
       " '53e997d7b7602d9701fcb143': 'In this paper, novel low and high frequency performance improvement techniques are proposed. High frequency performance improvement method is based on the single pole effect reduction/completely elimination by creating a corresponding zero while a low one is based on reduction/elimination of Z terminal parasitic resistors. Therefore, two grounded capacitance multipliers, three grounded inductor simulators, a floating inductor and a voltage-mode (VM) universal filter using second-generation current conveyors (CCIIs) are used to explain the developed methods. One of the grounded inductor simulators, floating simulated inductor and VM biquad are novel. Further, both of the new simulated inductors use a grounded capacitor; accordingly, they are suitable for IC fabrication. However, all the proposed circuits need a single matching condition. A number of simulations through PSPICE program and experimental tests are accomplished to demonstrate the workability, performance and effectiveness.',\n",
       " '53e997d7b7602d9701fcb15b': 'Conventional molecular dynamics simulations macromolecules require long\\ncomputational times because the most interesting motions are very slow compared\\nwith the fast oscillations of bond lengths and bond angles that limit the\\nintegration time step. Simulation of dynamics in the space of internal\\ncoordinates, that is with bond lengths, bond angles and torsions as independent\\nvariables, gives a theoretical possibility to eliminate all uninteresting fast\\ndegrees of freedom from the system. This paper presents a new method for\\ninternal coordinate molecular dynamics simulations of macromolecules. Equations\\nof motion are derived which are applicable to branched chain molecules with any\\nnumber of internal degrees of freedom. Equations use the canonical variables\\nand they are much simpler than existing analogs. In the numerical tests the\\ninternal coordinate dynamics are compared with the traditional Cartesian\\ncoordinate molecular dynamics in simulations a 56 residue globular protein. It\\nis shown that the traditional and internal coordinate dynamics require the same\\ntime step size for the same accuracy and that in the standard geometry\\napproximation of amino acids, that is with fixed bond lengths, bond angles and\\nrigid aromatic groups, the characteristic step size is 4 fsec, that is two\\ntimes higher than with fixed bond lengths only. The step size can be increased\\nup to 11 fsec when rotation of hydrogen atoms is suppressed.',\n",
       " '53e997d7b7602d9701fcb205': \"Considering the speed in which humans resolve syntactic ambiguity, and the overwhelming evidence that syntactic ambiguity is resolved through selection of the analysis whose interpretation is the most 'sensible', one comes to the conclusion that interpretation, hence parsing take place incrementally, just about every word. Considerations of parsimony in the theory of the syntactic processor lead one to explore the simplest of parsers: one which represents only analyses as defined by the grammar and no other information.Toward this aim of a simple, incremental parser I explore the proposal that the competence grammar is a Combinatory Categorial Grammar (CCG). I address the problem of the proliferating analyses that stem from CCG's associativity of derivation. My solution involves maintaining only the maximally incremental analysis and, when necessary, computing the maximally right-branching analysis. I use results from the study of rewrite systems to show that this computation is efficient.\",\n",
       " '53e997d7b7602d9701fcb180': \"We propose a new deflnition of Representation theo- rem for many-valued modal logics, based on a complete latice of algebraic truth values, and deflne the stronger relationship between algebraic models of a given logic L and relational structures used to deflne the Kripke possible-world semantics for L. Such a new frame- work ofiers clear semantics for the satisfaction alge- braic relation, based on many-valued models of a logic L, avoiding the necessity to deflne a designated sub- set of logic values for a satisfaction relation, often dif- flcult to determine for many-valued logic, especially for bilattice based logic. We deflne the subclass of many-valued modal logics based on distributive lattices which have compact autoreferential cannonical repre- sentation. The signiflcant member of this subclass is the paraconsistent fuzzy logic extended by new logic val- ues in order to deal with incomplete and inconsistent information also. The Kripke-style semantics for this subclass of modal logics have as set of possible worlds the joint-irriducible subset of the carrier set of many- valued algebras. Such a new theory is applied for the case of autoepistemic intuitionistic many-valued logic based on Belnap's 4-valued bilattice as minimal exten- sion of classic logic used to manage incomplete and in- consistent information also.\",\n",
       " '53e997d7b7602d9701fcb244': 'The analysis of gene expression data of breast cancer is important for discovering the signatures that can classify different subtypes of tumors and predict prognosis. Biclustering algorithms have been proven to be able to group the genes with similar expression patterns under a number of samples and offer the capability to analyze the microarray data of cancer. In this study, we propose a new biclustering algorithm which uses an evolutionary search procedure. The algorithm is applied to the conditions to search for combinations of conditions for a potential bicluster. Preliminary results using synthetic and real yeast data sets demonstrate that our algorithm outperforms several existing ones. We have also applied the method to real microarray data sets of breast cancer, and successfully found several biclusters, which can be used as signatures for differentiating tumor types.',\n",
       " '53e997d7b7602d9701fcb24b': 'The hierarchical mixture of experts architecture provides a flexible procedure for implementing classification algorithms. The classification is obtained by a recursive soft partition of the feature space in a data-driven fashion. Such a procedure enables local classification where several experts are used, each of which is assigned with the task of classification over some subspace of the feature space. In this work, we provide data-dependent generalization error bounds for this class of models, which lead to effective procedures for performing model selection. Tight bounds are particularly important here, because the model is highly parameterized. The theoretical results axe complemented with numerical experiments based on a randomized algorithm, which mitigates the effects of local minima which plague other approaches such as the expectation-maximization algorithm.',\n",
       " '53e997d7b7602d9701fcb24c': 'We propose a novel algorithm based on random graphs to construct minimal perfect hash functions h. For a set of n keys, our algorithm outputs h in expected time O(n). The evaluation of h(x) requires two memory accesses for any key x and the description of h takes up 1.15n words. This improves the space requirement to 55% of a previous minimal perfect hashing scheme due to Czech, Havas and Majewski. A simple heuristic further reduces the space requirement to 0.93n words, at the expense of a slightly worse constant in the time complexity. Large scale experimental results are presented.',\n",
       " '53e997d7b7602d9701fcb252': 'Critical to crystallization chemical product design is the choice of an appropriate solvent. Traditional methods have focused on bench scale experiments using classes of solvents (e.g. polarity) with the different classes giving rise to different crystal morphologies. However, there are instances where some solvents belonging to a particular class give completely different morphology from other solvents in the same class. There has been some modeling effort aimed at predicting crystal morphology. A major drawback with some of these morphology prediction models is that they tend to be limited in application. It is clear that the solvent selection, with respect to crystal morphology cannot be carried out efficiently by just experimentation or modeling alone. This paper outlines a systematic methodology which combines targeted bench scale crystallization experiments, an efficient computer-aided molecular design (CAMD) approach and a database search approach for the design and selection of solvents for crystallization of carboxylic acids.',\n",
       " '53e997d7b7602d9701fcb558': 'The segmentation of fractured bone from computed tomographies (CT images) is an important process in medical visualization and simulation, because it enables such applications to use data of a specific patient. On the other hand, the labeling of fractured bone usually requires the participation of an expert. Moreover, close fragment can be joined after the segmentation because of their proximity and the resolution of the CT image. Classical methods perform well in the segmentation of healthy bone, but they are not able to identify bone fragments separately. In this paper, we propose a method to segment and label bone fragments from CT images. Labeling involves the identification of bone fragments separately. The method is based on 2D region growing and requires minimal user interaction. In addition, the presented method is able to separate wrongly joined fragments during the segmentation process.',\n",
       " '53e997d7b7602d9701fcb55c': 'The semiclassical limit of a weakly coupled nonlinear focusing Schrodinger system in presence of a nonconstant potential is studied. The initial data is of the form (u(1), u(2)) with u(i) = r(i)(x-(x) over tilde/epsilon)e((i/epsilon)x.xi), where (r(1), r(2)) is a real ground state solution, belonging to a suitable class, of an associated autonomous elliptic system. For epsilon sufficiently small, the solution (phi(1), phi(2)) will been shown to have, locally in time, the form (r(1)(x-x(t)/epsilon)e((i/epsilon)x.xi(t)),r(2)(x-x(t)/epsilon)e((i/epsilon)x.xi(t))), where (x(t), xi(t)) is the solution of the Hamiltonian system (x) over dot(t) = xi(t), (xi) over dot(t) = -del V(x(t)) with x(0) = (x) over tilde and xi(0) = (xi) over tilde.',\n",
       " '53e997d7b7602d9701fcb2af': 'Based on cellular automaton and multi-agent model, we propose a novel hybrid model for distributed simulation of train-group in the railway traffic. An improved cellular automaton model is designed for simulating the underlying structure of the railroad and a multi-agent model is used for dynamic scheduling of the train-group. The combination of both models answers for the distributed simulation of train-group on the railroad. Simulation and scheduling strategies are proposed to analyze and solve the problems about safety operation and dynamic scheduling. In the end, we analyze the characteristics of train flows and dealing with the emergencies of the railroad net. Experimental results show that the hybrid model is proper and efficient for distributed simulation of train-group of the railroad net.',\n",
       " '53e997d7b7602d9701fcb2ce': 'Data grid architectures must confront the antipodal problems of data locality and global data coherence. Here, we examine a new approach to meeting these conflicting needs that leverages a storage wide area network, a clustered file system, a distributed block system, and a distributed resource scheduler. We examine multi-site concurrent I/O performance on an emulated three-site data grid with inter-site separations ranging from 1000km-5000km. We examine workloads that require all sites to interact with a single, globally coherent data image. In particular, all three sites perform concurrent write operations to shared files within a single file system.',\n",
       " '53e997d7b7602d9701fcb306': 'This paper presents a methodology based on automatic knowledge discovery that aims to identify and predict the possible causes\\n that makes a patient to be considered of high cost. The experiments were conducted in two directions. The first was the identification\\n of important relationships among variables that describe the health care events using an association rules discovery process.\\n The second was the discovery of precise prediction models of high cost patients, using classification techniques. Results\\n from both methods are discussed to show that the patterns generated could be useful to the development of a high cost patient\\n eligibility protocol, which could contribute to an efficient case management model.\\n ',\n",
       " '53e997d7b7602d9701fcb2d4': 'Mitochondrial genes code for additional proteins after +2 frameshifts by reassigning stops to code for amino acids, which defines overlapping genetic codes for overlapping genes. Turtles recode stops UAR→Trp and AGR→Lys (AGR→Gly in the marine Olive Ridley turtle, Lepidochelys olivacea). In Lepidochelys the +2 frameshifted mitochondrial Cytb gene lacks stops, open reading frames from other genes code for unknown proteins, and for regular mitochondrial proteins after frameshifts according to the overlapping genetic code. Lepidochelys’ inversion between proteins coded by regular and overlapping genetic codes substantiates the existence of overlap coding. ND4 differs among Lepidochelys mitochondrial genomes: it is regular in DQ486893; in NC_011516, the open reading frame codes for another protein, the regular ND4 protein is coded by the frameshifted sequence reassigning stops as in other turtles. These systematic patterns are incompatible with Genbank/sequencing errors and DNA decay. Random mixing of synonymous codons, conserving main frame coding properties, shows optimization of natural sequences for overlap coding; Ka/Ks analyses show high positive (directional) selection on overlapping genes. Tests based on circular genetic codes confirm programmed frameshifts in ND3 and ND4l genes, and predicted frameshift sites for overlap coding in Lepidochelys. Chelonian mitochondria adapt for overlapping gene expression: cloverleaf formation by antisense tRNAs with predicted anticodons matching stops coevolves with overlap coding; antisense tRNAs with predicted expanded anticodons (frameshift suppressor tRNAs) associate with frameshift-coding in ND3 and ND4l, a potential regulation of frameshifted overlap coding. Anaeroby perhaps switched between regular and overlap coding genes in Lepidochelys.',\n",
       " '53e997d7b7602d9701fcb398': \"Specularities often confound algorithms designed to solve computer vision tasks such as image segmentation, object detection, and tracking. These tasks usually require color image segmentation to partition an image into regions, where each region corresponds to a particular material. Due to discontinuities resulting from shadows and specularities, a single material is often segmented into several sub-regions. In this paper, a specularity detection and removal technique is proposed that requires no camera calibration or other a priori information regarding the scene. The approach specifically addresses detecting and removing specularities in facial images. The image is first processed by the Luminance Multi-Scale Retinex [B. V. Funt, K. Barnard, M. Brockington, V. Cardei, Luminance-Based Multi-Scale Retinex, AIC'97, Kyoto, Japan, May 1997]. Second, potential specularities are detected and a wave-front is generated outwards from the peak of the specularity to its boundary or until a material boundary has been reached. Upon attaining the specularity boundary, the wavefront contracts inwards while coloring in the specularity until the latter no longer exists. The third step is discussed in a companion paper [M. D. Levine, J. Bhattacharyya, Removing shadows, Pattern Recognition Letters, 26 (2005) 251-265] where a method for detecting and removing shadows has also been introduced. The approach involves training Support Vector Machines to identify shadow boundaries based on their boundary properties. The latter are used to identify shadowed regions in the image and then assign to them the color of non-shadow neighbors of the same material as the shadow. Based on these three steps, we show that more meaningful color image segmentations can be achieved by compensating for illumination using the Illumination Compensation Method proposed in this paper. It is also demonstrated that the accuracy of facial skin detection improves significantly when this illumination compensation approach is used. Finally, we show how illumination compensation can increase the accuracy of face recognition.\",\n",
       " '53e997d7b7602d9701fcb39a': 'We consider a picocell system that implements multiple input multiple output (MIMO) using cell bonding with one antenna per node. We show that this picocell system can initially have 1/2 or fewer access points (APs) with similar voice over IP coverage as a conventional system, yet can gracefully grow to the capacity of conventional systems with the same number of APs. Although the coverage is somewhat lower for MIMO (both 2X2 and 4X4), we show that through the use of dynamic antenna/node selection the picocell system with cell bonding can have similar coverage to conventional systems.',\n",
       " '53e997d7b7602d9701fcb3cb': 'While a number of privacy-enhancing technologies have been proposed over the past quarter century, very little has been done to generalise the notion. Privacy-enhancing technologies have typically been discussed for specific applications (such as confidential and/or anonymous e-mail) or in specific contexts (such as on the Internet). This paper takes cognisance of existing privacy- enhancing technologies, abstracts from them to a more general environment, and structures the technologies in a general architecture, based on the relationships between the technologies. The resulting architecture consists of four layers, viz the personal communications, identity management, organisational safeguards and personal control layers. It is also argued that a strong ordering exists between the layers — in the order just given. The proposed architecture can form the basis of an approach to constructing integrated, com- prehensive privacy solutions.',\n",
       " '53e997d7b7602d9701fcb6c3': 'IEEE 802.11s wireless LAN mesh network technology is the next step in the evolution of wireless architecture. A WLAN mesh\\n network consists of WLAN devices with relay functions that communicate directly. In this type of networks, path selection\\n is based on two protocols: HWMP and RA-OLSR. This paper presents a detailed study of the performance of the proposed path\\n selection algorithms for IEEE 802.11s WLAN mesh networks based on the current draft standard D1.08 from January 2008 under\\n different scenarios to provide conditions of the applicability of the protocols.\\n ',\n",
       " '53e997d7b7602d9701fcb6e7': 'Holonic Manufacturing Systems have emerged over the last seven years as strategy for manufacturing control system design. A new approach called Holonic Component-Based Architecture (HCBA) to establish a manufacturing control system as to cope with rapid changes in manufacturing environment is presented in this paper. Intelligent building blocks in terms of resource and product are proposed to dynamically form a virtual controller via a computer network, and to perform co-operative control execution and diagnosis operations. This concept enables the design, operation and maintenance of the manufacturing controller to be performed in a distributed manner, which can increase the agility and responsiveness of an integrated system. This flexible structure has been implemented in a robot assembly cell to show its plug-and-play capability via an Internet-based infrastructure.',\n",
       " '53e997d7b7602d9701fcb535': 'In this paper, we propose an augmented coupling interface method on a Cartesian grid for solving eigenvalue problems with sign-changed coefficients. The underlying idea of the method is the correct local construction near the interface which incorporates the jump conditions. The method, which is very easy to implement, is based on finite difference discretization. The main ingredients of the proposed method comprise (i) an adaptive-order strategy of using interpolating polynomials of different orders on different sides of interfaces, which avoids the singularity of the local linear system and enables us to handle complex interfaces; (ii) when the interface condition involves the eigenvalue, the original problem is reduced to a quadratic eigenvalue problem by introducing an auxiliary variable and an interfacial operator on the interface; (iii) the auxiliary variable is discretized uniformly on the interface, the rest of variables are discretized on an underlying rectangular grid, and a proper interpolation between these two grids are designed to reduce the number of stencil points. Several examples are tested to show the robustness and accuracy of the schemes.',\n",
       " '53e997d7b7602d9701fcb537': \"Discovery of association rules is an important database mining problem. Mining for association rules involves extracting pat- terns from large databases and inferring useful rules from them. Several parallel and sequential algorithms have been proposed in the literature to solve this problem. Almost all of these algo- rithms make repeated passes over the database to determine the commonly occurring patterns or itemsets (set of items), thus in- curring high I/O overhead. In the parallel case, these algorithms do a reduction at the end of each pass to construct the global patterns, thus incurring high synchronization cost. In this paper we describe a new parallel association min- ing algorithm. Our algorithm is a result of detailed study of the available parallelism and the properties of associations. The algorithm uses a scheme to cluster related frequent itemsets to- gether, and to partition them among the processors. At the same time it also uses a different database layout which clusters related transactions together, and selectively replicates the database so that the portion of the database needed for the computation of associations is local to each processor. After the initial set-up phase, the algorithm eliminates the need for further communi- cation or synchronization. The algorithm further scans the local database partition only three times, thus minimizing I/O over- heads. Unlike previous approaches, the algorithms uses simple intersection operations to compute frequent itemsets and doesn't have to maintain or search complex hash structures. Our experimental testbed is a 32-processor DEC Alpha clus- ter inter-connected by the Memory Channel network. We present results on the performance of our algorithm on various databases, and compare it against a well known parallel algorithm. Our al- gorithm outperforms it by an more than an order of magnitude. provide a host of useful information on customer groups, buying patterns, stock trends, etc. This process of automatic informa- tion inferencing is commonly known as Knowledge Discovery and Data mining (KDD). We look at one aspect of this process — mining for associations. Discovery of association rules is an important problem in database mining. The prototypical appli- cation is the analysis of sales or basket data (2). Basket data consists of items bought by a customer along with the transac- tion identifier. Association rules have been shown to be useful in domains that range from decision support to telecommunications alarm diagnosis, and prediction.\",\n",
       " '53e997d7b7602d9701fcb73c': 'Software services are, just like any other software system, subject to permanent change. We argue that these changes should generally be transparent to service consumers. However, currently consumers are often tied to a given version of a service and have no means of easily upgrading to a newer version. In this paper we propose a WSDL-driven classification of Web service change types and discuss a versioning mechanism for service-oriented systems that considers revision management on registry- and client-side. We use the concepts of service version graphs and selection strategies to provide transparent end-to-end versioning support, and show how this approach is implemented in our service-oriented computing runtime VRESCo. Furthermore, we illustrate the advantages of our approach in comparison to the current state of the art using a realistic case study.',\n",
       " '53e997d7b7602d9701fcb744': 'A massively parallel architecture called the mesh-of-appendixed-trees (MAT) is shown to be suitable for processing artificial neural networks (ANNs). Both the recall and the learning phases of the multilayer feedforward with backpropagation ANN model are considered. The MAT structure is refined to produce two special-purpose array processors; FMAT1 and FMAT2, for efficient ANN computation. This refinement tends to reduce circuit area and increase hardware utilization. FMAT1 is a simple structure suitable for the recall phase. FMAT2 requires little extra hardware but supports learning as well. A major characteristic of the proposed neurocomputers is high performance. It takesO (logN) time to process a neural network withN neurons in its largest layer. Our proposed architecture is shown to provide the best number of connections per unit time when compared to several major techniques in the literature. Another important feature of our approach is its ability to pipeline more than one input pattern which further improves the performance.',\n",
       " '53e997d7b7602d9701fcb88d': \"A new version of the Multi-objective Alliance Algorithm (MOAA) is described. The MOAA's performance is compared with that of NSGA-II using the epsilon and hypervolume indicators to evaluate the results. The benchmark functions chosen for the comparison are from the ZDT and DTLZ families and the main classical multi-objective (MO) problems. The results show that the new MOAA version is able to outperform NSGA-II on almost all the problems.\",\n",
       " '53e997d7b7602d9701fcb7ee': 'This paper considers how different views of real-time program specification and verification arise from different assumptions about the representation of time external to the program, the representation of time in the program and the verification of the timing properties on an implementation. Three different views are compared: real-time programming without time, the synchrony hypothesis and asynchronous real-time programs. Questions about the representation of time are then related to different models of time and their roles at different levels of analysis. The relationship between the development of a program from a specification and its timing characteristics in an implementation is discussed and it is suggested that the formal verification of timing properties can be extended towards the implementation. The need for fault-tolerance in a real-time system is then considered and ways examined of incorporating a formal proof of fault-tolerance along with proof of its timing properties. Keywords: real-time specification, synchronous real-time, asynchronous real-time, fault-tolerance. Presented at the REX Workshop on Real-time Systems: Theory in Practice, Plasmolen, Mook, The Netherlands, June 1991.',\n",
       " '53e997d7b7602d9701fcb848': 'Microsurface scale characteristics (roughness, waviness and form) and the workpiece mounting fixture effects must be accounted and compensated for during laser micromachining such that the focused laser spot position is known in the coordinates of the measured surfaces. Thus, allowing rapid and accurate micromachining on the true workpiece engineering surface. The thin-plate splines (TPSs), a mathematically simple theory, is modified and employed in the reconstruction of 2 1/2 D unfolded continuous and differentiable microtopographical surfaces from a limited set of sampled digital elevation data. The TPS theory aids in restoring bad samples and in enhancing the visualization of the reconstructed surface and the characterization of microelectromechanical systems (MEMS) structures. The reverse engineered surface could also be interfaced and used with a CAD/CAM system to compensate for the focal spot location of a laser beam based on the actual reversed engineered workpiece surface. The practical examples of the real microsurfaces presented in this work, combine comprehensive identification with the ultimate goal of utilizing the algorithms in the compensation of the laser focused spot for a femtosecond laser micromachining (FLM) system currently under development in our laboratory.',\n",
       " '53e997d7b7602d9701fcbd2d': 'Using the Malliavin calculus on Poisson space we compute Greeks in a market driven by a discontinuous process with Poisson jump times and random jump sizes, following a method initiated on the Wiener space in [5]. European options do not satisfy the regularity conditions required in our approach, however we show that Asian options can be considered due to a smoothing effect of the integral over time. Numerical simulations are presented for the Delta and Gamma of Asian options, and confirm the efficiency of this approach over classical finite difference Monte-Carlo approximations of derivatives.',\n",
       " '53e997d7b7602d9701fcbd2e': 'This paper discusses the semi-formal language of mathematics and presents the Naproche CNL, a controlled natural language for mathematical authoring. Proof Representation Structures, an adaptation of Discourse Representation Structures, are used to represent the semantics of texts written in the Naproche CNL. We discuss how the Naproche CNL can be used in formal mathematics, and present our prototypical Naproche system, a computer program for parsing texts in the Naproche CNL and checking the proofs in them for logical correctness.',\n",
       " '53e997d7b7602d9701fcc086': 'The more-for-less (MFL) problem in fuzzy posynomial geometric programming (FPGP) is advanced in this paper. The research results presented here focus primarily on the nonconvex FPGP in both objective functions and constraint functions. Convexification, quasiconvex, or pseudoconvex, is extended in the sense of an MFL paradox by consolidating the necessary and sufficient conditions. Since the FPGP is equivalent to fuzzy linear programming correspondingly, there exists a solution to the FPGP. Furthermore, the duality or strong duality theorem, the equivalent condition of the MFL paradox and its condition under expansion are examined in detail. It is well known that the fundamental understanding of problems on MFL paradox is of paramount importance to applications of resource allotments and optimal resource management, and correspondingly that the information science and technology advancement play a rule to resource allotments and resource option in management problems. In fact, they are dependent and interwinded.',\n",
       " '53e997d7b7602d9701fcc088': 'We consider the problem of managing access privileges on protected objects. We associate one or more locks with each object, one lock for each access right defined by the object type. Possession of an access right on a given object is certified by possession of a key for this object, if this key matches one of the object locks. We introduce a number of variants to this basic key–lock technique. Polymorphic access rights make it possible to decrease the number of keys required to certify possession of complex access privileges that are defined in terms of several access rights. Multiple locks on the same access right allow us to exercise forms of selective revocation of access privileges. A lock conversion function can be used to reduce the number of locks associated with any given object to a single lock. The extent of the results obtained is evaluated in relation to alternative methodologies for access privilege management.',\n",
       " '53e997d7b7602d9701fcc0bc': 'One objective for classifying textures in natural images is to achieve the best performance possible. Unsupervised techniques are suitable when no prior knowledge about the image content is available. The main drawback of unsupervised approaches is its worst performance as compared against supervised ones. We propose a new unsupervised hybrid approach based on two well-tested classifiers: Vector Quantization (VQ) and Fuzzy k-Means (FkM). The VQ unsupervised methods establishes an initial partition which is validated and improved through the supervised FkM. A comparative analysis is carried out against classical classifiers, verifying its performance.',\n",
       " '53e997d7b7602d9701fcbd74': 'This paper deals with the numerical analysis and computing of a nonlinear model of option pricing appearing in illiquid markets with observable parameters for derivatives. A consistent monotone finite difference scheme is proposed and a stability condition on the stepsize discretizations is given.',\n",
       " '53e997d7b7602d9701fcbd83': 'Many organizations providing products with com-mon features wish to take advantage of that similarity in order to reduce development and maintenance efforts. Their goal is to move from a single-system development paradigm towards a product line approach. However, this transition is not trivial and requires a systematic scoping phase to decide how the product line should be defined, i.e. what products and features should be included and thus developed for reuse. Currently available prod-uct line scoping approaches require huge upfront investments in the scoping phase, consuming a lot of time and resources. Our experience has shown that small and medium enterprises require a lightweight approach to decide how the existing products are related to each other so that their potential for reuse can be esti-mated more easily. In this paper we present a conceptual solution and early tool support enabling companies to semi-automatically identify similarity within existing product configurations.',\n",
       " '53e997d7b7602d9701fcbd88': 'Kernel selection is one of the key issues both in recent research and application of kernel methods. This is usually done by minimizing either an estimate of generalization error or some other related performance measure. It is well known that a kernel matrix can be interpreted as an empirical version of a continuous integral operator, and its eigenvalues converge to the eigenvalues of integral operator. In this paper, we introduce new kernel selection criteria based on the eigenvalues perturbation of the integral operator. This perturbation quantifies the difference between the eigenvalues of the kernel matrix and those of the integral operator. We establish the connection between eigenvalues perturbation and generalization error. By minimizing the derived generalization error bounds, we propose the kernel selection criteria. Therefore the kernel chosen by our proposed criteria can guarantee good generalization performance. To compute the values of our criteria, we present a method to obtain the eigenvalues of integral operator via the Fourier transform. Experiments on benchmark datasets demonstrate that our kernel selection criteria are sound and effective.',\n",
       " '53e997d7b7602d9701fcc111': 'We propose an extension of the join calculus with pattern matching on algebraic data types. Our initial motivation is twofold: to provide an intuitive semantics of the interaction between concurrency and pattern matching; to define a practical compilation scheme from extended join definitions into ordinary ones plus ML pattern matching. To assess the correctness of our compilation scheme, we develop a theory of the applied join calculus, a calculus with value passing and value matching. We implement this calculus as an extension of the current JoCaml system.',\n",
       " '53e997d7b7602d9701fcbdb7': 'In this letter, we provide a novel theoretical framework for studying the effects of correlated shadowing, in the number of relays that are capable of helping two nodes (sources) to exchange their messages. The relays use network coding to simultaneously transmit the received messages to the sources. We prove theoretically and verify by means of simulations that the average number of relays that are capable of forwarding the network coded message, is independent from any correlation between the links from one source to the relays. Finally, we apply this framework, to compute the network outage probability. The presented results are essential for the theoretical study of medium access control and relay selection protocols designed for network coded cooperative communications.',\n",
       " '53e997d7b7602d9701fcc12c': \"This study employed the Perl program, Excel software, and some bibliometric techniques to investigate growth pattern, journal characteristics, and author productivity of the subject indexing literature from 1977 to 2000, based on the subject search of a descriptor field in the Library and Information Science Abstracts (LISA) data-base. The literature growth from 1977 to 2000 in subject indexing could be fitted well by the logistic curve. The Bradford plot of journal literature fits the typical Bradford-Zipf S-shaped curve. Twenty core journals making a significant contribution could be identified from the Bradford-Zipf distribution. Four major research topics in the area of subject indexing were identified as: (1) information organization, (2) information processing, (3) information storage and retrieval, and (4) information systems and services. It was also found that a vast majority of authors (76.7%) contributed only one article, which is a much larger percentage than the 60% of original Lotka's data. The 15 most productive authors and the key concepts of their research were identified.\",\n",
       " '53e997d7b7602d9701fcc149': \"By studying the multiple granularities of time constraints' descriptions and consistency problems, in this paper, we establish a dynamic fine-grained access control model, thereby improve the access control precision, quality and efficiency. This paper describes the related concepts of time and the time constraints of multi-granularity. It proposes a kind of TRBAC model with multiple granularities of time constraints by extending the time dimension of the RBAC model. At the same time, it discusses and analyzes the maintenance of the state of the consistency of the system, which enhances the ability of system portray authority in reality and brings it with stronger, more comprehensive description of system security capabilities.\",\n",
       " '53e997d7b7602d9701fcbe22': 'This paper deals with the output feedback sliding mode control for Itô stochastic time-delay systems. The system states are unmeasured, and the uncertainties are unmatched. A sliding mode control scheme is proposed based on the state estimates. By utilizing a novel switching function, the derivative of the switching function is ensured to be finite variation. It is shown that the sliding mode in the estimation space can be attained in finite time. The sufficient condition for the asymptotic stability (in probability) of the overall closed-loop stochastic system is derived. Finally, a simulation example is shown to illustrate the proposed method.',\n",
       " '53e997d7b7602d9701fcbe1d': 'Process mining is an emerging analysis technique, which extracts process knowledge from data and provides various benefits to organizations. In Service Oriented Computing environment, different services collaborate with others to carry out the operations and therefore overall picture of operations and execution is not clear. Process mining extracts the information from log files of systems, as recorded during executions, and depicts the reality. In order to apply process mining, extraction of process trace data from log files is a pre-requisite step. A case study demonstrates the practical applicability of our proposed framework for extraction of the process trace data from application systems and integration portals.',\n",
       " '53e997d7b7602d9701fcc1a5': 'Pipa is a behavioral interface specification language (BISL) tailored to Aspect J, an aspect-oriented programming language. Pipa is a simple and practical extension to the Java Modeling Language (JML), a BISL for Java. Pipa uses the same basic approach as JML to specify Aspect J classes and interfaces, and extends JML, with just a few new constructs, to specify Aspect J aspects. Pipa also supports aspect specification inheritance and crosscutting. This paper discusses the goals and overall approach of Pipa. It also provides several examples of Pipa specifications and discusses how to transform an Aspect J program together with its Pipa specification into a corresponding Java program and JML specification. The goal is to facilitate the use of existing JML-based tools to verify Aspect J programs.',\n",
       " '53e997d7b7602d9701fcc1ab': 'The developers of e-commerce applications have a problem in gauging the knowledge and expertise of end-users. The developer\\n must therefore design the interface to the system so that the use thereof is sufficiently intuitive to require the minimum\\n of background knowledge. Developers need to enhance the usability of their web sites so that incidental users will find the\\n site rewarding and enticing, and encourage them to explore further. This paper will explore the role of feedback as a valuable\\n tool in enhancing the interpretability of e-commerce applications. We discuss firstly how applications may be designed to\\n make use of an enriched model of application feedback, and secondly how developers may evaluate their sites so as to gauge\\n the efficacy of the currently provided feedback. We propose a novel method for analysing the purchasing phase of the e-commerce\\n experience. We then introduce an evaluation method for determining whether a particular site provides adequate feedback or\\n not. Three sites were evaluated using the proposed model, and the results of this evaluation are analysed.\\n ',\n",
       " '53e997d7b7602d9701fcbe1f': 'We present a proposal intended to demonstrate the applicability of tabulation techniques to pattern recognition problems, when dealing with structures sharing some common parts. This work in motivated by the study of information retrieval for textual databases, using pattern matching as a basis for querying data.',\n",
       " '53e997d7b7602d9701fcbe27': 'The importance of performance management in supply chains has long been recognized from a variety of functional disciplines. But much of the work has focused on designing performance measures with less concern for the other stages of the entire performance management process. In this paper, an integrated, efficient and effective performance management system, \"active performance management system\", is presented. The system covers the entire performance management process including measures design, analysis, and dynamic update. The analysis of performance measures using causal loop diagrams, qualitative inference and analytic network process is mainly discussed in this paper. A real world case study is carried out throughout the paper to explain how the framework works. A software tool, supply chain performance analyzer, is also introduced. Some related topics and further research directions are discussed as concluding remarks.',\n",
       " '53e997d7b7602d9701fcbe2d': 'This paper presents a parallel algorithm using GPU for computer simulation of Electrocardiogram (ECG) based on a 3-dimensional (3D) whole-heart model. The computer heart model includes approximately 50,000 discrete elements (model cells) inside a torso model represented by 344 nodal points with 684 triangular meshes. After the excitation propagation is simulated in the heart model, the Poisson question is applied to the volume conductor for computing ECG, which involves a maximum of about 50,000 electric current dipole sources and four boundaries including the torso surface and three surfaces of heart model. The parallel algorithm was designed to solve this problem based on GPU. It accelerates the speed of calculation of ECG to 2.74 times with very low relative error compared with the serial algorithm. This study demonstrates a potential method using GPU for parallel computing in biomedical simulation study.',\n",
       " '53e997d7b7602d9701fcc4e2': 'The rapid development of computer and network technologies has attracted researchers to investigate strategies for and the effects of applying information technologies in learning activities; simultaneously, learning environments have been developed to record the learning portfolios of students seeking web information for problem-solving. Although previous research has demonstrated the benefits of applying information technologies to learning activities, the difficulties in doing so have also been revealed. One of the major difficulties is the lack of a mechanism to assist teachers in evaluating the problem-solving ability of the students, such that constructive suggestions can be given to the students, and tutoring strategies can be improved accordingly. To cope with this problem, in this paper, an auto-scoring mechanism is developed to analyze the various information searching abilities of individual students. Indicators of information searching ability (ISA) are proposed based on the famous Big6 model and are adopted in our auto-scoring mechanism. Moreover, two experiments have been conducted to demonstrate the effectiveness of this innovative approach. The experimental results show high correlation between the scores of the auto-scoring mechanism and the manual scoring. Moreover, the feedbacks from 158 teachers also show that the innovative approach is highly accepted by the teachers.',\n",
       " '53e997d7b7602d9701fcc4f0': 'Images are ubiquitous in biomedical applications from basic research to clinical practice. With the rapid increase in resolution, dimensionality of the images and the need for real-time performance in many applications, computational requirements demand proper exploitation of multicore architectures. Towards this, GPU-specific implementations of image analysis algorithms are particularly promising. In this paper, we investigate the mapping of an enhanced motion estimation algorithm to novel GPU-specific architectures, the resulting challenges and benefits therein. Using a database of three-dimensional image sequences, we show that the mapping leads to substantial performance gains, up to a factor of 60, and can provide near-real-time experience. We also show how architectural peculiarities of these devices can be best exploited in the benefit of algorithms, most specifically for addressing the challenges related to their access patterns and different memory configurations. Finally, we evaluate the performance of the algorithm on three different GPU architectures and perform a comprehensive analysis of the results.',\n",
       " '53e997d7b7602d9701fcc2a8': 'Various information delivery schemes based on store-carry-forward routing with extensions have been studied as part of efforts to support non-real-time communications in sparse or intermittent networks. However, communication service schemes that cover large areas based solely on store-carry-forward technology have previously been considered impractical because the delay time for message delivery increases in proportion to the size of the service area. Therefore, in an effort to extend the communication service area of such networks, while providing controllable performance in a cost-effective manner, the authors previously proposed a Virtual Segment (VS) concept in which global communications are provided through a combination of a store-carry-forward service provided by vehicles traveling on roadways along with broadband wireless/wired network devices connected to the Internet. A prototype system has been developed for proof-of-concept implementation. In the present paper, we report a case study evaluation of the VS concept and introduce a sample RSS distribution application of the prototype system. Following a field experiment to validate the system concept, and to obtain realistic data reflecting the geographical features of the segment, a simulation using information obtained from the field experiment was conducted to evaluate the VS concept from realistic and quantitative viewpoints.',\n",
       " '53e997d7b7602d9701fcc281': 'The hierarchical Bayesian optimization algorithm (hBOA) can solve nearly decomposable and hierarchical problems of bounded difficulty in a robust and scalable manner by building and sampling probabilistic models of promising solutions. This paper analyzes probabilistic models in hBOA on four important classes of test problems: concatenated traps, random additively decomposable problems, hierarchical traps and two-dimensional Ising spin glasses with periodic boundary conditions. We argue that although the probabilistic models in hBOA can encode complex probability distributions, analyzing these models is relatively straightforward and the results of such analyses may provide practitioners with useful information about their problems. The results show that the probabilistic models in hBOA closely correspond to the structure of the underlying optimization problem, the models do not change significantly in consequent iterations of BOA, and creating adequate probabilistic models by hand is not straightforward even with complete knowledge of the optimization problem.',\n",
       " '53e997d7b7602d9701fcc523': 'An educational environment must constantly keep up with ever-changing Information Technologies. Therefore, computer center administrators of universities must overcome the increasingly expensive effort of providing up to date facilities. National Technical University of Athens (NTUA), is a technical university, where students gain hands-on practical experience in computer laboratories as an essential complement to lectures they attend. NTUA\\'s computer laboratories have to be highly flexible and configurable for a variety of applications: from the implementation of a wide range of computer networks, to the incorporation of computer-aided engineering/manufacturing(CAE/CAM) software. Traditional system and network administration practices are usually complicated in terms of configuration and maintenance and generally lack flexibility and scalability. We discuss the architecture of a personalized cloud-based solution for the specific needs of educational institutions. The goals are high utilization of resources and minimization of the administration effort needed to maintain physical laboratories, achieved by the centralized integration of computing, storage, network and application facilities. Such a centralized integration utilizes the high-speed network interconnections of the university campus and takes advantage of the broadband technologies that are globally available outside of it. We introduce NTUA\\'s \"Central Cloud Front\" (CCF) to prove the concept of the proposed architecture.',\n",
       " '53e997d7b7602d9701fcc542': \"Evolutionary multi-objective optimization (EMO) methodologies, suggested in the beginning of Nineties, focussed on the task of finding a set of well-converged and well-distributed set of solutions using evolutionary optimization principles. Of the EMO methodologies, the elitist non-dominated sorting genetic algorithm or NSGA-II, suggested in 2000, is now probably the most popularly used EMO procedure. NSGA-II follows three independent principles -- domination principle, diversity preservation principle and elite preserving principle -- which make NSGA-II a flexible and robust EMO procedure in the sense of solving various multi-objective optimization problems using a common framework. In this paper, we describe NSGA-II through a functional decomposition following the implementation of these three principles and demonstrate how various multi-objective optimization tasks can be achieved by simply modifying one of the three principles. We argue that such a functionally decomposed and modular implementation of NSGA-II is probably the reason for it's popularity and robustness in solving various types of multi-objective optimization problems.\",\n",
       " '53e997d7b7602d9701fcc2bb': 'So far, approaches towards gesture recognition focused mainly on deictic and emblematic gestures. Iconics, viewed as iconic signs in the sense of Peirce, are different from deictics and emblems, for their relation to the referent is based on similarity. In the work reported here, the breakdown of the complex notion of similarity provides the key idea towards a computational model of gesture semantics for iconic gestures. Based on an empirical study, we describe first steps towards a recognition model for shape-related iconic gestures and its implementation in a prototype gesture recognition system. Observations are focused on spatial concepts and their relation to features of iconic gestural expressions. The recognition model is based on a graph-matching method which compares the decomposed geometrical structures of gesture and object.',\n",
       " '53e997d7b7602d9701fcbff2': \"The main purpose of this paper is to investigate the optimal retailer's replenishment decisions for deteriorating items under two levels of trade credit policy to reflect supply chain management situation within the economic production quantity (EPQ) framework. In this paper, it is assumed that the retailer maintains a powerful decision-making right and can obtain the full trade credit offered by the supplier yet retailer just offers the partial trade credit to his/her customers. Under these conditions, the retailer can obtain the most benefits. Then, we model the retailer's inventory system as a cost minimization problem to determine the retailer's optimal replenishment decisions under the supply chain management. Some easy-to-use theorems are developed to efficiently determine the optimal replenishment decisions for the retailer. We deduce some previously published results of other researchers as special cases. Finally, numerical examples are given to illustrate the theorems obtained in this paper. Then, as well as, we obtain a lot of managerial phenomena from numerical examples.\",\n",
       " '53e997d7b7602d9701fcc592': \"Contemporary, high-throughput sequencing efforts have identified a rich source of naturally occurring single nucleotide polymorphisms (SNPs), a subset of which occur in the coding region of genes and result in a change in the encoded amino acid sequence (non-synonymous coding SNPs or 'nsSNPs'). It is hypothesized that a subset of these nsSNPs may underlie common human disease. Testing all these polymorphisms for disease association would be time consuming and expensive. Thus, computational methods have been developed to both prioritize candidate nsSNPs and make sense of their likely molecular physiologic impact.We have developed a method to prioritize nsSNPs and have applied it to the human protein kinase gene family. The results of our analyses provide high quality predictions and outperform available whole genome prediction methods (74% versus 83% prediction accuracy). Our analyses and methods consider both DNA sequence conservation, which most traditional methods are based on, as well unique structural and functional features of kinases. We provide a ranked list of common kinase nsSNPs that have a higher probability of impacting human disease based on our analyses.\",\n",
       " '53e997d7b7602d9701fcbff9': \"•We consider a quantum version of the Winternitz–Hellman related-key attack model.•Main result: this model gives a quantum attackers big advantage.•Any block cipher can be broken via a quantum related key attack.•This includes most block ciphers of practical relevance (AES, DES).•Secret key is found via quantum algorithm for Simon's problem.\",\n",
       " '53e997d7b7602d9701fcc002': 'This paper presents the design and optimization of a nested-Miller compensated, three-stage operational transconductance amplifier (OTA) for use in switched-capacitor (SC) circuits. Existing design methods for three-stage OTAs often lead to sub-optimal solutions because they decouple inter-related metrics like noise and settling performance. In our approach, the problem of finding an optimal design with the best total integrated noise and settling time has been cohesively solved by formulating a nonlinear constrained optimization program. Equality, inequality, and semi-infinite constraints are formed using closed form symbolic expressions obtained by a closed loop analysis of the SC gain stage and the optimization program is solved by using the interior-point algorithm. For the optimization routine, there is no need to interface with a circuit simulator because all significant device parasitics are included in the model. The optimization and modeling steps are general in nature and can be applied to any amplifier or filter topology. Simulation results show that a 90-nm prototype amplifier achieves a dynamic error settling time of 2.5 ns with a total integrated noise of 240 , while consuming 5.2 mW from a 1-V power supply.',\n",
       " '53e997d7b7602d9701fcc003': 'Blind fractionally spaced equalizers reduce intersymbol interference using second-order statistics without the need for training sequences. Methods for finding FIR zero-forcing blind equalizers directly from the observations are described, and adaptive versions are developed. In contrast, most current methods require channel estimation as a first step to estimating the equalizer. The direct methods can be zero-forcing, minimum mean-square error, or even minimum mean square error (MMSE) within the class of zero-forcing equalizers. Performance of the proposed methods and comparisons with existing approaches are shown for a variety of channels, including an empirically measured digital microwave channel',\n",
       " '53e997d7b7602d9701fcc30b': 'We present the first comprehensive approach to integrating cardinality and weight rules into conflict-driven ASP solving. We begin with a uniform, constraint-based characterization of answer sets in terms of nogoods. This provides the semantic underpinnings of our approach in fixing all necessary inferences that must be supported by an appropriate implementation. We then provide key algorithms detailing the salient features needed for implementing weight constraint rules. This involves a sophisticated unfounded set checker as well as an extended propagation algorithm along with the underlying data structures. We implemented our techniques within the ASP solver clasp and demonstrate their effectiveness by an experimental evaluation.',\n",
       " '53e997d7b7602d9701fcc59d': 'In order to facilitate product design and realization processes, presently, research is actively carried out for developing methodologies and technologies of collaborative computer-aided design systems to support design teams geographically dispersed based on the quickly evolving information technologies. In this paper, the developed collaborative systems, methodologies and technologies, which are organized as a horizontal or a hierarchical manner, are reviewed. Meanwhile, a 3D streaming technology, which can effectively transmit visualization information across networks for Web applications, is highlighted and the algorithms behind it are disclosed.',\n",
       " '53e997d7b7602d9701fcc5aa': 'Current twig join algorithms incur high memory costs on queries that involve child-axis nodes. In this paper we provide an analytical explanation for this phenomenon. In a first large-scale study of the space complexity of evaluating XPath queries over indexed XML documents we show the space to depend on three factors: (1) whether the query is a path or a tree; (2) the types of axes occurring in the query and their occurrence pattern; and (3) the mode of query evaluation (filtering, full-fledged, or \"pattern matching\"). Our lower bounds imply that evaluation of a large class of queries that have child-axis nodes indeed requires large space. Our study also reveals that on some queries there is a large gap between the space needed for pattern matching and the space needed for full-fledged evaluation or filtering. This implies that many existing twig join algorithms, which work in the pattern matching mode, incur significant space overhead. We present a new twig join algorithm that avoids this overhead. On certain queries our algorithm is exceedingly more space-efficient than existing algorithms, sometimes bringing the space down from linear in the document size to constant.',\n",
       " '53e997d7b7602d9701fcc02a': 'Recently, federated search in P2P networks has received much attention. Most of the previous work assumed a cooperative environment where each peer can actively participate in information publishing and distributed document indexing. However, little work has addressed the problem of incorporating uncooperative peers, which do not publish their own corpus statistics, into a network. This paper presents a P2P-based federated search framework called PISA which incorporates uncooperative peers as well as the normal ones. In order to address the indexing needs for uncooperative peers, we propose a novel heuristic query-based sampling approach which can obtain high-quality resource descriptions from uncooperative peers at relatively low communication cost. We also propose an effective method called RISE to merge the results returned by uncooperative peers. Our experimental results indicate that PISA can provide quality search results, while utilizing the uncooperative peers at a low cost.',\n",
       " '53e997d7b7602d9701fcc02e': 'We consider the problem of generating a large state-space in a distributed fashion. Unlike previously proposed solutions that partition the set of reachable states according to a hashing function provided by the user, we explore heuristic methods that completely automate the process. The first step is an initial random walk through the state space to initialize a search tree, duplicated in each processor. Then, the reachability graph is built in a distributed way, using the search tree to assign each newly found state to classes assigned to the available processors. Furthermore, we explore two remapping criteria that attempt to balance memory usage or future workload, respectively. We show how the cost of computing the global snapshot required for remapping will scale up for system sizes in the foreseeable future. An extensive set of results is presented to support our conclusions that remapping is extremely beneficial.',\n",
       " '53e997d7b7602d9701fcc030': 'We consider the problem of generating a large state-space in a distributed fashion. Unlike previously proposed solutions that partition the set of reachable states according to a hashing function provided by the user, we explore heuristic methods that completely automate the process. The first step is an initial random walk through the state space to initialize a search tree, duplicated in each processor. Then, the reachability graph is built in a distributed way, using the search tree to assign each newly found state to classes assigned to the available processors. Furthermore, we explore two remapping criteria that attempt to balance memory usage or future workload, respectively. We show how the cost of computing the global snapshot required for remapping will scale up for system sizes in the foreseeable future. An extensive set of results is presented to support our conclusions that remapping is extremely beneficial.',\n",
       " '53e997d7b7602d9701fcc059': 'Over six years, we iterated on the design of a language for describing the functionality of appliances, such as televisions, telephones, VCRs, and copiers. This language has been used to describe more than thirty diverse appliances, and these descriptions have been used to automatically generate both graphical and speech user interfaces on handheld computers, mobile phones, and desktop computers. In this article, we describe the final design of our language and analyze the key design choices that led to this design. Through this analysis, we hope to provide a useful guide for the designers of future user interface description languages.',\n",
       " '53e997d7b7602d9701fcc3cf': \"Color histograms have been widely used successfully in many computer vision and image processing applications. However, they do not include any spatial information. In this paper, we propose a statistical model to integrate both color and spatial information. Our model is based on finite multiple-Bernoulli mixtures. For the estimation of the model's parameters, we use a maximum a posteriori (MAP) approach through deterministic annealing expectation maximization (DAEM). Smoothing priors on the components parameters are introduced to stabilize the estimation. The selection of the number of clusters is based on stochastic complexity. The results show that our model achieves good performance in some image classification problems.\",\n",
       " '53e997d7b7602d9701fcc4a3': 'This paper studies the vector optimization problem of finding weakly efficient points for mappings in a Banach space Y, with respect to the partial order induced by a closed, convex, and pointed cone C⊂Y with a nonempty interior. The proximal method in vector optimization is extended to develop an approximate proximal method for this problem by virtue of the approximate proximal point method for finding a root of a maximal monotone operator. In this approximate proximal method, the subproblems consist of finding weakly efficient points for suitable regularizations of the original mapping. We present both an absolute and a relative version, in which the subproblems are solved only approximately. Weak convergence of the generated sequence to a weak efficient point is established. In addition, we also discuss an extension to Bregman-function-based proximal algorithms for finding weakly efficient points for mappings.',\n",
       " '53e997d7b7602d9701fcc8c5': 'The ubiquity of textual information nowadays reflects its great significance in knowledge discovery. However, effective usage of these textual materials is always hampered by data incompleteness in real-life applications. In this paper, we apply a closest fit approach to attack textual missing values. To evaluate the closeness of texts in this application, we present an order perspective of text similarity and propose a hybrid order-semisensitive measure, M-similarity, to capture the proximity of texts. This measure combines single item matching, maximum sequence matching and potential matching and get a proper balance between usage of sequence information and efficiency. We incorporate M-similarity into two closest fit methods to missing values in textual attributes and evaluate them on data sets of Traditional Chinese Medicine (TCM). Experimental results illustrate the effectiveness of these methods with M-similarity.',\n",
       " '53e997d7b7602d9701fcc8f0': 'Broadcast is a fundamental operation of wireless ad hoc networks (WANETs) and has widely been studied over the past few decades. However, most existing broadcasting strategies assume nonsleeping wireless nodes and thus are not suitable for uncoordinated duty-cycled WANETs, in which each node periodically switches on and off to save energy. In this paper, we study the minimum-transmission broadcast problem in uncoordinated duty-cycled WANETs (MTB-UD problem) and prove its NP-hardness. We show that modifications of existing broadcast approaches can only provide a linear approximation ratio of O(n) (where n is the number of nodes in the network). We propose a novel set-cover-based approximation (SCA) scheme with both centralized and distributed approximation algorithms. The centralized SCA (CSCA) algorithm has a logarithmic approximation ratio of 3(ln ?? + 1) and time complexity of O(n 3) (?? is the maximum degree of the network). The distributed SCA (DSCA) algorithm has a constant approximation ratio of at most 20 while keeping both linear time and message complexities. We have conducted both theoretical analysis and simulations to evaluate the performance of the proposed algorithms. Results show that both the CSCA and DSCA algorithms outperform the modified versions of existing broadcast approaches by at least 50%.',\n",
       " '53e997d7b7602d9701fcc901': 'In this paper, we present an approach to provide different execution environments within Omnivore, our peer-to peer based scheduling system for operating a pool of unused desktop computers as a desktop Grid, and integrating the desktop Grid into a larger Grid environment via the Grid Way meta-scheduler. The proposed approach is based on extending Omnivore to support virtualization technologies. A new plug-in infrastructure allows us to plug in different kinds of execution modules (including virtual machines) and select the most adequate execution environment for a job at run time. Since each job runs in its own virtual machine containing the required operating system, software and data, administrative efforts are reduced, security is improved, ease of use of the resources is enhanced, and utilization of the resources is increased. Experimental results are presented to indicate that there is a small overhead in using the proposed approach, but for long running jobs it is negligible.',\n",
       " '53e997d7b7602d9701fcc909': 'Hidden Markov models (HMMs) have received considerable attention in various communities (e.g, speech recognition, neurology and bioinformatic) since many applications that use HMM have emerged. The goal of this work is to identify efficiently and correctly the model in a given dataset that yields the state sequence with the highest likelihood with respect to the query sequence. We propose SPIRAL, a fast search method for HMM datasets. To reduce the search cost, SPIRAL efficiently prunes a significant number of search candidates by applying successive approximations when estimating likelihood. We perform several experiments to verify the effectiveness of SPIRAL. The results show that SPIRAL is more than 500 times faster than the naive method.',\n",
       " '53e997d7b7602d9701fcc93c': 'The purpose of this study is to release user\\'s feeling of unease and loneliness, which occur in people\\'s mind while heading for the destination, by appropriate encouragement by multiple robots. In our development, we combined the robot technology with \"good old-fashioned guidance know-how\" and employed five concepts: Manual less, Operation free, Device free, corporeality of robot and Advantage of network robot. A comparative experiment was conducted. It proved the effectiveness of robot giving ease and joy to user. At the same time, we have found that the user come to feel uneasy with increasing distance from the robot. Therefore, measures for improving this problem have been studied. Those are the quantification of psychic distance concerning unease and the introduction of mobile robot.',\n",
       " '53e997d7b7602d9701fcc948': 'Web application development frameworks, like the Java Server Pages framework (JSP), provide web applications with essential functions such as maintaining state information across the application and access control. In the fast paced world of web applications, new frameworks are introduced and old ones are updated frequently. A framework is chosen during the initial phases of the project. Hence, changing it to match the new requirements and demands is a cumbersome task. We propose an approach (based on Water Transformations) to migrate web applications between various web development frameworks. This migration process preserves the structure of the code and the location of comments to facilitate future manual maintenance of the migrated code. Consequently, developers can move their applications to the framework that meets their current needs instead of being locked into their initial development framework. We give an example of using our approach to migrate a web application written using the Active Server Pages (ASP) framework to the Netscape Server Pages (NSP) framework.',\n",
       " '53e997d7b7602d9701fcc959': \"Recent supply chain reengineering efforts have focused on integrating firms' production, inventory and replenishment activities with the help of communication networks. While communication networks and supply chain integration facilitate optimization of traditional supply chain functions, they also exacerbate the information security risk: communication networks propagate security breaches from one firm to another, and supply chain integration causes breach on one firm to affect other firms in the supply chain. We study the impact of network security vulnerability and supply chain integration on firms' incentives to invest in information security. We find that even though an increase in either the degree of network vulnerability or the degree of supply chain integration increases the security risk, they have different impacts on firms' incentives to invest in security. If the degree of supply chain integration is low, then an increase in network vulnerability induces firms to reduce, rather than increase, their security investments. A sufficiently high degree of supply chain integration alters the impact of network vulnerability into one in which firms have an incentive to increase their investments when the network vulnerability is higher. Though an increase in the degree of supply integration enhances firms' incentives to invest in security, private provisioning for security always results in a less than socially optimal security level. A liability mechanism that makes the responsible party partially compensate for the other party's loss induces each firm to invest at the socially optimal level. If firms choose the degree of integration, in addition to security investment, then firms may choose a higher degree of integration when they decide individually than when they decide jointly, suggesting an even greater security risk to the supply chain.\",\n",
       " '53e997d7b7602d9701fcc960': \"Issues related to the design of virtual reality systems are discussed in the context of the development effort for a virtual cockpit. Subsystems provide visual imagery, hand and head tracking , control logic, and force feedback. The force feedback subsystem uses robotic positioning to place an assortment of knobs and switches into position to be touched; the user's hand trajectory is extrapolated and the correct type of control is placed just in time to be actuated. Discussion focuses on selecting among alternative system elements and configurations in arriving at an overall systems design.\",\n",
       " '53e997d7b7602d9701fcc989': 'Let c be an edge-colouring of a graphG such that for every vertexv there are at least d >= 2 different colours on edges incident tov. We prove that G contains a properly coloured path of length2d or a properly coloured cycle of length at leastd+1. Moreover, if G does not contain any properly coloured cycle, then there exists a properly coloured path of length 3x2d-1-2.',\n",
       " '53e997d7b7602d9701fcce3f': 'In Nature, living beings improve their adaptation to surrounding environments by means of two main orthogonal processes: evolution and lifetime learning. Within the Artificial Intelligence arena, both mechanisms inspired the development of non-orthodox problem solving tools, namely: Genetic and Evolutionary Algorithms (GEAs) and Artificial Neural Networks (ANNs). In the past, several gradient-based methods have been developed for ANN training, with considerable success. However, in some situations, these may lead to local minima in the error surface. Under this scenario, the combination of evolution and learning techniques may induce better results, desirably reaching global optima. Comparative tests that were carried out with classification and regression tasks, attest this claim.',\n",
       " '53e997d7b7602d9701fcce4d': 'Abstract   The performance of a single regressor/classifier can be improved by combining the outputs of several predictors. This is true provided the combined predictors are accurate and diverse enough, which posses the problem of generating suitable aggregate members in order to have optimal generalization capabilities. We propose here a new method for selecting members of regression/classification ensembles. In particular, using artificial neural networks as learners in a regression context, we show that this method leads to small aggregates with few but very diverse individual networks. The algorithm is favorably tested against other methods recently proposed in the literature, producing equal performance on the standard statistical databases used as benchmarks with ensembles that have 75% less members on average.    Keywords: Neural Networks, Ensemble Learning, Regression, Bias/Variance Decomposition.      1. Introduction   Ensemble techniques have been used recently in regression/classification ,tasks ,with ,considerable success. The motivation for this procedure is based',\n",
       " '53e997d7b7602d9701fccea2': 'There is a long tradition in Computational Vision research regarding Vision as an information processing task which builds up from low level image features to high level reasoning functions. As far as low level image detectors are concerned there is a plethora of techniques found in the literature, although many of them especially designed for particular applications. For natural scenes, where objects and backgrounds change frequently, finding regions of interest which were triggered by a concentration of non-accidental properties can provide a more informative and stable intermediate mechanism for scene description than just low level features. In this paper we propose such a mechanism to detect and rank salient regions in natural and cluttered images. First, a bank of Gabor filters is applied to the image in a variety of directions. The most prominent directions found are then selected as primitive features. Starting from the selected directions with largest magnitudes a resultant is computed by including directional features in the image neighborhood. The process stops when inclusion of other points in the region makes the resultant direction change significantly from the initial one. This resultant is the axis of symmetry of that salient region. A rank is built showing in order the salient regions found in a scene. We report results on natural images showing a promising line of research for scene description and visual attention.',\n",
       " '53e997d7b7602d9701fccf63': 'The real-valued Lambert W-functions considered here are w 0(y) and w 驴驴驴1(y), solutions of we w 驴=驴y, 驴驴1/e驴y驴 $\\\\int_1^\\\\infty [-w_0(-xe^{-x})]^\\\\alpha x^{-\\\\beta}\\\\d x$ , 驴驴驴0, β驴驴驴驴, and $\\\\int_0^1 [-w_{-1}(-x e^{-x})]^\\\\alpha x^{-\\\\beta}\\\\d x$ , 驴驴驴驴驴1, β驴驴驴驴驴β, and explicit formulae involving the trigamma function, if 驴驴=驴β.',\n",
       " '53e997d7b7602d9701fccdd2': \"In order to overcome the problems of high computational complexity and serious matrix singularity for feature extraction using Principal Component Analysis (PCA) and Fisher's Linear Discrinimant Analysis (LDA) in high-dimensional data, sample-space-based feature extraction is presented, which transforms the computation procedure of feature extraction from gene space to sample space by representing the optimal transformation vector with the weighted sum of samples. The technique is used in the implementation of PCA, LDA, Class Preserving Projection (CPP) which is a new method for discriminant feature extraction proposed, and the experimental results on gene expression data demonstrate the effectiveness of the method.\",\n",
       " '53e997d7b7602d9701fcd193': 'Internet worms are increasing every year, and they increasingly threaten the availability and integrity of Internet-based services. Polymorphic worms evade signature-based Intrusion Detection Systems (IDSs) by varying their payload on every infection attempt. In this paper, we propose a system for automated signature generation for Zero-day polymorphic worms. We have designed a novel double-honeynet system, which is able to detect new worms that have not been seen before. The system is based on an efficient algorithm that uses worms binary representation for pattern matching. The system is able to generate accurate signatures for single and multiple worms.',\n",
       " '53e997d7b7602d9701fccf50': 'Volunteer computing is a form of distributed computing in which the general public volunteers processing and storage to scientific research projects. BOINC, a middleware system for volunteer computing, is currently used by about 20 projects, to which 300,000 volunteers and 450,000 computers supply 350 TeraFLOPS of processing power. A BOINC client program runs on the volunteered hosts and manages the execution of applications. Together with a library linked to applications, it implements a runtime system providing process management, graphics control, checkpointing, file access, and other functions. This runtime system must handle widely varying applications, must provide features and properties desired by volunteers, and must work on many platforms. This paper describes the problems in designing a runtime system having these properties, and how these problems are solved in BOINC.',\n",
       " '53e997d7b7602d9701fcd1aa': 'Scarcity of the spectrum resource and mobility of users make Quality-of-Service (QoS) provision a critical issue in wireless multimedia networks. This paper uses neural network as call admission controller to perform call admission decision. A performance measurement is formed as a weighted linear function of new call and handoff call blocking probabilities of each service class. Simulation compares the neural network with complete sharing policy. Simulation results show that neural network has a better performance in terms of average blocking criterion.',\n",
       " '53e997d7b7602d9701fcd1bb': \"Network worms are a clear and growing threat to the security of today's Internet-connected hosts and networks. One of the most common and effective ways to detect worm attacks is to implement a signature-based IDS. An IDS samples suspicious flow in the network with the goal of detecting previously encountered worms. The two significant drawbacks in these approaches are manual signature generation and lack of accurate signatures to detect polymorphic worms. This approach proposes a new Network Signature Generator (NSG), Extended PolyTree that automatically and quickly generates accurate signatures for worms, especially polymorphic worms. It is observed that signatures from worms and their variants are relevant and a tree structure can properly reflect their familial resemblance. Therefore, the signatures extracted from worm samples are organized into a tree structure called Signature Tree. This approach comprises of five phases namely, traffic data collection, SRE signature generation, signature tree generation, signature selection for IDS and worm detection & removal. Based on the suspicious traffic collected, SRE signatures are generated. These signatures are aligned in such a way that they represent their familial resemblance in the form of signature tree. From the generated most specific signatures, few signatures are selected and given to IDS for worm detection. The simulation analysis of this work shows the increase in time consumption to construct the tree and worm detection time. The accuracy in signature generation in this work is better than any existing system.\",\n",
       " '53e997d7b7602d9701fccfcb': 'This paper presents a new key predistribution scheme for sensor networks\\nbased on structured graphs. Structured graphs are advantageous in that they can\\nbe optimized to minimize the parameter of interest. The proposed approach\\nachieves a balance between the number of keys per node, path lengths, network\\ndiameter and the complexity of routing algorithm.',\n",
       " '53e997d7b7602d9701fccfea': \"The energy efficiency of a robotic manipulator is important, particularly when that manipulator is used in conjunction with a mobile robot with limited battery life. In the paper the energy efficiency (in terms of the electrical energy usage) of a spatial three DOF parallel manipulator is compared to a serial manipulator with the same drive motors and a similar workspace. The effects of end-effector position, velocity and acceleration, and static loading due to gravity are examined. Over a range of conditions, the average energy usage of the parallel manipulator was determined to be 26% of the serial manipulator's. This benefit is not due simply to the reduction in moving mass achieved by the parallel design since its moving mass is 70% of the serial manipulator's. Static loading due to gravity was found to roughly double the power usage of both manipulators without significantly affecting their relative energy efficiency.\",\n",
       " '53e997d7b7602d9701fccdc9': 'In this paper, a new algorithm for Support Vector classification is described. It is shown how to use the parametric margin model with non-constant radius. This is useful in many cases, especially when the noise is heteroscedastic, that is, where it depends on x. Moreover, for a priori chosen v, the proposed new SV classification algorithm has advantage of using the parameter 0 les v les 1 on controlling the number of support vectors. To be more precise, v is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Hence, the selection of v is more intuitive. The algorithm is analyzed theoretically and experimentally.',\n",
       " '53e997d7b7602d9701fcd233': \"In this paper, we propose a metaheuristic based on annealing-like restarts to diversify and intensify local searches for solving the vehicle routing problem with time windows (VRPTW). Using the Solomon's benchmark instances for the problem, our method obtained 7 new best results and equaled 19 other best results. Extensive comparisons indicate that our method is comparable to the best published literatures.\",\n",
       " '53e997d7b7602d9701fcd1c2': 'In general, two quadric surfaces intersect in a nonsingular quartic space curve. Under special circumstances, however, this intersection may “degenerate” into a quartic with a double point, or a composite of lines, conics, and twisted cubics whose degrees, counted over the complex projective domain, sum to four. Such degenerate forms are important since they occur with surprising frequency in practice and, unlike the generic case, they admit rational parameterizations. Invoking concepts from classical algebraic geometry, we formulate the condition for a degenerate intersection in terms of the vanishing of a polynomial expression in the quadric coefficients. When this is satisfied, we apply a multivariate polynomial factorization algorithm to the projecting cone of the intersection curve. Factors of this cone which correspond to intersection components “at infinity” may be removed a priori. A careful examination of the remaining cone factors then facilitates the identification and parameterization of the various real, affine intersection elements that may arise: isolated points, lines, conics, cubics, and singular quartics. The procedure is essentially automatic (avoiding the tedium of case-by-case analyses), encompasses the full range of quadric forms, and is amenable to implementation in exact (symbolic) arithmetic.',\n",
       " '53e997d7b7602d9701fcd1c8': 'This paper presents stylized models for conducting performance analysis of the manufacturing supply chain network (SCN) in\\n a stochastic setting for batch ordering. We use queueing models to capture the behavior of SCN. The analysis is clubbed with\\n an inventory optimization model, which can be used for designing inventory policies . In the first case, we model one manufacturer\\n with one warehouse, which supplies to various retailers. We determine the optimal inventory level at the warehouse that minimizes\\n total expected cost of carrying inventory, back order cost associated with serving orders in the backlog queue, and ordering\\n cost. In the second model we impose service level constraint in terms of fill rate (probability an order is filled from stock\\n at warehouse), assuming that customers do not balk from the system. We present several numerical examples to illustrate the\\n model and to illustrate its various features. In the third case, we extend the model to a three-echelon inventory model which\\n explicitly considers the logistics process.',\n",
       " '53e997d7b7602d9701fcd334': 'We present TimeTilt, a sensor-based technique that allows multiple windows switching on mobile devices, and which overcomes the limitations of mobile devices, i.e. their impoverished input bandwidth (often no keyboard, a small tactile screen and the drawbacks of one-handed interaction). TimeTilt, which is based on a lenticular metaphor, aims at both reducing the activation time when switching between views, and supporting a natural mapping between the gestures and the navigation. We draw a brief classification of sensor-based gestures that could be used in mobile conditions, and we present an experiment.',\n",
       " '53e997d7b7602d9701fcd324': \"Contrary to popular belief, biologists discovered that worker ants are really not all hardworking. It has been found that in three separate 30-strong colonies of black Japanese ants (Myrmecina nipponica), about 20% of worker ants are diligent, 60% are ordinary, and 20% are lazy. That is called 20:60:20 rule. Though they are lazy, biologists suggested that lazy worker ants could be contributing something to the colony that is yet to be determined. In our last research, we used CHC (cross generational elitist selection, heterogeneous recombination, and cataclysmic mutation) with the worker ants' rule (WACHC) aiming at solving optimization problems in changing environments. CHC is a nontraditional genetic algorithm (GA) which combines a conservative selection strategy that always preserves the best individuals found so far with a radical (highly disruptive) recombination operator. In our last research, we verified that WACHC performs better than CHC in only one case of fully changing environment. In this paper, we further discuss our proposed WACHC dealing with changing environment problems with varying degree of difficulty, compare our proposal with hypermutation GA which is also proposed for dealing with changing environment problems, and discuss the difference between our proposal and ant colony optimization algorithms.\",\n",
       " '53e997d7b7602d9701fcd3ca': 'The authors present a structured codebook for reducing the complexity and memory of a CELP stochastic codebook search. The tree-structured delta codebook, whose code vectors are generated from a small number of delta vectors, can be searched efficiently by calculating the vector correlations recursively. The complexity was reduced to 1/70 that of a conventional full-Gaussian codebook, and the memory for codebook storage was reduced to 1/100. Another excellent feature of this codebook is the feasibility of changing the distribution of code vectors adaptively. The codebook adaptation method (delta vector sorting) provides an SNRseg improvement of 0.6 dB, and consistent improvement in perceptual quality, by changing the order of delta vectors to fit the input speech',\n",
       " '53e997d7b7602d9701fcd3d3': 'This letter introduces a new rate control method devised to provide quality scalability to JPEG2000 codestreams containing a single or few quality layers. It is based on a Reverse subband scanning Order and a coding passes Concatenation (ROC) that does not use distortion measures based on the original image. The proposed ROC method allows a flexible rate control when the image has already been enc...',\n",
       " '53e997d7b7602d9701fcd3f1': 'Most machine learning algorithms are eager methods in the sense that a model is generated with the complete training data\\n set and, afterwards, this model is used to generalize the new test instances. In this work we study the performance of different\\n machine learning algorithms when they are learned using a lazy approach. The idea is to build a classification model once\\n the test instance is received and this model will only learn a selection of training patterns, the most relevant for the test\\n instance. The method presented here incorporates a dynamic selection of training patterns using a weighting function. The\\n lazy approach is applied to machine learning algorithms based on different paradigms and is validated in different classification\\n domains.\\n ',\n",
       " '53e997d7b7602d9701fcd263': 'This paper reports on an approach to model generalized implicatures using nonmonotonic logics. The approach, called compositional, is based on the idea of compositional semantics, where the implicatures carried by a sentence are constructed from the implicatures carried by its constituents, but it also includes some aspects nonmonotonic logics in order to model the defeasibility of generalized implicatures.',\n",
       " '53e997d7b7602d9701fccfe4': 'For certain problems of casual modeling in mar- keting, the information is obtained by means of questionnaires. When these questionnaires include more than one item for each observable variable, the value of this variable can not be assigned a number, but a potentially scattered set of values. In this paper, we propose to represent the information contained in this set of values by means of a fuzzy number. A novel fuzzy statistics-based interpretation of the semantic of a fuzzy set will be used for this purpose, as we will consider that this fuzzy number is a nested family of confidence intervals for a central tendency measure of the value of the variable. A genetic learning algorithm, able to extract association fuzzy rules from this data, is also proposed. The accuracy of the model will be expressed by means of a fuzzy-valued function. We propose to jointly minimize this function and the complexity of the rule based model with multicriteria genetic algorithms, that in turn will depend on a fuzzy ranking-based ordering of individuals.',\n",
       " '53e997d7b7602d9701fcd2a0': 'In this paper, a digital planar curve approximation method based on a multi-objective genetic algorithm is proposed. In this method, the optimization/exploration algorithm locates breakpoints on the digital curve by minimizing simultaneously the number of breakpoints and the approximation error. Using such an approach, the algorithm proposes a set of solutions at its end. The user may choose his own solution according to its objective. The proposed approach is evaluated on curves issued from the literature and compared successfully with many classical approaches.',\n",
       " '53e997d7b7602d9701fcd0ff': 'Recently, the problem of extending an alignment with k-mismatches and a single gap for pairwise sequence alignment was introduced (Flouri et al., 2011). The authors considered the problem of extending an alignment under the Hamming distance model by also allowing the insertion of a single gap; and presented a @Q(m@b)-time algorithm to solve it, where m is the length of the shortest sequence to be extended, and @b is the maximum allowed length of the single gap. Very recently, it was shown (Flouri et al., 2012) that this problem is strongly and directly motivated by the next-generation re-sequencing application: aligning tens of millions of short DNA sequences against a reference genome. In this article, we consider an extension of this problem: extending an alignment with k-mismatches and two gaps; and present a @Q(m@b)-time algorithm to solve it. This extension is proved to be fundamental in the next-generation re-sequencing application (Alachiotis et al., 2012). In addition, we present a generalisation of our solution to solve the problem of extending an alignment with k-mismatches and @?-gaps in time @Q(m@b@?). The presented solutions work provided that all gaps in the alignment must occur in one of the two sequences.',\n",
       " '53e997d7b7602d9701fcd6f2': 'Abstract Itis one of the key problems for web based decision support systems  to  generate ,knowledge  from  huge  database containing  inconsistent  information.  In  this  paper, a  learning  algorithm  for  multiple  rule  trees  (MRT) is developed, which is based on ID3 algorithm and,rough  set  theory.  MRT  algorithm  can  quickly  generate decision  rules  from  inconsistent  decision  information tables.  Both  space  and  time complexities  ofMRT algorithm  are  just  polynomial,  while those  of  Skowron’s default  decision  rule  generation  algorithm  are exponential.  With  the  increasing  of  the  number  of  records and  core  attributes  of  an  information  table,  Skowron’s default  algorithm  needs  more  memory ,and  time for generating rules than MRT algorithm. In some cases, Skowron’s default  decision  rule  generation  algorithm could  not  generate  rules  due  to  the  lack  of  memory. It’s  proved  by  our ,simulation  experiment  results that MRT algorithm is effective and valid.',\n",
       " '53e997d7b7602d9701fcd116': 'Purpose - This paper aims to discuss a new tool for requirements gathering in the Web 2.0 era. It seeks to investigate the features that this kind of tool should have in order to be as widely applicable and useful as possible. Further, it aims to explore the extent to which business requirements for enterprise resource planning (ERP) systems can be collected and discussed collaboratively in a worldwide community of business process experts. Design/methodology/approach - The paper is a combination of empirical research, hermeneutics and design research. Findings - The proposed Living Requirements Space (LRS) platform has the potential of becoming an international forum for collecting and discussing business requirements for ERP systems. Practical implications - The LRS platform will allow ERP developers, ERP systems implementers, and academics to better understand the evolution of business requirements for ERP systems. It will create a knowledge base of ERP business requirements, that is, a repository that guarantees open and unrestricted access to content. It will thus allow for more international ERP systems and far more comprehensive education on and understanding of business processes and ERP systems. Originality/value - LRS is an open access tool that allows for the gathering of ERP systems requirements in a vendor-and project-independent approach that is unbiased towards any geographic region.',\n",
       " '53e997d7b7602d9701fcd34f': 'We consider the design of DSM consitency protocolsfor hierachical architectures.Such architectures typicallyconsist of a constellation of loosely-interconnected clusters,each cluster consisting of a set of tightly-interconnectednodes running multithreaded programs.We claim that highperformance can only be reached by taking into accountthis interconnection hierachy at the very core of the protocol design.Previous work has focused on improving localityin data management by caching remote data withinclusters.In contrast, our idea is to improve locality in thesynchronization management.We demonstrate the feasibilitythrough an experimental implementation of this ideain a home-based protocol for Release Consistency, and weprovide a preliminary evaluation of the expectable performance gain.',\n",
       " '53e997d7b7602d9701fcd13d': 'Because of rapid channel attenuation in lossy environment, there is a severe packet drop loss problem in wireless ad hoc networks. Though MIMO (Multi-Input Multi-Output) technology can alleviate this effect greatly, it will result in higher transmission energy depletion as using more transmit antennas. Lots of schemes have been proposed to reduce such energy cost by transmit antenna selection. However, most of those only consider QoS requirements between neighbor nodes while ignoring QoS requirements for entire network. In this paper, we investigate an energy-efficient transmit antenna selection problem for many-to-one communications, while guaranteeing a certain packet drop loss ratio from any node to the sink. We prove that this problem (Transmit Antenna Selection Problem, TASP) is NP-hard. Thus, a greedy algorithm TASA (Transmit Antenna Selection Algorithm) is proposed to solve this problem. This paper also prove the correctness of the proposed algorithm and the time complexity of TASA algorithm is O(|E|+|V|-MT), where |V| represents the number of nodes, |E| indicates the number of links and MT is the number of transmit antennas on each node, respectively. Our numerical results show that in a 100 node network, TASA can reduce transmission energy cost by 44.3% on average.',\n",
       " '53e997d7b7602d9701fcd2d9': \"Since 1981, when Lamport introduced the remote user authentication scheme\\nusing table, a plenty of schemes had been proposed with tables or without table\\nusing. Recently Das et al. proposed a dynamic id-based remote user\\nauthentication scheme. They claimed that their scheme is secure against\\nID-theft, and can resist the reply attacks, forgery attacks, insider attacks an\\nso on. In this paper we show that Das et al's scheme is completly insecure and\\nusing of this scheme is like an open server access without password.\",\n",
       " '53e997d7b7602d9701fcd2da': 'Wireless embedded systems, especially life-critical body-area networks, need security in order to prevent unauthorized and malicious users from injecting traffic and accessing confidential data. Coupled with the security costs in system performance and power consumption, embedded systems are also restricted by the type of security that can fit in their limited memory. To address these issues, we introduce a Dynamic Security System (DYNASEC), a novel architecture that ensures message integrity and confidentiality in wireless embedded systems. A delay-aware heuristic attempts to maximize security levels of different nodes throughout the system while ensuring that timing constraints are met. This experimental analysis on a reconfigurable electrocardiogram (ECG) application validates the efficacy of the DYNASEC architecture in a body area network. Our experiments demonstrate that DYNASEC enables lightweight medical embedded systems to dynamically optimize security levels to meet timing constraints in a body sensor network.',\n",
       " '53e997d7b7602d9701fcd2dc': 'Model checking has been successfully applied to system verification. However, there are no standard and universal tools to date being applied for system modification. This paper introduces a formal approach called the Linear Temporal Logic (LTL) model update for system modification. In contrast to previous error repairing methods, which were usually simple program debugging and specialized technical methods, our LTL model update modifies the existing LTL model of an abstracted system to correct automatically the errors occurring within this model. We introduce three single operations to represent, update, and simplify the updating problem. The minimal change rules are then defined based on such update operations. We show how our approach can eventually be applied in system modifications by illustrating an example of program corrections and characterizing some frequently used properties in the LTL Kripke model.',\n",
       " '53e997d7b7602d9701fcd39b': 'Social networks are emerging as the arteries for information flow on the web. In a previous paper, we introduced a new community-centric approach for information flow control for the social web. This paper introduces two key improvements to the previously introduced mechanisms. The first improvement is the ability to model user heterogeneity with regard to information relaying on the social web. The second improvement is the ability to reduce information flow to a specific user (i.e.,block information flowing to a specific user). We evaluate the algorithmic ideas using traces of interactions obtained from Facebook and Flickr. Our evaluations indicate that the algorithmic ideas developed by us are useful in controlling the information flow in the social web.',\n",
       " '53e997d7b7602d9701fcd609': 'Multicampaign assignment problem is a campaign model to overcome the multiple recommendation problem which occurs when conducting several personalized campaigns simultaneously. In this paper, we propose a Lagrangian method for the problem. The original problem space is transformed to another simpler one by introducing Lagrange multipliers which relax the constraints of the multicampaign assignment problem. When the Lagrangian vector is supplied, we can compute the optimal solution under this new environment in O(NK^2) time, where N and K are the numbers of customers and campaigns, respectively. This is a linear-time method when the number of campaigns is a constant. However, it is not easy to find a Lagrangian vector in exact accord with given problem constraints. We thus combine the Lagrangian method with a genetic algorithm to find good feasible solutions. We verify the effectiveness of our evolutionary Lagrangian approach in both theoretical and experimental views of points. The suggested Lagrangian approach is practically attractive for large-scale real-world problems.',\n",
       " '53e997d7b7602d9701fcd699': 'The usual way to design a simulation of a given phenomenon is to first build a model and then to implement it. The study of the simulation and its outcomes tells if the model is adequate and can explain the phenomenon. In this paper, we reverse this process by building a browser in simulations space: we study an automatically built simulation to understand its underlying model and explain the phenomenon we obtain. This paper deals with automated construction of models and their implementations from an ontology, consisting of generic interactions that can be assigned to families of agents. Thanks to the measurement tools that we define, we can automatically qualify characteristics of our simulations and their underlying models. Finally, we offer tools for processing and simplifying found or existing models: these allow an iterative construction of new models by involving the user in their assessment. This simulation space browser is called LEIA for \"LEIA lets you Explore Interactions for your Agents\".',\n",
       " '53e997d7b7602d9701fcd6a7': 'In this paper, we propose a practical online design rule checking system, which can provide the following features: an ability to cope with a complicated and wide variety of design rules with high accuracy, easy use, high speed incremental DRC (simultaneous checking with pattern editing) and total DRC (non-simultaneous checking), high speed pattern editing, and a small memory space. For the last three items, a field block data structure is developed. Experimental results show this system is very effective for VLSI cell layout design.',\n",
       " '53e997d7b7602d9701fcd5e2': 'Previous literature reviews or meta-analysis based studies on game-assisted learning have provided important results, but few studies have considered the importance of learning theory, and coverage of papers after 2007 is scant. This study presents a systematic review of the literature using a meta-analysis approach to provide a more comprehensive analysis and synthesis of relevant studies based on four orientations of learning theories and principles: behaviorism, cognitivism, humanism, and constructivism. Major findings of this study include that the majority of published studies were not based on learning theory and the development of learning theory orientations has prompted more studies to focus on constructivism and humanism than on behaviorism and cognitivism. In addition, most studies adopted a descriptive approach, followed by experimental methods and surveys, and most presented positive outcomes. These findings not only advance understanding of game-assisted learning from the important perspective of learning theory, but also provide useful insights for researchers and educators in issues related to game-assisted learning.',\n",
       " '53e997d7b7602d9701fcd5ea': 'We present a new and comprehensive approach to inductive databases in the relational model. The main contribution is a new inductive query language extending SQL, with the goal of supporting the whole knowledge discovery process, from pre-processing via data mining to post-processing. A prototype system supporting the query language was developed in the SINDBAD (structured inductive database development) project. Setting aside models and focusing on distance-based and instance-based methods, closure can easily be achieved. An example scenario from the area of gene expression data analysis demonstrates the power and simplicity of the concept. We hope that this preliminary work will help to bring the fundamental issues, such as the integration of various pattern domains and data mining techniques, to the attention of the inductive database community.',\n",
       " '53e997d7b7602d9701fcd895': 'As a promising method for pattern recognition and function estimation, least squares support vector machines (LS-SVM) express the training in terms of solving a linear system instead of a quadratic programming problem as for conventional support vector machines (SVM). In this paper, by using the information provided by the equality constraint, we transform the minimization problem with a single equality constraint in LS-SVM into an unconstrained minimization problem, then propose reduced formulations for LS-SVM. By introducing this transformation, the times of using conjugate gradient (CG) method, which is a greatly time-consuming step in obtaining the numerical solution, are reduced to one instead of two as proposed by Suykens et al. (1999). The comparison on computational speed of our method with the CG method proposed by Suykens et al. and the first order and second order SMO methods on several benchmark data sets shows a reduction of training time by up to 44%.',\n",
       " '53e997d7b7602d9701fcd8d2': 'In this paper we present an approach to provide efficient low-complexity video encoding for wireless sensor networks. We present a method based on removing the most time-consuming task, that is motion estimation, from the encoder. Instead the decoder will perform motion prediction based on the available decoded frame and send the predicted motion vectors to the encoder. We present results based on a modified H.264 implementation. Our results shows that this approach can provide rather good coding efficiency even for relatively high network delays.',\n",
       " '53e997d7b7602d9701fcd5f7': 'In this paper we propose an efficient and incremental plan recognition method for cognitive assistance. We design our unique method based on graph matching and heuristic chaining rules in order to deal with interleaved and sequential activities. The finding of this research work is to be applied to predict abnormal behavior of the users, and optimize assistance for them. We have studied a use case of kitchen environment during lunch time that we will discuss in this paper and targeted Dementia patients. We will present implementation details as well as our evaluation plan.',\n",
       " '53e997d7b7602d9701fcd8d3': 'Traffic volume anomalies refer to apparent abrupt changes in time series of traffic volume, which can be propagate through the network. Detecting and tracing anomalies is a critical and difficult task for network operators. In this paper, we first propose a traffic decomposition method, which decomposes the traffic into three components: trend component, autoregressive (AR) component, and noise component. A traffic volume anomaly is detected when the AR component is out of prediction band for multiple links simultaneously. Then, the anomaly is traced using the projection of the detection result matrices for the observed links which are selected by a shortest-path-first algorithm. Finally we validate our detection and tracing method by using traffic data of the third-generation Science Information Network (SINET3) and show the detected and traced results.',\n",
       " '53e997d7b7602d9701fcd79b': ' A block image labeling method is presented. It doesnot assume that the blocks to be treated are alreadysegmented nor that they contain homogeneous data.It is based on connected component analysis to labelthe blocks\" contents as small letter text, medium lettertext, large letter text, graphics or photographs, givingthe percentage of each of these components with respectto the surface area it occupies. It uses a recursive algorithmthat allows one to improve on the result of segmentation.... ',\n",
       " '53e997d7b7602d9701fcd7be': 'Some comments are offered on the number of I/O terminals p of the Q-universal logic modules (ULM) [2]. An equation is given in this note which will determine minimum number of I/O terminals for all n.',\n",
       " '53e997d7b7602d9701fcd7f2': 'The performance of neural nets can be improved through the use of ensembles of redundant nets. In this paper, some of the available methods of ensemble creation are reviewed and the \"test and select\" methodolology for ensemble creation is considered. This approach involves testing potential ensemble combinations on a validation set, and selecting the best performing ensemble on this basis, which is then tested on a final test set. The application of this methodology, and of ensembles in general, is explored further in two case studies. The first case study is of fault diagnosis in a diesel engine, and relies on ensembles of nets trained from three different data sources. The second case study is of robot localisation, using an evidence-shifting method based on the output of trained SOMs. In both studies, improved results are obtained as a result of combining nets to form ensembles.',\n",
       " '53e997d7b7602d9701fcd9d3': 'This paper discusses a high efficient scheme for the Steklov eigenvalue problem. A two-grid discretization scheme of nonconforming Crouzeix–Raviart element is established. With this scheme, the solution of a Steklov eigenvalue problem on a fine grid πh is reduced to the solution of the eigenvalue problem on a much coarser grid πH and the solution of a linear algebraic system on the fine grid πh. By using spectral approximation theory and Nitsche–Lascaux–Lesaint technique in space H-12(∂Ω), we prove that the resulting solution obtained by our scheme can maintain an asymptotically optimal accuracy by taking H=h. And the numerical experiments indicate that when the eigenvalues λk,h of nonconforming Crouzeix–Raviart element approximate the exact eigenvalues from below, the approximate eigenvalues λk,h∗ obtained by the two-grid discretization scheme also approximate the exact ones from below, and the accuracy of λk,h∗ is higher than that of λk,h.',\n",
       " '53e997d7b7602d9701fcdab7': \"Latency-insensitive design is the foundation of a correct-by-construction methodology for SOC design. This approach can handle latency's increasing impact on deep-submicron technologies and facilitate the reuse of intellectual-property cores for building complex systems on chips, reducing the number of costly iterations in the design process.\",\n",
       " '53e997d7b7602d9701fcdb04': 'This paper considers the problem of optimal server allocation in a time-slotted system with N statistically symmetric queues and K servers when the arrivals and channels are stochastic and time-varying. In this setting, we identify two classes of \"desirable\" policies with potentially competing goals of maximizing instantaneous throughput versus balancing the load. Via an example, we show that these goals, in general, can be incompatible, implying an empty intersection between the two classes of polices. On the other hand, we establish the existence of a policy achieving both goals when the connectivities between each queue and each server are random and either \"ON\" or \"OFF.\" We use dynamic programming (DP) and properties of the value function to establish the delay optimality of a policy, which at each time-slot, simultaneously maximizes the instantaneous throughput and balances the queues.',\n",
       " '53e997d7b7602d9701fcdb19': \"Predicting MOSFET models plays a pivotal role in circuit design and its optimization. Independent Gate FinFETs (IGFinFET) are interesting for designers as they are more flexible than Common Multi-Gate FinFETs (CMGFinFET) in digital circuit design. In this work, we implement a model for symmetrical IGFinFET using CMGFinFET model based on Multi-Gate Predictive Technology Model (PTM-MG). This model has been developed from TCAD IGFinFET, based on previously published experimental results of CMG-FinFET. Different basic gates in SG (shorted gate), LP (low power), IG (low area), and IG/LP modes have been designed using the implemented model. For LP, IG, and IG/LP NAND gates, the leakage power is reduced by 89%, 26%, and 67%, respectively in comparison to SG. To show that our model does not have any convergence problem for large circuits, we used ISCAS'85 benchmark suite. The results show that for independent gate in high performance PTM-MG library, on average we can save up to 24% in the number of transistors and lower the total power by 42%.\",\n",
       " '53e997d7b7602d9701fcdb1f': 'The automated extraction of roads from aerial imagery can be of value for tasks including mapping, surveillance and change detection. Unfortunately, there are no public databases or standard evaluation protocols for evaluating these techniques. Many techniques are further hindered by a reliance on manual initialisation, making large scale application of the techniques impractical. In this paper, we present a public database and evaluation protocol for the evaluation of road extraction algorithms, and propose an improved automatic seed finding technique to initialise road extraction, based on a combination of geometric and colour features.',\n",
       " '53e997d7b7602d9701fcdb34': 'The rapid development of mobile and wireless communication technologies requires that wireless devices should support voice, video, and data communications and should flexibly access IP based core networks. This paper proposes a new wireless module architecture for the next generation wireless networks and discusses its access methods based on access bridge point. In addition, the protocols for accessing IP based core networks are discussed for the proposed access network architecture.',\n",
       " '53e997d7b7602d9701fcdb3a': \"Conclusion\\xa0\\xa0The objective of this work is not to replicate subjective impressions and certainly not to supplant them, but to explore means\\n by which the second dimension of literary impact, qualities of emotional expression, can be objectively studied through the\\n collection and display of measures made possible by the computer. With that goal, this paper has illustrated two approaches\\n to the analysis and display of three fundamental emotional tone scores. The first is the production of a combined score, tension,\\n which has been derived from previous studies of literary text and criterion passages. The second approach is the generation\\n of transition graphs which identify the emotional state of passages of text according to the categories proposed in Mehrabian's\\n theoretical system.\\n \\n Both of these approaches to the modeling of emotional tone scores generate meaningful displays of data which can be used in\\n objective comparisons of different stories and which lead to fresh interpretations of the reasons for their impact on a reader.\\n They can be applied to actual samples of the kind of literature that is spontaneously read for pleasure in addition to being\\n of interest for analytic purposes.\\n \\n \\n \",\n",
       " '53e997d7b7602d9701fcdb5b': 'We present an algorithm for the efficient and accu- rate computation of geodesic distance fields on tri- angle meshes. We generalize the algorithm orig- inally proposed by Surazhsky et al. (1). While the original algorithm is able to compute geodesic distances to isolated points on the mesh only, our generalization can handle arbitrary, possibly open, polygons on the mesh to define the zero set of the distance field. Our extensions integrate naturally into the base algorithm and consequently maintain all its nice properties. For most geometry processing algorithms, the exact geodesic distance information is sampled at the mesh vertices and the resulting piecewise lin- ear interpolant is used as an approximation to the true distance field. The quality of this approxima- tion strongly depends on the structure of the mesh and the location of the medial axis of the distance field. Hence our second contribution is a simple adaptive refinement scheme, which inserts new ver- tices at critical locations on the mesh such that the final piecewise linear interpolant is guaranteed to be a faithful approximation to the true geodesic dis- tance field.',\n",
       " '53e997d7b7602d9701fce815': 'Tabletop and tangible interfaces are often described in terms of their support for shared access to digital resources. However, it is not always the case that collaborators want to share and help one another. In this paper we detail a video-analysis of a series of prototyping sessions with children who used both cardboard objects and an interactive tabletop surface. We show how the material qualities of the digital interface and physical objects affect the kinds of bodily strategies adopted by children to stop others from accessing them. We discuss how children fight for and maintain control of physical versus digital objects in terms of embodied interaction and what this means when designing collaborative applications for shareable interfaces.',\n",
       " '53e997d7b7602d9701fce81a': \"Interoperability is a fundamental concern in many areas of software engineering, such as software reuse or infrastructures for software development environments. Of particular interest to software engineers are the interoperability problems arising in polylingual software systems. The defining characteristic of polylingual systems is their focus on uniform interaction among a set of components written in two or more different languages.Existing approaches to support for interoperability are inadequate because they lack seamlessness: that is, they generally force software developers to compensate explicitly for the existence of multiple languages or the crossing of language boundaries. In this paper we first discuss some foundations for polylingual interoperability, then review and assess existing approaches. We then outline PolySPIN, an approach in which interoperability can be made transparent and existing systems can be made to interoperate with no visible modifications. We also describe PolySPINner, our prototype implementation of a toolset providing automated support for PolySPIN. We illustrate the advantages of our approach by applying it to an example problem and comparing PolySPIN's ease of use with that of an alternative, CORBA-style approach.\",\n",
       " '53e997d7b7602d9701fce844': 'Purpose - This paper aims to address the problem of enhancing the selection of titles offered by a digital library, by analysing the differences in these titles when they are cited by local authors in their publications and when they are listed in the digital library offer. Design/methodology/approach - Text mining techniques were used to identify duplicate references. Moreover, the process of identifying syntactically different data was improved with the automated discovery of thesauri from correctly matched data, and the generated thesaurus was further used in semantic clustering. The results were effectively visually represented. Findings - The paper finds that the function based on the Jaro-Winkler algorithm may be efficiently used in the de-duplication process. A generated thesaurus that utilises domain-specific knowledge can also be used in the semantic clustering of references. It was shown that semantic clustering may be most useful in partitioning data, which is particularly significant when dealing with large amounts of data, which is usually the case. Moreover, those references that have the same or similar scores may be considered as candidate matches in the further de-duplication process. Finally, it proved to be a more efficient way of visually representing the results. Originality/value - This function can be implemented to enhance the selection of titles to be offered by a digital library, in terms of making that offer more compliant with what the library users frequently cite.',\n",
       " '53e997d7b7602d9701fce88f': 'The paper deals with the application of the Artificial Immune System to the optimization and identification of composites. To reduce the computational time parallel computations are performed. Composite structures in form of multilayered laminates are taken into account. Simple and hybrid (with laminas made of different materials) laminates are examined. Different optimization criteria connected with stiffness and modal properties of laminate structures are considered. Continuous and discrete variants of design variables are regarded. The aim of the identification is to find laminate elastic constants on the basis of measurements of state variable values. The Finite Element Method is employed to solve the boundary-value problem for laminates. Numerical examples presenting effectiveness of proposed method are attached.',\n",
       " '53e997d7b7602d9701fce8b1': 'We describe a novel two-parameter continuation method combined with a spectral-collocation method (SCM) for computing the ground state and excited-state solutions of spin-1 Bose-Einstein condensates (BEC), where the second kind Chebyshev polynomials are used as the basis functions for the trial function space. To compute the ground state solution of spin-1 BEC, we implement the single parameter continuation algorithm with the chemical potential µ as the continuation parameter, and trace the first solution branch of the Gross-Pitaevskii equations (GPEs). When the curve-tracing is close enough to the target point, where the normalization condition of the wave function is going to be satisfied, we add the magnetic potential λ as the second continuation parameter with the magnetization M as the additional constraint condition. Then we implement the two-parameter continuation algorithm until the target point is reached, and the ground state solution of the GPEs is obtained. The excited state solutions of the GPEs can be treated in a similar way. Some numerical experiments on Na 23 and Rb 87 are reported. The numerical results on the spin-1 BEC are the same as those reported in 10]. Further numerical experiments on excited-state solutions of spin-1 BEC suffice to show the robustness and efficiency of the proposed two-parameter continuation algorithm.',\n",
       " '53e997d7b7602d9701fce8ca': 'During the last years, the design and development of technology-enhanced training systems for disabled groups of learners has attracted the attention of the technology-enhanced learning community. However, although a number of such systems have been designed to meet accessibility needs and preferences for those groups, most of them anticipate special-purpose e-training material and keep their e-training activities local to the particular system in use. As a result, neither reuse of existing digital training resources (widely available nowadays in web-based repositories) nor sharing of best technology-facilitated training practices among the communities of educational practitioners and training organizations is supported by these systems. Within this context, in this paper, we present the eAccess2Learn Framework which aims at providing tools and services that facilitate the design and development of accessible e-training resources and courses that bare the potential to be interexchanged between different e-training platforms and programs, thus making them potentially exploitable and reusable between different disabled user groups.',\n",
       " '53e997d7b7602d9701fce8f9': 'Many distributed collective decision-making processes must balance diverse individual preferences with a desire for collective unity. We report here on an extensive session of behavioral experiments on biased voting in networks of individuals. In each of 81 experiments, 36 human subjects arranged in a virtual network were financially motivated to reach global consensus to one of two opposing choices. No payments were made unless the entire population reached a unanimous decision within 1 min, but different subjects were paid more for consensus to one choice or the other, and subjects could view only the current choices of their network neighbors, thus creating tensions between private incentives and preferences, global unity, and network structure. Along with analyses of how collective and individual performance vary with network structure and incentives generally, we find that there are well-studied network topologies in which the minority preference consistently wins globally; that the presence of \"extremist\" individuals, or the awareness of opposing incentives, reliably improve collective performance; and that certain behavioral characteristics of individual subjects, such as \"stubbornness,\" are strongly correlated with earnings.',\n",
       " '53e997d7b7602d9701fce8ef': \"Recently we have introduced a new technique for combining classical bivariate Shepard operators with three point polynomial interpolation operators (Dell'Accio and Di Tommaso, On the extension of the Shepard-Bernoulli operators to higher dimensions, unpublished). This technique is based on the association, to each sample point, of a triangle with a vertex in it and other ones in its neighborhood to minimize the error of the three point interpolation polynomial. The combination inherits both degree of exactness and interpolation conditions of the interpolation polynomial at each sample point, so that in Caira et al. (J Comput Appl Math 236:1691---1707, 2012) we generalized the notion of Lidstone Interpolation (LI) to scattered data sets by combining Shepard operators with the three point Lidstone interpolation polynomial (Costabile and Dell'Accio, Appl Numer Math 52:339---361, 2005). Complementary Lidstone Interpolation (CLI), which naturally complements Lidstone interpolation, was recently introduced by Costabile et al. (J Comput Appl Math 176:77---90, 2005) and drawn on by Agarwal et al. (2009) and Agarwal and Wong (J Comput Appl Math 234:2543---2561, 2010). In this paper we generalize the notion of CLI to bivariate scattered data sets. Numerical results are provided.\",\n",
       " '53e997d7b7602d9701fce905': 'Commercial applications are playing an important role in information society, requiring an effective and powerful management system to ensure availability and QoS of these applications. In this paper, a set of complete and systematic theory on managing commercial applications is presented, according to logical sequence: description, monitoring, analysis and maintenance. Then a system named AMPS (Application Management Platform Service) is designed and implemented correspondingly. Afterwards, the performance of the AMPS is evaluated in terms of a series of experiments. This work has three contributions: (i), research work aiming at managing commercial applications is carried out in terms of logical sequence: description, monitoring, analysis and maintenance, enlightening a new approach for management system; secondly (ii), how to effectively manage multi-tier commercial applications is under focus; and thirdly (iii), a general purpose application management platform service system AMPS is implemented based on our theory and evaluated through experiments.',\n",
       " '53e997d7b7602d9701fcec3c': \"This paper presents the design of an optimal controller which is applied to control the actuator force of the semi-active vibration isolator in order to suppress vibration effectively. The controller design technique is based on dual heuristic dynamic programming (DHP), which is one of the structures in adaptive dynamic programming (ADP). Least mean square (LMS) algorithm is applied to the learning rules of the action network and the critic network. The update equations for the weights based on backpropagation algorithm are illustrated in detail. A single-stage training process is also demonstrated as a useful training strategy. Two types of vibrations are utilized to verify the effectiveness of this control design. Simulation results show that, compared with the passive system, the semiactive vibration isolator can significantly improve the system's performances in acceleration, velocity and displacement. © 2008 IEEE.\",\n",
       " '53e997d7b7602d9701fcec85': 'This paper presents a report on APNOMS 2013, which was held September 25---27, 2013 in the International Conference Center, Hiroshima, Japan. The theme of APNOMS 2013 was \\\\\"Integrated Management of Network Virtualization\\\\\".',\n",
       " '53e997d7b7602d9701fcec88': 'In this paper, we consider a problem of recognizing the shape of an event region in wireless sensor networks (WSNs). The basic idea of our algorithm is to focus on a distance field defined by the hop count from the boundary of the event region. By constructing such field, we can easily identify several critical points in the event region (e.g., local maximum and saddle point), which will be used to characterize the shape of the event region. The communication cost required for a shape recognition significantly decreases compared with a naive centralized scheme by selectively allowing those critical points to send a notification message to a data aggregation point. The performance of the proposed scheme is evaluated by simulation. The result of simulations indicates that: 1) accuracy of shape recognition depends on the density of the underlying WSN, while it is robust against the lack of sensors in a particular region in the field, and 2) the cost of shape recognition significantly decreases by applying the proposed scheme.',\n",
       " '53e997d7b7602d9701fcecb1': 'An inductive learning algorithm takes a set of data as input and generates a hypothesis as output. A set of data is typically consistent with an infinite number of hypotheses; therefore, there must be factors other than the data that determine the output of the learning algorithm. In machine learning, these other factors are called the bias of the learner. Classical learning algorithms have a fixed bias, implicit in their design. Recently developed learning algorithms dynamically adjust their bias as they search for a hypothesis. Algorithms that shift bias in this manner are not as well understood as classical algorithms. In this paper, we show that the Baldwin effect has implications for the design and analysis of bias shifting algorithms. The Baldwin effect was proposed in 1896 to explain how phenomena that might appear to require Lamarckian evolution (inheritance of acquired characteristics) can arise from purely Darwinian evolution. Hinton and Nowlan presented a computational model of the Baldwin effect in 1987. We explore a variation on their model, which we constructed explicitly to illustrate the lessons that the Baldwin effect has for research in bias shifting algorithms. The main lesson is that it appears that a good strategy for shift of bias in a learning algorithm is to begin with a weak bias and gradually shift to a strong bias.',\n",
       " '53e997d7b7602d9701fcecde': 'Given a tree T=(V,E) on n vertices, we consider the (1:q) Maker-Breaker tree embedding game T\"n. The board of this game is the edge set of the complete graph on n vertices. Maker wins T\"n if and only if she is able to claim all edges of a copy of T. We prove that there exist real numbers @a,@e0 such that, for sufficiently large n and for every tree T on n vertices with maximum degree at most n^@e, Maker has a winning strategy for the (1:q) game T\"n, for every q@?n^@a. Moreover, we prove that Maker can win this game within n+o(n) moves which is clearly asymptotically optimal.',\n",
       " '53e997d7b7602d9701fce488': ' Model Theory. These are non-empty families I ofpartial isomorphisms between models M and N , closed under taking restrictions tosmaller domains, and satisfying the usual Back-and-Forth properties for extensionwith objects on either side -- restricted to apply only to partial isomorphisms of size atmost k . \"Invariance for k--partial isomorphism\" means having the same truth value attuples of objects in any two models that are connected by a partial isomorphism insuch a set. The precise... ',\n",
       " '53e997d7b7602d9701fce499': 'This paper presents a simulation study of a cellular highcapacity land-mobile radio system using the hybrid channel assignment scheme with Erlang- C service. The performance of this system in regard to the probability of queueing, the average queueing time, and the average number of queued calls is evaluated and compared with that of the systems employing either the fixed or the dynamic channel assignment scheme.',\n",
       " '53e997d7b7602d9701fcedf2': 'In this paper we present two approximation algorithms for the permutation flow shop problem with makespan objective. The first algorithm has an absolute performance guarantee, the second one is an \\n<img src=\"/fulltext-image.asp?format=htmlnonpaginated&src=T753583N38K33X47_html\\\\10479_2004_Article_5272561_TeX2GIFIE1.gif\" border=\"0\" alt=\"\\n$$O(\\\\sqrt {m{\\\\text{ log }}m} )$$\\n\" />-approximation algorithm. The last result is almost best possible approximation algorithm which can be obtained using the trivial lower bound (maximum of the maximum machine load and the maximum job length) (Potts, Shmoys, and Williamson, 1991).',\n",
       " '53e997d7b7602d9701fceadf': 'Pattern matching in directed graphs is useful in many areas including type systems, functional languages, regular tree expressions, cyclic term graph rewriting systems and machine translation. We investigate in this paper the problem of directed graph pattern matching allowing some mismatches in labels. Two algorithms for computing the distance between ordered labeled rooted directed graphs, which is the central part of approximate pattern matching, are presented: the first algorithm computes the distance between two graphs P and T in time O(¦E\\nP\\n¦ ¦V\\nT\\n¦) and space O(¦E\\nT\\n¦+¦E\\nP\\n¦ ¦V\\nT\\n¦). It is as fast as the best solution for determining whether a directed graph matches another. The second algorithm computes the distance between every subgraph of P and every subgraph of T in time O(¦E\\nP\\n¦ ¦V\\nT\\n¦ (¦V\\nP\\n¦+¦V\\nT\\n¦)) and space O(¦V\\nP\\n¦ ¦V\\nT\\n¦ (¦V\\nP\\n¦+¦V\\nT\\n¦)). It is the first solution for this problem.',\n",
       " '53e997d7b7602d9701fceb2b': 'Linear systems with M-matrices often occur in a wide variety of areas including scientific computing, operations research, stochastic analysis and economics. In this paper, we use a new preconditioner for solving such a kind of system by the preconditioned conjugate gradient (PCG) method. We show that our preconditioner can obtain a fast convergence rate. A numerical example is also given.',\n",
       " '53e997d7b7602d9701fceb8c': 'We describe recently developed semantics-based support tools for Z, a mathematical specification language based on typed set theory. Z has proven very useful and popular with a number of industrial as well as academic software developers. These tools are components of Forsite, a support environment currently under development to integrate languages and operations with formally defined semantics and implementable operations. We anticipate that these tools will impose a de facto standard for the language.',\n",
       " '53e997d7b7602d9701fceb8e': 'Requirements elicitation describes the activities needed to determine the requirements of a software system. It is essential to the success of a software development project. Requirements elicitation techniques are used by requirements engineers to discover the information needed to build systems that will satisfy the needs of stakeholders. This paper presents different types of software systems and the factors that would affect the selection of attributes needed to determine appropriate elicitation techniques. These attributes could then be used to determine elicitation techniques that would be successful for different project types.',\n",
       " '53e997d7b7602d9701fceb91': 'We present and validate an enhanced analytical queueing network model of zoned RAID. The model focuses on RAID levels 01 and 5, and yields the distribution of I/O request response time. Whereas our previous work could only sup- port arrival streams of I/O requests of the same type, the model presented here supports heterogeneous streams with a mixture of read and write requests. This improved realism is made possible through multiclass extensions to our ex- isting model. When combined with priority queueing, this development also enables more accurate modelling of the way subtasks of RAID 5 write requests are scheduled. In all cases we derive analytical results for calculating not only the mean but also higher moments and the full distri- bution of I/O request response time. We validate our model against measurements from a real RAID system.',\n",
       " '53e997d7b7602d9701fcf029': 'The Arabidopsis Information Resource (TAIR, http://arabidopsis.org) is the model organism database for the fully sequenced and intensively studied model plant Arabidopsis thaliana. Data in TAIR is derived in large part from manual curation of the Arabidopsis research literature and direct submissions from the research community. New developments at TAIR include the addition of the GBrowse genome viewer to the TAIR site, a redesigned home page, navigation structure and portal pages to make the site more intuitive and easier to use, the launch of several TAIR web services and a new genome annotation release (TAIR7) in April 2007. A combination of manual and computational methods were used to generate this release, which contains 27,029 protein-coding genes, 3889 pseudogenes or transposable elements and 1123 ncRNAs (32,041 genes in all, 37,019 gene models). A total of 681 new genes and 1002 new splice variants were added. Overall, 10,098 loci (one-third of all loci from the previous TAIR6 release) were updated for the TAIR7 release.',\n",
       " '53e997d7b7602d9701fcf182': 'We show that for every conjunctive query, the complexity of evaluating it on a probabilistic database is either PTIME or P-complete, and we give an algorithm for deciding whether a given conjunctive query is PTIME or P-complete. The dichotomy property is a fundamental result on query evaluation on probabilistic databases and it gives a complete classification of the complexity of conjunctive queries.',\n",
       " '53e997d7b7602d9701fcf1d4': 'It has long been known that pattern matching in the Hamming distance metric can be done in O(min(|@S|,m/logm)nlogm) time, where n is the length of the text, m is the length of the pattern, and @S is the alphabet. The classic algorithm for this is due to Abrahamson and Kosaraju. This paper considers the following generalization, motivated by the situation where the entries in the text and pattern are analog, or distorted by additive noise, or imprecisely given for some other reason: in any alignment of the pattern with the text, two aligned symbols a and b contribute +1 to the similarity score if they differ by no more than a given threshold @q, otherwise they contribute zero. We give an O(min(|@S|,m/logm)nlogm) time algorithm for this more general version of the problem; the classic Hamming distance matching problem is the special case of @q=0.',\n",
       " '53e997d7b7602d9701fcf224': '\\n Recent advances in interactive pen-aware systems, pattern recognition technologies, and human–computer interaction have provided\\n new opportunities for pen-based communication between human users and intelligent computer systems. Using interactive maps,\\n users can annotate pictorial or cartographic information by means of pen gestures and handwriting. Interactive maps may provide\\n an efficient means of communication, in particular in the envisaged contexts of crisis management scenarios, which require\\n robust and effective exchange of information. This information contains, e.g., the location of objects, the kind of incidents,\\n or the indication of route alternatives. When considering human interactions in these contexts, various pen input modes are\\n involved, like handwriting, drawing, and sketching. How to design the required technology for grasping the intentions of the\\n user based on these pen inputs remains an elusive challenge, which is discussed in this chapter. Aspects like the design of\\n a suitable set of pen gestures, data collection in the context of the envisaged scenarios, and the development of distinguishing\\n features and pattern recognition technologies for robustly recognizing pen input from varying modes are described. These aspects\\n are illustrated by presenting our recent results on the development of interactive maps within the framework of the ICIS project\\n on crisis management systems.\\n \\n ',\n",
       " '53e997d7b7602d9701fcf23d': 'In this paper, we consider semi-online minimum makespan scheduling problem with reassignment on two identical machines. Two versions are discussed. In the first version, one can reassign the last job of one machine that is based on the problem proposed by Tan and Yu (2008) [1], in which case the last job of each machine is allowed to be reassigned. An optimal algorithm which has the same competitive ratio 2 is presented. In the second version we consider the combination of the next two conditions: the total size of all jobs is known in advance and one can reassign the last job of one machine. For this problem an optimal algorithm with competitive ratio 54 is also given.',\n",
       " '53e997d7b7602d9701fcf25b': \"(MATH) Let f: {0,1}n → {0,1} be a boolean function. For ε&roe; 0 De(f) be the minimum depth of a decision tree for f that makes an error for &xie;ε fraction of the inputs &khar; &Egr; {0,1}n. We also make an appropriate definition of the approximate certificate complexity of f, Cε(f). In particular, D0(f) and C0(f) are the ordinary decision and certificate complexities of f. It is known that $D_0(f) \\\\leq (C_0(f))^2$. Answering a question of Tardos from 1989, we show that for all $\\\\Ge > 0$ there exists a $\\\\Gd' > 0$ such that for all $0 \\\\leq \\\\Gd  0$ is a constant independent of f. The algorithm used in the proof is modeled after those developed by R. Impagliazzo and S. Rudich for use in other problems.\",\n",
       " '53e997d7b7602d9701fcf256': \"Storytelling is an essential activity in the life of children. By listening or sharing their stories and ideas they give meaning to their world and practice their communication skills. Even though today computers are already present in a child's world, there is a difference between software programs developed for them and specific programs that allow them to express themselves. This paper presents the Reactoon System: an authoring tool for building 2D animations for a tabletop with tangible user interface and multi-touch screen.\",\n",
       " '53e997d7b7602d9701fcf39f': 'Software productivity has been steadily increasing over the past 30 years, but not enough to close the gap between the demands placed on the software industry and what the state of the practice can deliver nothing short of an order of magnitude increase in productivity will extricate the software industry from its perennial crisis. Several decades of intensive research in software engineering and artificial intelligence left few alternatives but software reuse as the (only) realistic approach to bring about the gains of productivity and quality that the software industry needs. In this paper, we discuss the implications of reuse on the production, with an emphasis on the technical challenges. Software reuse involves building software that is reusable by design and building with reusable software. Software reuse includes reusing both the products of previous software projects and the processes deployed to produce them, leading to a wide spectrum of reuse approaches, from the building blocks (reusing products) approach, on one hand, to the generative or reusable processor (reusing processes), on the other. We discuss the implication of such approaches on the organization, control, and method of software development and discuss proposed models for their economic analysis. Software reuse benefits from methodologies and tools to:build more readily reusable software andlocate, evaluate, and tailor reusable software, the last being critical for the building blocks approach.Both sets of issues are discussed in this paper, with a focus on application generators and OO development for the first and a thorough discussion of retrieval techniques for software components, component composition (or bottom-up design), and transformational systems for the second. We conclude by highlighting areas that, in our opinion, are worthy of further investigation.',\n",
       " '53e997d7b7602d9701fcf36a': 'Real world environments are so dynamic and unpredictable that a goal-oriented autonomous system performing a set of tasks repeatedly never experiences the same situation even though the task routines are the same. Hence, manually designed solutions to execute such tasks are likely to fail due to such variations. Developmental approaches seek to solve this problem by implementing local learning mechanisms to the systems that can unfold capabilities to achieve a set of tasks through interactions with the environment. However, gathering all the information available in the environment for local learning mechanisms to process is hardly possible due to limited resources of the system. Thus, an information acquisition mechanism is necessary to find task-relevant information sources and applying a strategy to update the knowledge of the system about these sources efficiently in time. A modular systems approach may provide a useful structured and formalized basis for that. In such systems different modules may request access to the constrained system resources to acquire information they are tuned for. We propose a reward-based learning framework that achieves an efficient strategy for distributing the constrained system resources among modules to keep relevant environmental information up to date for higher level task learning and executing mechanisms in the system. We apply the proposed framework to a visual attention problem in a system using the iCub humanoid in simulation.',\n",
       " '53e997d7b7602d9701fcf39e': 'A composition ¿ = a 1 a 2 ¿ a m of n is an ordered collection of positive integers whose sum is n . An element a i in ¿ is a strong (weak) record if a i a j ( a i ¿ a j ) for all j = 1 , 2 , ¿ , i - 1 . Furthermore, the position of this record is i . We derive generating functions for the total number of strong (weak) records in all compositions of n , as well as for the sum of the positions of the records in all compositions of n , where the parts a i belong to A = d ] : = { 1 , 2 , ¿ , d } or A = N . In particular when A = N , we find the asymptotic mean values for the number, and for the sum of positions of records in compositions of n .',\n",
       " '53e997d7b7602d9701fcf5ff': 'In spatial reasoning the qualitative description of relations between spatial regions is of practical importance and has been widely studied. Examples of such relations are that two regions may meet only at their boundaries or that one region is a proper part of another. This paper shows how systems of relations between regions can be extended from precisely known regions to approximate ones. One way of approximating regions with respect to a partition of the plane is that provided by rough set theory for approximating subsets of a set. Relations between regions approximated in this way can be described by an extension of the RCC5 system of relations for precise regions. Two techniques for extending RCC5 are presented, and the equivalence between them is proved. A more elaborate approximation technique for regions (boundary sensitive approximation) takes account of some of the topological structure of regions. Using this technique, an extension to the RCC8 system of spatial relations is presented.',\n",
       " '53e997d7b7602d9701fcf661': 'Traditional customer relationship management (CRM) models often ignore the correlation that could exist in the purchasing behavior of neighboring customers. Instead of treating this correlation as nuisance in the error term, a generalized linear autologistic regression can be used to take these neighborhood effects into account and improve the predictive performance of a customer identification model for a Japanese automobile brand. In addition, this study shows that the level on which neighborhoods are composed has an important influence on the extra value that results from the incorporation of spatial autocorrelation.',\n",
       " '53e997d7b7602d9701fcf677': 'As the clock frequencies used in industrial applications increase, the timing requirements on routing problems become tighter, and current routing tools can not successfully handle these constraints any more. We focus on the high-performance single-layer bus routing problem, where the objective is to match the lengths of all nets belonging to each bus. An effective approach to solve this problem is to allocate extra routing resources around short nets during routing; and use those resources for length extension afterwards. We first propose a provably optimal algorithm for routing nets with min-area max-length constraints. Then, we extend this algorithm to the case where minimum constraints are given as exact length bounds. We also prove that this algorithm is optimal within a constant factor. Both algorithms proposed are also shown to be scalable for large circuits, since the respective time complexities are O(A) and O(A log A), where A is the area of the intermediate region between chips.',\n",
       " '53e997d7b7602d9701fcf6a9': 'A graph is called perfect matching compact (briefly, PM-compact), if its perfect matching graph is complete. Matching-covered PM-compact bipartite graphs have been characterized. In this paper, we show that any PM-compact bipartite graph G with delta(G) >= 2 has an ear decomposition such that each graph in the decomposition sequence is also PM-compact, which implies that G is',\n",
       " '53e997d7b7602d9701fcf9c7': \"Non-real-time services are an important category of network services in future wireless networks. When mobile users access non-real-time services, mobile users usually care about two important points; one is whether mobile users are not forced to terminate during their lifetime, and the other is whether the total time to complete mobile users' data transfer is within their time tolerance. Low forced termination probability can be achieved through use of the technique of bandwidth adaptation which dynamically adjusts the number of bandwidths allocated to a mobile user during the mobile user's connection time. However, there is not a metric at a connection level to present the degree of the length of the total completion time. In this paper, we describe a quality-of-service metric, called stretch ratio, to present the degree of the length of the total completion time. We design a measurement based call admission control scheme that uses the stretch ratio to determine whether or not to accept a new mobile user into a cell. Extensive simulation results show that the measurement based call admission control scheme not only satisfies the quality-of-service requirement of mobile users (in terms of the stretch ratio) but also highly utilizes radio resource.\",\n",
       " '53e997d7b7602d9701fcfa65': 'Internet-based clusters of workstations have been extensively used to execute parallel applications. Although these internet-based clusters seem to be an easy and inexpensive way of obtaining great performance, it may not always be so. When using such a cluster for executing a parallel application, performance may not be as good as expected due to delays in communication . Also, the heterogeneity in communication makes it hard to take advantage, or reuse, communication strategies that were useful in regular-topology platforms, e.g., parallel machines or LAN-based clusters of workstations. For instance, broadcasting in an internet-based cluster may be more challenging due to the variety of communication links and, consequently, of point-to-point latencies. In this paper, we present a strategy to improve hypercube-based broadcasting algorithms that are used in regular-topol ogy platforms, so that they can execute efficiently in internet-based clusters of workstations.',\n",
       " '53e997d7b7602d9701fcf96d': \"Internet commerce, and especially web commerce, applications are often used as part of firm's business strategy. The paper presents results of analysis of e-commerce adoption in top 100 firms in the Czech Republic (TOP 100 CZ firms). These firms belong to various industrial business sectors. Analysis is based on assessment of company's websites. Assessment is based on evaluation of company websites executed from customer's point of view. For evaluation, the WBE (website business evaluation) method was used. Results show that most of the top 100 Czech firms (in fact all but four) are using e-commerce, typically in the form of websites; however, most of these are only for support of the information phase of the market transaction. Even in the Czech Republic, this use which a few years ago could present competitive advantage is becoming common and almost imperative from a business point of view.\",\n",
       " '53e997d7b7602d9701fcf96f': 'We propose a system named AIGNET (Algorithms for Inference of Genetic Networks), and introduce two top-down approaches for the inference of interrelated mechanism among genes in genetic network that is based on the steady state and temporal analyses of gene expression patterns against some kinds of gene perturbations such as disruption or overexpression. The former analysis is performed by a static Boolean network model based on multi-level digraph, and the latter one is by S-system model. By integrating these two analyses, we show our strategy is flexible and rich in structure to treat gene expression patterns; we applied our strategy to the inference of a genetic network that is composed of 30 genes as a case study. Given the gene expression time-course data set under the conditions of wild-type and the deletion of one gene, our system enabled us to reconstruct the same network architecture as original one.',\n",
       " '53e997d7b7602d9701fcfa3b': 'An improved method is proposed based on compressed and Krylov-subspace iterative approaches to perform efficient static and transient simulations for large-scale power grid circuits. It is implemented with CG and BiCGStab algorithms and an excellent result has been obtained. Extensive experimental results on large-scale power grid circuits show that the present method is over 200 times faster than SPICE3 and around 10-20 times faster than ICCG method in transient simulations. Furthermore, the presented algorithm saves the memory usage over 95% of SPICE3 and 75% of ICCG method, respectively while the accuracy is not compromised.',\n",
       " '53e997d7b7602d9701fd0004': \"An ad hoc team setting is one in which teammates must work together to obtain a common goal, but without any prior agreement regarding how to work together. In this work we introduce a role-based approach for ad hoc teamwork, in which each teammate is inferred to be following a specialized role that accomplishes a specific task or exhibits a particular behavior. In such cases, the role an ad hoc agent should select depends both on its own capabilities and on the roles currently selected by other team members. We present methods for evaluating the influence of the ad hoc agent's role selection on the team's utility and we examine empirically how to choose the best suited method for role assignment in a complex environment. Finally, we show that an appropriate assignment method can be determined from a limited amount of data and used successfully in new tasks that the team has not encountered before.\",\n",
       " '53e997d7b7602d9701fd003c': 'In [C.H. Tsai, S.Y. Jiang, Path bipancyclicity of hypercubes, Inform. Process. Lett. 101 (2007) 93-97], the authors showed that any path in an n-cube with length of k, 2=',\n",
       " '53e997d7b7602d9701fd005a': 'The main aim of this work is to construct a general scheme that would permit to \\\\\"insert\\\\\" the notion of roughness into polygroups. We consider the factor polygroup P/N and interprete the lower and upper approximations as subsets of the factor polygroup P/N. Then we introduce the concept of factor rough subpolygroups. Also, using the concept of fuzzy set, we introduce and discuss the concept of fuzzy rough polygroup and then we obtain the relation between fuzzy rough subpolygroups and level rough sets. This relation is expressed in terms of a necessary and sufficient condition.',\n",
       " '53e997d7b7602d9701fd00a5': 'Over the last several years, studies of instant messaging have observed its increasing role in the workplace[1] and in social situations[2]. We propose that modifying applications to interact with users over Instant Messaging (as IM bots) extends the collaborative benefits of IM into new areas. As IM Bots participating in group chatrooms, applications that had previously been restricted to a single user command line are able to engage in many to many interactions between users and applications. Current command line oriented user interfaces can be made into collaborative interfaces that exhibit (at a basic level) the ethnomethodological property of accountability as well as supporting legitimate peripheral participation.',\n",
       " '53e997d7b7602d9701fd0177': \"Positron Emission Tomography scans are a promising source of information for early diagnosis of Alzheimer's disease. However, such neuroimaging procedures usually generate high-dimensional data. This complicates statistical analysis and modeling, resulting in high computational complexity and typically more complicated models. However, the utilization of domain-knowledge can reduce the complexity and promote simpler models. In this study, we investigate Gaussian processes, which may incorporate domain-knowledge, for predictive modeling of Alzheimer's disease. This study uses features extracted from PET imagery by 3D Stereotactic Surface Projection. Since the number of features can be high even after applying prior knowledge, we examine the benefits of a correlation-based feature selection method. Feature selection is desirable as it enables the detection of metabolic abnormalities that only span certain portions of the anatomical regions. Our proposed utilization of Gaussian processes is superior to the alternative (Automatic Relevance Determination), resulting in more accurate diagnosis with less computational effort.\",\n",
       " '53e997d7b7602d9701fd0194': ' The nearest neighbor (NN) classifiers, especially the k-NN algorithm, are among the simplestand yet most efficient classification rules and are widely used in practice. We introduce threeadaptation rules that can be used in iterative training of a k-NN classifier. This is a novelapproach both from the statistical pattern recognition and the supervised neural network learningpoints of view. The suggested learning rules resemble those of the well-known Learning VectorQuantization (LVQ)... ',\n",
       " '53e997d7b7602d9701fd0209': 'This paper addresses the issue of cost-optimal voice over IP (VoIP) network design. In the applied model, the whole VoIP network is divided into two logical components: the access network and the transport network. The access network consists of VoIP end-points that connect to the transport network through edge routers serving as gateways. Since multiple edge routers may be available for any given VoIP node, one task of the design process is to assign a particular edge router to every VoIP node. The edge routers have to be connected in a way that security and availability can be assured for the VoIP traffic. One obvious approach to fulfilling these requirements, which is assumed throughout the paper, is to define a virtual private network (VPN). Supposing a large volume of VoIP traffic, the cost of the VPN can be significant; thus, the other task of VoIP network design is to specify the transport VPN in the most economical way. These two tasks of VoIP network design can be solved separately using existing methods; nevertheless, the specification of VoIP regions influences the cost of the final solution to a great extent. Therefore, in this paper a novel approach is proposed in which the edge router assignment process takes the objective function of VPN specification into consideration as well. In order to realize the new approach, multiple methods are introduced which are based on the paradigms of genetic algorithms and simulated annealing. These methods perform a sophisticated optimization of the gateway assignments using various cost calculation methods. To evaluate the new algorithms, a method based on a well-known greedy solution to the problem is used as reference. Moreover, a VPN specification algorithm is presented which utilizes the stepwise nature of the cost functions. The performance of the presented methods is evaluated with the help of simulations. It is shown that the proposed methods outperform the reference algorithm significantly in the simulation scenarios investigated.',\n",
       " '53e997d7b7602d9701fd0228': '•Optimization-based approach for coarse-grained design of synthetic genetic circuits.•Qualitative description of basic biological parts used for the circuit design.•Promoters and ribosome binding sites categorized as low, medium and high efficiency.•Designed a genetic toggle switch, a 2–4 genetic decoder and a genetic half adder.•Identified novel and non-intuitive as well as known circuit structures.',\n",
       " '53e997d7b7602d9701fcfe28': 'An ad-hoc wireless network is a cooperative body of mobile wireless nodes that is capable of sustaining multi-hop communication. DSR is a well known routing protocol that has been proven to be robust and reliable in forming such networks. The protocol inherently has a large control packet and byte overhead, so it makes extensive use of its caching strategy to optimise its performance. However, large control packets severely restrict the protocol’s ability to scale to larger networks. In addition, the source controlled routing paradigm introduces high latency when scaled. In this paper we address this problem by presenting a hybrid protocol, called Finite Horizon Routing (FHR), which improves upon the existing routing strategy of DSR and makes it more resilient to link failures in oversized networks. FHR partially removes the large control packet problem of DSR. Through our simulations we show that the reliability, total overhead, and latency can be improved by up to 10%, 23% and 53% respectively.',\n",
       " '53e997d7b7602d9701fcff3e': 'The development of spoken dialogue systems is often limited by the performance of their speech recognition component. The impact of speech recognition errors on dialogue systems is often studied at the global level of task completion. In this paper, we carry an empirical study on the consequences of speech recognition errors on a fully-implemented dialogue prototype, based on a speech acts formalisms. We report the impact of speech recognition errors on speech act identification and discuss how standard control mechanisms can participate to robustness by assisting the user in repairing the consequences of speech recognition errors.',\n",
       " '53e997d7b7602d9701fd0424': 'The success of the Internet has made some dramatic changes in the way scientists and students are supplied with scientific literature. They can now operate in an information market, where the value of a piece of information is determined by the law of supply and demand. Traders assist the customer in this market. This paper describes a basic approach to trading in the open, distrib- uted, and heterogeneous environment of a market of scientific information.',\n",
       " '53e997d7b7602d9701fd0430': 'The objective of this work is to automatically determine, in an unsupervised manner, Spanish prepositional phrases of the type preposition – nominal phrase – preposition (P(NP(P) that behave in a sentence as a lexical unit and their semantic and syntactic properties cannot be deduced from the corresponding properties of each simple form, e.g., por medio de (by means of), a fin de (in order to), con respecto a (with respect to). We show that idiomatic P(NP(P combinations have some statistical properties distinct from those of usual idiomatic collocations. We also explore a way to differentiate P(NP(P combinations that could perform either as a regular prepositional phrase or as idiomatic prepositional phrase.',\n",
       " '53e997d7b7602d9701fd0442': 'The subject of this note is the parallel algorithm for depth first searching of a directed acyclic graph by Ghosh and Bhattacharjee. It is pointed out that their algorithm does not always work. A counter example is given. This paper also states the necessary and sufficient condition for the algorithm to fail, or to work correctly.',\n",
       " '53e997d7b7602d9701fd0465': 'We correct an inconsistency in the efficiency comparison reported in [Y. Chen, T. Sönmez, School choice: An experimental study, J. Econ. Theory 127 (1) (2006) 202–231]. The efficiency comparison of the three school choice mechanisms in our paper is based on recombinant estimation with an identical set of 10 tie-breakers, while the statistics reported in Table 7 is computed using 14,400 tie-breakers.',\n",
       " '53e997d7b7602d9701fd048a': 'Internet technologies have a great potential for changing fundamentally the banks and the banking industry. The opportunities, which the e-banking services and technologies offer to the banking sector in order to fulfil existing customer needs and to attract new prospective customers, are the driving forces for banks in order to design, develop and operate their own e-banking systems. This paper examines the challenges and opportunities of e-banking for the Greek banking sector, during the e-commerce era, and also presents the results of a survey of banking executives working at banks offering e-banking services. The main findings demonstrate that banks expand to e-banking services in order to remain competitive, to keep track with technological developments and to benefit from the lower cost of e-banking transactions. The major problems they face are the low response rate from customers and the implementation of security and data protection mechanisms. The relatively low Internet usage, the non-familiarity with technologically advanced devices and problems regarding security and privacy are the main factors that have a negative influence on the adoption of e-banking services by customers in Greece.',\n",
       " '53e997d7b7602d9701fd04d7': 'As ad hoc wireless networks require less or no fixed infrastructure support, communications among nodes can be quickly and adaptively constructed. This property ensures that ad hoc wireless networks are especially suitable for communications in critical application such as military, emergency and rescue missions. In this paper, we design and evaluate a security framework for multilevel ad hoc wireless networks with unmanned aerial vehicles. Our design, based on identity-based public key infrastructure using bilinear pairing, consists of group key management architecture, static pair-wise communication, tripartite key agreement, and group key agreement. Compared to its counterparts, our design reduces the certificate validation process, improves computational efficiency and reduces storage requirement',\n",
       " '53e997d7b7602d9701fd04ec': 'Probabilistic graphical model representations of relational data provide a number of desired features, such as inference of missing values, detection of errors, visualization of data, and probabilistic answers to relational queries. However, adoption has been slow due to the high level of expertise expected both in probability and in the domain from the user. Instead of requiring a domain expert to specify the probabilistic dependencies of the data, we present an approach that uses the relational DB schema to automatically construct a Bayesian graphical model for a database. This resulting model contains customized distributions for the attributes, latent variables that cluster the records, and factors that reflect and represent the foreign key links, whilst allowing efficient inference. Experiments demonstrate the accuracy of the model and scalability of inference on synthetic and real-world data.',\n",
       " '53e997d7b7602d9701fd04f2': 'As the VLSI technology scales towards the nanometer regime, circuit performance is increasingly affected by variations. These variations need to be considered at an early stage in performance optimization. This work proposes a new placement methodology that facilitates low cost and robust clock network. It is based on the observation that bringing tightly constrained flipflops close to each other can reduce the noncommon paths between them in clock network. Such a reduction will inturn improve the tolerance of the clock network towards variations in delay/skew. Monte Carlo experiments (based on spatial correlations) indicate that our methodology can reduce the maximum skew violation due to variations by up to 62% with less than 2.7% increase in total wire length.',\n",
       " '53e997d7b7602d9701fd051c': 'Widowing algorithms are an important class of synchronization algorithms for parallel discrete event simulation. In these algorithms, a simulation window is chosen such that all events within the window can be executed concurrently without the possibility of a causality error. Using the terminology of Chandy and Sherman (1989), these are unconditional events. Windowing algorithms, as all non-aggressive algorithms, have been criticized for not allowing a computation to proceed because there exists the possibility of a causality error. We are interested in the impact of extending the simulation window in order to allow the computation of conai’tional events, that is, those events that may cause an error. In this paper we develop a model to investigate the probability of a causality error occurring when the simulation window is extended to allow conditional events into the computation stream. Also we give results ffom simulation studies which validate our model.',\n",
       " '53e997d7b7602d9701fd051d': \"This paper deals with estimating performance measures, such as average response time for spatially distributed networks. Demands are generated stochastically at the nodes and the service units are stationed at service centers when available. Whenever a call arrives, a service unit will travel to the call's location. When there are no available servers, the call will enter an infinite capacity queue at that node. The service units will travel from node to node serving the calls and return to the service center only when there are no more calls waiting. In most cases, exact models are too complicated to analyze. This paper presents approximations which are tested using simulation and found to give good results.\",\n",
       " '53e997d7b7602d9701fd0538': 'A probabilistic model for software development projects is constructed. The model can be applied to compute an estimate for the development time of a project. The chances of succeeding with a given amount of time and the risk of deviating from the estimate can be computed as well. Examples show that the model behaves as expected when the input data are changed.',\n",
       " '53e997d7b7602d9701fd053b': 'The validation of data from sensors has become an important issue in the operation and control of modern industrial plants. One approach is to use knowledge based techniques to detect inconsistencies in measured data. This article presents a probabilistic model for the detection of such inconsistencies. Based on probability propagation, this method is able to find the existence of a possible fault among the set of sensors. That is, if an error exists, many sensors present an apparent fault due to the propagation from the sensor(s) with a real fault. So the fault detection rueehanism can only tell if a sensor has a potential fault, but it can not tell if the fault is real or apparent. So the central problem is to develop a theory, and then an algorithm, for distinguishing real and apparent faults, given that one or more sensors can fail at the same time. This article then, presents an approach based on two levels: (i) probabilistic reasoning, to detect a potential fault, and (ii) constraint management, to distinguish the real fault from the apparent ones. The proposed approach is exemplified by applying it to a power plant model.',\n",
       " '53e997d7b7602d9701fd053f': \"We present a design for performance management of SMS systems. The design takes as input the administrator's performance objectives, which can be adjusted at run-time. Based on these objectives, the design takes the necessary actions to achieve them and it dynamically adapts to changing networking conditions. It does so by periodically solving a linear optimization problem that computes a new configuration for the SMS system. We have evaluated the design through extensive simulations in various scenarios using traces from a production SMS system. It has proved effective in achieving the administrator's performance objectives, and efficient in terms of computational cost. Our experiments also show that the design is adaptive, i.e., it effectively adapts the systems's configuration to changes in the networking conditions, in order to continuously meet the performance objectives. Finally, the feasibility of our design is proved through the development of a prototype on a commercial SMS platform.\",\n",
       " '53e997d7b7602d9701fd054f': 'Finding the correct category (class) a new unclassified document belongs to is an interesting and difficult problem, with a wide range of applications. Our methodology for narrative text classification is based on two techniques: we calculate the distance (similarity) between the new unclassified document and all the pre-classified documents of each class and also calculate the similarity of the new document to the â聙聵average class documentâ聙聶 of each class. In both cases we use key phrases (text phrases or key terms) as the distinctive features of our text classification methodology and eventually the proposed text classification method is based on the automatic extraction of an authority list of key phrases that is appropriate for discriminating between different classes. In this paper, we apply this methodology in handling Greek text and we present the key concepts, the algorithms, and some critical decisions. A number of parameters of the mining algorithm are also fine tuned. The actual text classification system, the adopted (embedded) ideas and the alternative values of parameters are evaluated using two training sets (test collections).',\n",
       " '53e997d7b7602d9701fd0554': 'Textual materials are source of extremely valuable information, for which there must be a reflection on the techniques of analysis to be used to avoid subjective interpretations especially in the content. The Textual Analysis (TA), which makes use of statistical techniques, ensures the systematic exploration of the structure of the text (size, occurrence, etc.) and simultaneously the possibility to return at any time to the original text for the appropriate interpretations. In this work we test a new technique based on a probabilistic model of language known in the literature as “topic model” for analyzing corpora of documents about electromagnetic pollution. The proposed method is able to reveal how the meaning of a document is distributed all along its spectrum (word-frequency) indicating that the real meaning of a document can be inferred following a multilevel analysis. Such analysis is carried out exploiting a new concept of ontology already used in literature and deeply explained here.',\n",
       " '53e997d7b7602d9701fd055c': 'A new (partition) method for solving a tndiagonal system of lmear equations is presented in this paper The method is suitable for both parallel and vector computers. Although the partition method has a shghtly higher vector operatmn count than those of the two competing methods (the recursive doubling method and the cychc reduction method), it has a scalar count much smaller than that of the recursive doubling. The scalar counts between the partition method and the cyclic reduction method are so close as to make a timing evaluation inconclusive without considering the data management problem, especmlly when large systems are solved. Various situations under which the partmon method can be preferable are described.',\n",
       " '53e997d7b7602d9701fd0555': 'We propose the development of a prediction market to provide a form of collective intelligence for forecasting prices for \"toxic assets\" to be transferred from Irish banks to the National Asset Management Agency. Such a market al- lows participants to assume a stake in a security whose value is tied to a future event. We propose that securities are created whose value hinges on the transfer amount paid for loans from the agency to a bank. In essence, bets are accepted on whether the price is higher or lower than a quoted figure. The prices of securities indicate expected transfer costs for toxic assets. Prediction markets offer a proven means of aggregating distributed knowledge pertaining to estimates of uncertain quantities and are robust to strategic manipulation. We propose that a prediction market runs in parallel to a pricing procedure for individual assets conducted by the government agency. We advocate an approach whereby prices are chosen as a convex combination of the agency\\'s internal estimate and that of the predic- tion market. We argue that this will substantially reduce the cognitive burden for the government agency and improve the accuracy, speed and scalability of pric- ing. This approach also offers a means of empowering both property experts and non-experts in a cost-effective and transparent manner.',\n",
       " '53e997d7b7602d9701fd0562': 'This paper explores several kernels in the context of text classification. A novel view of how documents might have been created is introduced and kernels are derived from this framework. The relations between these kernels as well as to the Gaussian kernel are discussed. Moreover, the popular tf-idf weighting scheme will be derived as a natural consequence. Finally, the kernels have been evaluated on the Reuters Corpus Volume I newswire database to assess their quality in a topic classification application.',\n",
       " '53e997d7b7602d9701fd0567': 'This paper proposes a model for tracking handwriting based on curvature minimization. Aiming at recovering stroke sequences from words written in the past, the problem is formulated as a graph theoretical question. The solution does not require mathematical functions. In particular, loops are considered elementary basic strokes and are not approximated with functions. Recovering stroke sequences is equivalent to ordering the edges of a graph, which can be derived in a straightforward manner from the scanned binary word image. This paper does not deal with any special recognition method. However, an important aim is to provide a mechanism for deriving temporal information to help to improve off-line recognition methods. The most important advantages of this method over methods proposed so far are: a single global principle (global optimization of curvature), implicit modeling of retraced strokes and simplicity.',\n",
       " '53e997d7b7602d9701fd05ab': 'This paper presents a probabilistic approach for DNA sequence analysis. A DNA\\nsequence consists of an arrangement of the four nucleotides A, C, T and G and\\ndifferent representation schemes are presented according to a probability\\nmeasure associated with them. There are different ways that probability can be\\nassociated with the DNA sequence: one way is when the probability of an\\noccurrence of a letter does not depend on the previous one (termed as\\nunsuccessive probability) and in another scheme the probability of occurrence\\nof a letter depends on its previous letter (termed as successive probability).\\nFurther, based on these probability measures graphical representations of the\\nschemes are also presented. Using the diagram probability measure one can\\neasily calculate an associated probability measure which can serve as a\\nparameter to check how close is a new sequence to already existing ones.',\n",
       " '53e997d7b7602d9701fd05b1': \"Uncertainty and dynamic changes in a software project cause iterations during development and the need for decision-making in planning and controlling a project. This paper presents a Software Project Review and Evaluation Model, SPREM, a superset of CPM/PERT which extends CPM/PERT's notation to four types of vertices (AA, AX, XA, and XX vertices) to express the non-deterministic and iterative behaviors of software engineering projects. Several behavioral properties of SPREM and analysis of them are discussed. For example, the enaction capability can be used to evaluate the possibility that a vertex will enact processes beforehand. Project managers can revise a SPREM graph to rescue dead vertices before project execution. Furthermore, enaction ordering allows project managers to calculate the dependency between processes to be enacted. This might help in computing important information such as critical paths among these processes.\",\n",
       " '53e997d7b7602d9701fd05b6': 'Web service compositions, usually defined as BPEL pro- cesses, need to adapt to changes in their constituent web services, in order to maintain functionality and perfor- mance. Therefore, BPEL processes must be able to de- tect web service failure and performance degradation re- sulting in the violation of service-level agreements. Auto- mated diagnosis and repair are equally important. How- ever, BPEL lacks constructs for web service monitoring and runtime adaptability, which are pre-requisites for diagnosis and repair. We present a solution for transparent runtime monitoring, as well as automated performance degradation detection, diagnosis, and repair for BPEL processes. Our solution uses lightweight monitoring techniques, supports customizable diagnosis and repair strategies, and is com- patible with any standards-compliant BPEL engine.',\n",
       " '53e997d7b7602d9701fd061c': \"In this paper, we discuss statistical analysis of human trajectories measured by GPS-like positioning devices. Our goal is to develop a system of trajectory analysis that distributes information optimized for each user. For such a system, we need a method to estimate a user's status from his/her trajectories. First, a trajectory needs to be divided into short temporal segments, which will be matched to action model patterns, to estimate a user's status. Second, we tried dividing actual human trajectories using a conventional trajectory-clustering method. Moreover, we adjusted parameters of the trajectory clustering by using information criteria experimentally. After the experiment, we confirmed that only a criterion in which noise data are counted worked well. However, we also confirmed that the number of clusters generated by the method is too small. Therefore, we conclude that an improvement in deciding which data are noise in trajectory clustering is necessary for estimating the status of users.\",\n",
       " '53e997d7b7602d9701fd05bb': 'In this paper we will present a language-independent probabilistic model which can automatically generate stemmers. Stemmers can improve the retrieval effectiveness of information retrieval systems, however the designing and the implementation of stemmers requires a laborious amount of effort due to the fact that documents and queries are often written or spoken in several different languages. The probabilistic model proposed in this paper aims at the development of stemmers used for several languages. The proposed model describes the mutual reinforcement relationship between stems and derivations and then provides a probabilistic interpretation. A series of experiments shows that the stemmers generated by the probabilistic model are as effective as the ones based on linguistic knowledge.',\n",
       " '53e997d7b7602d9701fd0632': 'It is known that Gauss-Radau quadrature rule@!-11f(x)dx~@?i=1na\"if(b\"i)+pf(-1)(orqf(1)),is exact for polynomials of degree at most 2n. In this paper we intend to find a formula which is nearly exact for monomial functions x^j, j=0,1,...,2n+2, instead of being analytically exact for the basis space x^j, j=0,1,...,2n. In this way, several examples are also given to show the numerical superiority of the presented rules with respect to usual Gauss-Radau quadrature rules.',\n",
       " '53e997d7b7602d9701fd0672': 'Designing user interfaces and designing computational software are very different processes. The differences lead to late discovery of design conflicts, which drives up development costs. A unifying methodology that could provide early discovery and resolution of design conflicts must account for the governing principles of both processes. Disciplined long-term investigation of candidate methodologies requires that these governing principles be fixed and that evolving development methods comprising each process be accommodated. This article describes an application of general systems theory to integrate these principles, proposes a process model that fixes them as explicit elements of a process program, argues the feasibility of the model and its worthiness for further study, and describes its initial implementation.',\n",
       " '53e997d7b7602d9701fd066f': \"The recognition of the importance of verification and validation tasks, within the software development process or life cycle, is growing significantly. Still, its unarguably complexity and the great amount of time and resources needed to perform testing properly, together with the industry's unawareness of the most powerful and versatile testing tools, makes that, in practise, these activities are often underestimated and diminished, or just simply ignored and skipped, sometimes due to client's demands or hard time-to-market constraints.Integration testing is a specific kind of testing, which is gathering more and more attention within a software engineering industry that has been for quite some time already relying in structuring application and systems in different modules and components. In this paper, we propose a generic and re-usable model-based methodology for testing integration between different components, and illustrate it using a real case study, LiveScheduler, a scheduler and control tool for transmissions on live broadcast channels through the Internet.\",\n",
       " '53e997d7b7602d9701fd067a': \"The amplification of variable regions of immunoglobulins has become a major challenge in the cloning of antibody genes, whether from hybridoma cell lines or splenic B cells. Using conventional protocols, the heavy-chain variable region genes often are not amplified successfully from the hybridoma cell lines.A novel method was developed to design the degenerated primer of immunoglobulin cDNA and to amplify cDNA ends rapidly. Polymerase chain reaction protocols were performed to recognize the VH gene from the hybridoma cell line. The most highly conserved region in the middle of the VH regions of the Ig cDNA was identified, and a degenerated 5'primer was designed, using our algorithms. The VH gene was amplified by both the 3'RACE and 5'RACE. The VH sequence of CSA cells was 399 bp.The new protocol rescued the amplifications of the VH gene that had failed under conventional protocols. In addition, there was a notable increase in amplification specificity. Moreover, the algorithm improved the primer design efficiency and was shown to be useful both for building VH and VL gene libraries and for the cloning of unknown genes in gene families.\",\n",
       " '53e997d7b7602d9701fd06af': 'We present a new algorithm for tracking the signal subspace recursively. It is based on an interpretation of the signal subspace as the solution of a constrained minimization task. This algorithm, referred to as the constrained projection approximation subspace tracking (CPAST) algorithm, guarantees the orthonormality of the estimated signal subspace basis at each iteration. Thus, the proposed algorithm avoids orthonormalization process after each update for postprocessing algorithms which need an orthonormal basis for the signal subspace. To reduce the computational complexity, the fast CPAST algorithm is introduced which has O(nr) complexity. In addition, for tracking the signal sources with abrupt change in their parameters, an alternative implementation of the algorithm with truncated window is proposed. Furthermore, a signal subspace rank estimator is employed to track the number of sources. Various simulation results show good performance of the proposed algorithms.',\n",
       " '53e997d7b7602d9701fd06e8': \"The availability of virtualization features in modern CPUs has reinforced the trend of consolidating multiple guest operating systems on top of a hypervisor in order to improve platform-resource utilization and reduce the total cost of ownership. However, today's virtualization stacks are unduly large and therefore prone to attacks. If an adversary manages to compromise the hypervisor, subverting the security of all hosted operating systems is easy. We show how a thin and simple virtualization layer reduces the attack surface significantly and thereby increases the overall security of the system. We have designed and implemented a virtualization architecture that can host multiple unmodified guest operating systems. Its trusted computing base is at least an order of magnitude smaller than that of existing systems. Furthermore, on recent hardware, our implementation outperforms contemporary full virtualization environments.\",\n",
       " '53e997d7b7602d9701fd06f7': 'In mobile commerce, a company provides location based services to a set of mobile users. The users report to the company their location with a level of granularity to maintain a degree of anonymity, depending on their perceived risk, and receive in return monetary benefits or better services from the company. This paper formulates a quantitative model in which information theoretic metrics such as entropy, quantify the anonymity level of the users. The individual perceived risks of users and the benefits they obtain are considered to be linear functions of their chosen location information granularity. The interaction between the mobile commerce company and its users are investigated using mechanism design techniques as a privacy game. The user best responses and optimal strategies for the company are derived under budgetary constraints on incentives, which are provided to users in order to convince them to share their private information at the desired level of granularity.',\n",
       " '53e997d7b7602d9701fd06f8': 'Multilabel classification is a relatively recent subfield of machine learning. Unlike to the classical approach, where instances are labeled with only one category, in multilabel classification, an arbitrary number of categories is chosen to label an instance. Due to the problem complexity (the solution is one among an exponential number of alternatives), a very common solution (the binary method) is frequently used, learning a binary classifier for every category, and combining them all afterwards. The assumption taken in this solution is not realistic, and in this work we give examples where the decisions for all the labels are not taken independently, and thus, a supervised approach should learn those existing relationships among categories to make a better classification. Therefore, we show here a generic methodology that can improve the results obtained by a set of independent probabilistic binary classifiers, by using a combination procedure with a classifier trained on the co-occurrences of the labels. We show an exhaustive experimentation in three different standard corpora of labeled documents (Reuters-21578, Ohsumed-23 and RCV1), which present noticeable improvements in all of them, when using our methodology, in three probabilistic base classifiers.',\n",
       " '53e997d7b7602d9701fd0701': 'The idea of minimizing the variance in biased estimation along with controlling the gradient of bias is well established for the case of singular Fisher information matrix (FIM) in order to find the biased estimators. In this paper, the biased Cramer-Rao lower bound (BCRLB) is used to derive and study the estimate of unknown parameters in a linear model with a known twice differentiable additive noise probability density function (PDF). Even if the additive noise is not Gaussian, we show that the derived linear estimators (not unique) are linear functions of the observations (where a constant number is inserted into observation vector) in a particular form. Examples are included to illustrate the estimators performances. We show that a biased estimator obtained by optimization of BCRLB is not necessary satisfactory in a general case; therefore, additional considerations must be taken into account when using this approach. For the case where the PDF of the additive noise is not differentiable, such as uniformly distributed or time invariant magnitude noises, an asymptotical approach is given to find the estimators. As an example, we evaluate the performance of the derived adaptive filter for a first-order Markov time varying system. If the FIM is singular, we use the method of singular value decomposition (SVD) to extract the parameter estimate of the linear models. For example we show that in a linear model, parameter estimation based on single observation leads to the normalized least mean square (NLMS) algorithm. In this example using BCRLB optimization, we find the relation between the step-size of the NLMS algorithm and the bound of the bias gradient matrix.',\n",
       " '53e997d7b7602d9701fd070f': 'In this paper we describe our implementation of a distributed sensor database that was designed to support the activities of teams of mobile robots as they explore an environment. Importantly, this approach effectively separates the process of acquiring sensor data from the process of exploiting it. This allows us to develop applications where robots and human users can automatically discover and utilize sensor measurements acquired by other robots in the team. We also explain our approach to implementing distributed queries, an important capability that allows us to perform queries in a way that makes best use of the limited available communication bandwidth. Finally we briefly describe how we have used the system to support situational awareness tasks. I. INTRODUCTION',\n",
       " '53e997d7b7602d9701fd070d': 'A significant visualization problem in automated cartography and Geographical Information Systems (GIS) is the map-labeling problem, which consists of placing text labels adjacent to geographic features in order to convey their meaning. In this paper, we propose one practical method for labeling line features using discrete model. A text label is treated as a sequence of axis-parallel squares considering the characteristic of Chinese character. The presented method firstly produces a candidate curve and then computes all the candidate positions. Then select labeling positions taking some esthetics criteria into account. This approach fulfills two requirements that no two labels overlap and that the label is placed alongside the corresponding curve. Experimental results illustrate that the new proposed algorithm is effective, implementation-friendly and efficient.',\n",
       " '53e997d7b7602d9701fd0732': 'We discuss the numerical differentiation from the noisy data. We propose a method, i.e., adding a differential term, for the differentiation problem. An error estimate shows that the method allows for the approximate recovery of the derivative function. Some interesting numerical tests display the properties of the proposed method.',\n",
       " '53e997d7b7602d9701fd0711': 'Capacity scaling laws of wireless networks have attracted a lot of attention. In this paper, we study the multicast capacity of bus-assistant VANETs (vehicular ad hoc networks) with two-hop relay scheme, which has not been addressed before. Assume that n ordinary vehicles and nb buses are deployed in a grid-like road framework while the number of roads increase linearly with n. All the ordinary vehicles obey the restricted mobility model. Thus, the spatial stationary distribution decays as power law with the distance from the centre spot (home-point) of a restrict region of each vehicle. All the buses deployed in all roads as intermediate nodes. They are used to forward packets for ordinary vehicles. Each ordinary vehicle randomly chooses k - 1 vehicles from the other ordinary vehicles as receivers. The packets could be transmitted directly from source to destination or be transmitted to an intermediate vehicle or bus, then be forwarded to the destination. We found that the social-proximity urban bus-assisted VANET has three routing methods. For each routing method, we derive the matching asymptotic upper and lower bounds of multicast capacity of bus-assisted VANET.',\n",
       " '53e997d7b7602d9701fd0736': 'We introduce a new approach to named entity classification which we term a Priority Model. We also describe the construction of a semantic database called SemCat consisting of a large number of semantically categorized names relevant to biomedicine. We used SemCat as training data to investigate name classification techniques. We generated a statistical language model and probabilistic context-free grammars for gene and protein name classification, and compared the results with the new model. For all three methods, we used a variable order Markov model to predict the nature of strings not represented in the training data. The Priority Model achieves an F-measure of 0.958--0.960, consistently higher than the statistical language model and probabilistic context-free grammar.',\n",
       " '53e997d7b7602d9701fd074a': 'There is empirical evidence, that the internal quality of software has an important impact on the external, i.e., user perceptible software quality. Our Evaluation Method for Internal Software Quality (EMISQ) -- based on the ISO 14598 standard for the ...',\n",
       " '53e997d7b7602d9701fd073e': 'MapReduce is a currently popular programming model to support parallel computations on large datasets. Among the several existing MapReduce implementations, Hadoop has attracted a lot of attention from both industry and research. In a Hadoop job, map and reduce tasks coordinate to produce a solution to the input problem, exhibiting precedence constraints and synchronization delays that are characteristic of a pipeline communication between maps (producers) and reduces (consumers). We here address the challenge of designing analytical models to estimate the performance of MapReduce workloads, notably Hadoop workloads, focusing particularly on the intra-job pipeline parallelism between map and reduce tasks belonging to the same job. We propose a hierarchical model that combines a precedence graph model and a queuing network model to capture the intra-job synchronization constraints. We first show how to build a precedence graph that represents the dependencies among multiple tasks of the same job. We then apply it jointly with an approximate Mean Value Analysis (aMVA) solution to predict mean job response time, throughput and resource utilization. We validate our solution against a queuing network simulator and a real setup in various scenarios, finding very close agreement in both cases. In particular, our model produces estimates of average job response time that deviate from measurements of a real setup by less than 15 %.',\n",
       " '53e997d7b7602d9701fd0745': 'Research has shown that the application of an attention algorithm to the front-end of an object recognition system can provide a boost in performance over extracting regions from an image in an unguided manner. However, when video imagery is taken from a moving platform, attention algorithms such as saliency can lose their potency. In this paper, we show that this loss is due to the motion channels in the saliency algorithm not being able to distinguish object motion from motion caused by platform movement in the videos, and that an object recognition system for such videos can be improved through the application of image stabilization and saliency. We apply this algorithm to airborne video samples from the DARPA VIVID dataset and demonstrate that the combination of stabilization and saliency significantly improves object recognition system performance for both stationary and moving objects.',\n",
       " '53e997d7b7602d9701fd0757': 'The advent of mobile computing has made power consump- tion a critical design factor. There has been little systematic consideration of the power sources for mobile computing systems. This paper presents a class of system-level metrics intended to make a systematic study more feasible and to more accurately reflect the trade-off between battery life and performance. An example involving the clock fre- quency of a CPU shows the possible impact of the nonlin- earity, an impact not predicted by existing metrics. Results of an initial attempt to verify the example are presented and explained. The paper concludes with drawbacks of the met- ric and possible extensions to overcome these drawbacks.',\n",
       " '53e997d7b7602d9701fd0758': 'We introduce a model for molecular reactions based on probabilistic rewriting rules. We give a probabilistic algorithm for rule applications as a semantics for the model, and we show how a probabilistic transition system can be derived from it. We use the algorithm in the development of an interpreter for the model, which we use to simulate the evolution of molecular systems. In particular, we show the results of the simulation of a real example of enzymatic activity. Moreover, we apply the probabilistic model checker PRISM to the transition system derived by the model of this example, and we show the results of model checking of some illustrative properties.',\n",
       " '53e997d7b7602d9701fd07be': 'This paper describes our second collection of online handwritten character patterns and their analysis. 163 writers presented about 10,000 character patterns, covering 4,438 categories mainly in the context of sentences. Together with our first collection, the Kuchibue database containing 12,000 patterns from 120 writers, we have now collected about 3 million patterns. For this second collection of online patterns, named Nakayosi, we analyzed stroke number and order variations',\n",
       " '53e997d7b7602d9701fd07d1': 'We introduce the concept of partial event as a pair of disjoint sets, respectively the favorable and the unfavorable cases.\\n Partial events can be seen as a De Morgan algebra with a single fixed point for the complement. We introduce the concept of\\n a measure of partial probability, based on a set of axioms resembling Kolmogoroff’s. Finally we define a concept of conditional\\n probability for partial events and apply this concept to the analysis of the two-slit experiment in quantum mechanics.',\n",
       " '53e997d7b7602d9701fd0828': 'We give a review of recent literature on the subject of trust in computational interactions between human and artificial autonomous agents, or between artificial autonomous agents. Trust is a central question in the development of an ethical approach to the design of multi-agent autonomous systems, which must lead to safe and sound interactions, and the development of secure systems based on ethically sound behavior design. The topic treatment in the literature is extensive and from a multitude of points of view, therefore the presented review may be falling short in many regards. We have focused on two main axes to organize the references: computational models and applications. Besides we have focused on various specific topics found in the literature.',\n",
       " '53e997d7b7602d9701fd0822': 'We present a methodology for building timed models of real-time systems by adding time constraints to their application software. The applied constraints take into account execution times of atomic statements, the behavior of the system&#39;s external environment, and scheduling policies. The timed models of the application obtained in this manner can be analyzed by using time analysis techniques to c...',\n",
       " '53e997d7b7602d9701fd0854': 'We present DUE (Distributed Usability Evaluation), a technique for collecting and evaluating usability data. The DUE infrastructure involves a client-server network. A client-based tool resides on the workstation of each user, providing a screen video recording, microphone input of voice commentary, and a window for a severity rating. The idea is for the user to work naturalistically, clicking a button when a usability problem or point of uncertainty is encountered, to describe it verbally along with illustrating it on screen, and to rate its severity. These incidents are accumulated on a server, providing access to an evaluator (usability expert) and to product developers or managers who want to review the incidents and analyse them. DUE supports evaluation in the development stages from running prototypes and onwards. A case study of the use of DUE in a corporate environment is presented. The study indicates that the DUE technique is effective in terms of low bias, high efficiency, and clear communication of usability issues among users, evaluators and developers. Further, DUE is supporting long-term evaluations making possible empirical studies of learnability.',\n",
       " '53e997d7b7602d9701fd083d': 'Traditionally, designers organize software system as active end-points (e.g. applications) linked by passive infrastruc- tures (e.g. networks). Increasingly, however, networks and infrastructures are becoming active components that contrib- ute directly to application behavior. Amongst the various problems that this presents is the question of how such active infrastructures should be programmed. We have been developing an active document management system called Placeless Documents. Its programming model is organized in terms of properties that actively contribute to the functionality and behavior of the documents to which they are attached. This paper discusses active properties and their use as a programming model for active infrastructures. We have found that active properties enable the creation of persistent, autonomous active entities in document systems, independent of specific repositories and applications, but present challenges for managing problems of composition.',\n",
       " '53e997d7b7602d9701fd085a': 'Algebraic specifications provide a formal basis for designing data-structures and reasoning about their properties. Sufficient-completeness and consistency are fundamental notions for building algebraic specifications in a modular way. We give in this paper effective methods for testing these properties for algebraic specifications including conditional axioms.',\n",
       " '53e997d7b7602d9701fd08a1': \"The MiFID is regarded as the core of EU financial market regulation. One of its quintessential features is the best execution of client orders. Best execution is virtually impossible to define; it is rather the definition of a suitable process for best execution compliance. To identify an appropriate approach, we analyze MiFID's requirements and develop a best execution process model with emphasis on minimal requirements to ensure MiFID compliance without real-time information. We also present an approach that extends the process model towards dynamic best execution.\",\n",
       " '53e997d7b7602d9701fd085d': \"Three studies of different mobile-device hand postures are presented. The first study measures the performance of postures in Fitts' law tasks using one and two hands, thumbs and index fingers, horizontal and vertical movements, and front- and back-of-device interaction. Results indicate that the index finger performs well on both the front and the back of the device, and that thumb performance on the front of the device is generally worse. Fitts' law models are created and serve as a basis for comparisons. The second study examines the orientation of shapes on the front and back of a mobile device. It shows that participants' expectations of visual feedback for finger movements on the back of a device reverse the direction of their finger movements to favor a ''transparent device'' orientation. The third study examines letter-like gestures made on the front and back of a device. It confirms the performance of the index finger on the front of the device, while showing limitations in the ability for the index finger on the back to perform complex gestures. Taken together, these results provide an empirical foundation upon which new mobile interaction designs can be based. A set of design implications and recommendations are given based directly on the findings presented.\",\n",
       " '53e997d7b7602d9701fd08ad': 'A class of languages C is perfect if it is closed under Boolean operations and the emptiness problem is decidable. Perfect language classes are the basis for the automata-theoretic approach to model checking: a system is correct if the language generated by the system is disjoint from the language of bad traces. Regular languages are perfect, but because the disjointness problem for CFLs is undecidable, no class containing the CFLs can be perfect. In practice, verification problems for language classes that are not perfect are often under-approximated by checking if the property holds for all behaviors of the system belonging to a fixed subset. A general way to specify a subset of behaviors is by using bounded languages (languages of the form w1* ... wk* for fixed words w1,...,wk). A class of languages C is perfect modulo bounded languages if it is closed under Boolean operations relative to every bounded language, and if the emptiness problem is decidable relative to every bounded language. We consider finding perfect classes of languages modulo bounded languages. We show that the class of languages accepted by multi-head pushdown automata are perfect modulo bounded languages, and characterize the complexities of decision problems. We also show that bounded languages form a maximal class for which perfection is obtained. We show that computations of several known models of systems, such as recursive multi-threaded programs, recursive counter machines, and communicating finite-state machines can be encoded as multi-head pushdown automata, giving uniform and optimal under approximation algorithms modulo bounded languages.',\n",
       " '53e997d7b7602d9701fd08c2': 'As more courseware becomes available, choosing the right functionality for a particular e-learning community is becoming more problematic. Systematic methods for evaluating courseware functionality components in their context of use are required. Of many general methods for ICT evaluation it is unclear how to assess their applicability in the context of courseware. We outline a practical method for courseware evaluation. We experiment with the method by evaluating the courseware functionality used in one core e-learning activity: the making of group assignments. One interesting finding is that the usefulness of an application to a large degree depends on the particular activity being supported, much less on the particular functionality used.',\n",
       " '53e997d7b7602d9701fd08c7': 'We describe a privacy manager for cloud computing, which reduces the risk to the cloud computing user of their private data being stolen or misused, and also assists the cloud computing provider to conform to privacy law. We describe different possible architectures for privacy management in cloud computing; give an algebraic description of obfuscation, one of the features of the privacy manager; and describe how the privacy manager might be used to protect private metadata of online photos.',\n",
       " '53e997d7b7602d9701fd08ce': 'Mixed Group Ranks is a parametric method for combining rank based classifiers that is effective for many-class problems. Its parametric structure combines qualities of voting methods with best rank approaches. In [1] the parameters of MGR were estimated using a logistic loss function. In this paper we describe how MGR can be cast as a probability model. In particular we show that using an exponential probability model, an algorithm for efficient maximum likelihood estimation of its parameters can be devised. While casting MGR as an exponential probability model offers provable asymptotic properties (consistency), the interpretability of probabilities allows for flexiblity and natural integration of MGR mixture models.',\n",
       " '53e997d7b7602d9701fd08d5': 'Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term de- pendencies in music. In this paper, a distributed repre- sentation for chords is designed such that Euclidean dis- tances roughly correspond to psychoacoustic dissimilar- ities. Estimated probabilities of chord substitutions are derived from this representation and are used to introduce smoothing in graphical models observing chord progres- sions. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algo- rithm is used for inference. Various model architectures are compared in terms of conditional out-of-sample like- lihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies.',\n",
       " '53e997d7b7602d9701fd08d6': 'We consider the specification and verification of cyclic (sequential and concurrent) programs. The input-output based concept of correctness traditionally applied to functional programs is replaced by another, based on the concept of eventual behaviour. Various types of eventual behaviour are introduced. In the case of concurrency, the introduction of interface-predicates reduces the proof complexity and achieves greater readability. All specifications use explicitly the auxiliary variables of a location counter ¿ and elapsing time t.',\n",
       " '53e997d7b7602d9701fd08e4': 'We consider minimization of functions that are compositions of prox-regular\\nfunctions with smooth vector functions. A wide variety of important\\noptimization problems can be formulated in this way. We describe a subproblem\\nconstructed from a linearized approximation to the objective and a\\nregularization term, investigating the properties of local solutions of this\\nsubproblem and showing that they eventually identify a manifold containing the\\nsolution of the original problem. We propose an algorithmic framework based on\\nthis subproblem and prove a global convergence result.',\n",
       " '53e997d7b7602d9701fd08f0': \"A method of network synthesis for general combinational functions that uses a number of fixed threshold logic units not greatly in excess of the theoretical minimum is described. It can be used with adaptive logic in which case a minority of the interconnections are simply adapted once according to the rule originally suggested by Hebb. This network on its own has no capacity for ``generalization,'' and if used for pattern recognition must therefore be operated in conjunction with a suitable property filter, such as that of the previous paper.[1] In this case the present network could perform as a trainable ``categorizer.''\",\n",
       " '53e997dcb7602d9701fd0be2': 'Geometric Algebra (GA) is an algebra that encodes geometry much better than standard techniques, which are mainly based on linear algebra with various extensions. Compared to standard techniques, GA has clearer semantics and a richer, more consistent language. This expresses itself, among others, in a much greater genericity of functions over the algebra. Exploiting this genericity efficiently is a problem that can be solved through generative programming.This paper describes our Geometric Algebra Implementation Generator Gaigen 2. Gaigen 2 synthesizes highly efficient GA implementations from the specification of the algebra. Functions over such algebras can be defined in a high-level coordinate-free domain-specific language, and Gaigen 2 transforms these functions into low-level coordinate-based code. This code can be emitted in any target language through a custom back-end. Benchmarks of our implementation show that the combination of GA and Gaigen 2 can rival the performance of standard geometry techniques, despite the greater abstraction and genericity of GA.To obtain this high performance, Gaigen 2 must adapt the generated code to the program that links to it. This is done via a profiling feedback loop. While running, the generated code makes a connection to the code generator. The generated code sends information about functions that should be optimized. The code generator registers this information and sends back new type information. After the program terminates, the code is regenerated according to the recorded profile. This profiling feedback technique may also be useful to implement other types of algebras.',\n",
       " '53e997dcb7602d9701fd0913': 'we present a process model for data bases, which integrates concepts of relation, process, integrity constraint, event and period. The model allows one to specify the synchronization of processes and the constraints due to consumption and production of data. In particular we present the formalization which is carried out with the Petri Nets. These are obtained by using refining primitives which keep the interpretation simple for designers. The aspects of verification and validation of the specification are also examined.',\n",
       " '53e997dcb7602d9701fd0915': 'AOP can be applied to not only modularization of crosscutting concerns but also other kinds of software development processes. As one of the applications, this paper proposes a design traceability mechanism originating in join points and pointcuts. It is not easy to design software architecture reflecting the intention of developers and implement the result of design as a program while preserving the architectural correctness. To deal with this problem, we propose two novel ideas: Archpoint (Architectural point) and Archmapping (Archpoint Mapping). Archpoints are points for representing the essence of architectural design in terms of behavioral and structural aspects. By defining a set of archpoints, we can describe the inter-component structure and the message interaction among components. Archmapping is a mechanism for checking the bidirectional traceability between design and code. The traceability can be verified by checking whether archpoints in design are consistently mapped to program points in code. For this checking, we use an SMT (Satisfiability Modulo Theories) solver, a tool for deciding the satisfiability of logical formulas. The idea of archpoints, program points, and their selection originates in AOP.',\n",
       " '53e997dcb7602d9701fd091d': 'In this paper, we analyze demand postponement as a strategy to handle potential demand surges. Under demand postponement, a fraction of the demands from the \"regular\" period are postponed and satisfied during a \"postponement\" period. This permits capacity to be procured to satisfy the postponed demands. A reimbursement per unit is paid to customers whose demands are postponed. The basic idea is that by preempting stockouts through demand postponement, we can reduce overall stockout costs. We formulate and solve a two-stage capacity planning problem under demand postponement. We propose a power range class of distributions to capture the nature of demand surges. We establish the scalability and conjugate properties of the power range distributions under demand postponement, which leads to a tractable analysis of the problem. We analytically solve the problem of determining the optimal regular and postponement period capacities, and the demand splitting rule to minimize the supplier\\'s expected cost. We show that (a) the value of postponement may be significant depending on cost and demand parameters, (b) a postponement strategy may lead to reduced investment in initial capacity, and (c) it may be optimal to do no demand postponement over a range of demands even after observing a higher demand signal. We then relax several model assumptions and provide results for these extensions. We conclude with managerial insights.',\n",
       " '53e997dcb7602d9701fd0b0e': 'Orthogonal moments such as Zernike moments and Legendre moments have been proven to have superior feature representation capability and low information redundancy. The number of orthogonal moments to be used as features or numerical attributes to perform any application is minimal due to the orthogonal nature. However, the information possessed by each moment order needs to be analysed to identify the appropriate moment orders for the undertaken task. In this work, a statistical significance test has been performed to select the best moment orders to discriminate normal and abnormal tissues in liver images. The experimental results reveal the efficacy of the proposed features. Keywords: Orthogonal moments, Texture, Feature Selection, Classifier',\n",
       " '53e997dcb7602d9701fd0d1d': \"Social advertising uses information about consumers' peers, including peer affiliations with a brand, product, organization, etc., to target ads and contextualize their display. This approach can increase ad efficacy for two main reasons: peers' affiliations reflect unobserved consumer characteristics, which are correlated along the social network; and the inclusion of social cues (i.e., peers' association with a brand) alongside ads affect responses via social influence processes. For these reasons, responses may be increased when multiple social signals are presented with ads, and when ads are affiliated with peers who are strong, rather than weak, ties. We conduct two very large field experiments that identify the effect of social cues on consumer responses to ads, measured in terms of ad clicks and the formation of connections with the advertised entity. In the first experiment, we randomize the number of social cues present in word-of-mouth advertising, and measure how responses increase as a function of the number of cues. The second experiment examines the effect of augmenting traditional ad units with a minimal social cue (i.e., displaying a peer's affiliation below an ad in light grey text). On average, this cue causes significant increases in ad performance. Using a measurement of tie strength based on the total amount of communication between subjects and their peers, we show that these influence effects are greatest for strong ties. Our work has implications for ad optimization, user interface design, and central questions in social science research.\",\n",
       " '53e997dcb7602d9701fd0974': 'In this paper, two main contributions are presented. First, the Behavioral Importance Dual Priority Server (BIDS) reservation mechanism is extended to contemplate more than two importance levels. In fact, an integer-value function is used to postpone the reactivation of the server based on the last result obtained. Second, a resource contention algorithm for BIDS is presented. Although it may seem simple, the resource reservation paradigm may have certain anomalies while sharing resources. The proposal is an extension of the Stack Resource Policy which is formally proved.',\n",
       " '53e997dcb7602d9701fd0d69': 'Built-in test and on-chip calibration features are becoming essential for reliable wireless connectivity of next generation devices suffering from increasing process variations in CMOS technologies. This paper contains an overview of contemporary self-test and performance enhancement strategies for single-chip transceivers. In general, a trend has emerged to combine several techniques involving process variability monitoring, digital calibration, and tuning of analog circuits. Special attention is directed towards the investigation of temperature as an observable for process variations, given that thermal coupling through the silicon substrate has recently been demonstrated as mechanism to monitor the performances of analog circuits. Both Monte Carlo simulations and experimental results are presented in this paper to show that circuit-level specifications exhibit correlations with silicon surface temperature changes. Since temperature changes can be measured with efficient on-chip differential temperature sensors, a conceptual outline is given for the use of temperature sensors as alternative process variation monitors.',\n",
       " '53e997dcb7602d9701fd0997': \"Several works on grid computing have been proposed during the last few years. However, most of them including available software, can not deal properly with some issues related to abstraction and friendliness of grid. A much wider variety of applications and large community of users can get benefit once grid computing technologies become easier to use and more sophisticated. This work concentrates on presenting 'Users-Grid' (pro-middleware), which sits between the grid middleware and user applications. It introduces a high-level abstraction layer and hides the intricacies of the grid middleware. It attempts to make the grid-related aspects transparent to the end users. Its main features are automatic DAG inference and seamless job submission. It facilitates end-users to directly run their applications on grid by themselves without depending on the expertise of support teams.\",\n",
       " '53e997dcb7602d9701fd09b3': 'To meet the high demand for powerful embedded processors,VLIW architectures are increasingly complex (e.g.,multiple clusters), and moreover, they now run increasinglysophisticated control-intensive applications. As a result, developingarchitecture-specific compiler optimizations is becomingboth increasingly critical and complex, while time-to-market constraints remain very tight.In this article, we present a novel program optimizationapproach, called the Virtual Hardware Compiler (VHC),that can perform as well as static compiler optimizations,but which requires far less compiler development effort,even for complex VLIW architectures and complex targetapplications. The principle is to augment the target processorsimulator with superscalar-like features, observe howthe target program is dynamically optimized during execution,and deduce an optimized binary for the static VLIWarchitecture. Developing an architecture-specific optimizerthen amounts to modifying the processor simulator whichis very fast compared to adapting static compiler optimizationsto an architecture. We also show that a VHC-optimizedbinary trained on a number of data sets performs as wellas a statically-optimized binary on other test data sets. Theonly drawback of the approach is a largely increased compilationtime, which is often acceptable for embedded applicationsand devices. Using the Texas Instruments C62 VLIWprocessor and the associated compiler, we experimentallyshow that this approach performs as well as static compileroptimizations for a much lower research and developmenteffort. Using a single-core C60 and a dual-core clusteredC62 processors, we also show that the same approach canbe used for efficiently retargeting binary programs within afamily of processors.',\n",
       " '53e997dcb7602d9701fd09d3': 'Two finite automata are devised for modeling two classes of demand paging algorithms. The first one of one input and three outputs models the class of algorithms with a constant amount of allocated space. The second one of one input and six outputs models the class of algorithms with a variable amount of allocated space. Some evaluation techniques are developed following each model. The memory states of the first class algorithm with the Least Recently Used (LRU) replacement policy and the working set model of the second class are recursively defined by strings of the loaded pages. The adopted replacement policy and the state string updating procedure are imbedded in the recursive definition of memory states. Properties of some algorithms are developed to fit the finiteness assumption of a reference string.',\n",
       " '53e997dcb7602d9701fd0cda': 'We present a novel visual search system that deals with scalability, is fast enough for commercial applications, and ad dresses limitations present in current visual search engines. Most scalable visual search approaches rely on local features, the Bag of Visual Words representation, and a ranking mechanism based on some vector space model [1, 2]. However, since in those methods the initial rankings do not take into account any spatial information, they are not well suited to identify multiple small objects \"buried\" within complex scenes. To alleviate this limitation we propose to perform the initial ranking using clustering of matches in a limited pose space. We also describe its smooth integration with Soft Assignment of Visual Words and RANS AC-inspired spatial consistency verification. We demonstrate that our system addresses the problem and show the use of the method in several commercially attractive applications.',\n",
       " '53e997dcb7602d9701fd0daa': \"Multiple Display Environments (MDEs) facilitate collaborative activities that involve the use of electronic task artifacts. Supporting interactions and infrastructures have matured in recent years, allowing researchers to now study how the use of MDEs impacts group work in controlled and authentic settings. This has created a need for tools to understand and make sense of the resulting interaction data. To address this need, we have designed and developed a new interactive analysis tool called VICPAM. Our tool reduces the effort necessary to analyze and make sense of users' interaction data in MDEs. VICPAM consists of several components: (i) a time-aligned view, which shows users' activities over time and the duration of each activity; (ii) A spatial view, which gives a 2D overview of all users' activities in the environment; (iii) A time-bar, which allows selection of a desired time period for in-depth analysis; and (iv) a video player, which allows the user to watch a video of the session synchronized with the selected period of time.\",\n",
       " '53e997dcb7602d9701fd0a49': \"This paper presents a nonlinear optimal control technique based on approximating the solution to the Hamilton-Jacobi-Bellman (HJB) equation. The HJB solution (value function) is approximated as the output of a radial basis function neural network (RBFNN) with unknown parameters (weights, centers, and widths) whose inputs are the system's states. The problem of solving the HJB equation is therefore converted to estimating the parameters of the RBFNN. The RBFNN's parameters estimation is then recognized as an associated state estimation problem. An adaptive extended Kalman filter (AEKF) algorithm is developed for estimating the associated states (parameters) of the RBFNN. Numerical examples illustrate the merits of the proposed approach.\",\n",
       " '53e997dcb7602d9701fd142b': 'One main idea when developing user interfaces for digital and hybrid libraries is to make use of real-world metaphors. This gives library customers the advantage to access digital collections the same way as they would traditional collections. However, while existing \"real world\" library interfaces still miss the attraction of the wider public, game industry is very successful selling virtual reality games. In this paper we describe a study how to close the gap between these two worlds. We describe the development of a library interface that bases on a commercial computer game. The interface models the interior and exterior of an existing library building including the most important functions for literature search. Not only teenagers tested the developed prototype with big attention.',\n",
       " '53e997dcb7602d9701fd1442': 'We consider the problem of computing the second elementary symmetric\\npolynomial S^2_n(X) using depth-three arithmetic circuits of the form \"sum of\\nproducts of linear forms\". We consider this problem over several fields and\\ndetermine EXACTLY the number of multiplication gates required. The lower bounds\\nare proved for inhomogeneous circuits where the linear forms are allowed to\\nhave constants; the upper bounds are proved in the homogeneous model. For reals\\nand rationals, the number of multiplication gates required is exactly n-1; in\\nmost other cases, it is \\\\ceil{n/2}. This problem is related to the\\nGraham-Pollack theorem in algebraic graph theory. In particular, our results\\nanswer the following question of Babai and Frankl: what is the minimum number\\nof complete bipartite graphs required to cover each edge of a complete graph an\\nodd number of times? We show that for infinitely many n, the answer is\\n\\\\ceil{n/2}.',\n",
       " '53e997dcb7602d9701fd0be9': 'In this paper, we propose a novel feature localization method based on a global vector concentration approach. Our approach does not rely on the detection of local salient features around feature points. Instead, we exploit global structural information of the object extracted by calculat- ing the concentration of directional vectors from sampling points. Those vectors are combined with local pattern de- scriptors of a query image and selected from preliminar- ily trained extended templates by nearest neighbor search. Due to the insensitivity of local changes, our method can handle partially occluded and noisy objects. We apply the proposed method to fully automatic feature localization of the left ventricular in echocardiograms. The results show the effectiveness of our method in comparison with a con- ventional edge-based method in terms of accuracy and ro- bustness.',\n",
       " '53e997dcb7602d9701fd0cf6': 'To annotate voice onset time (VOT) of stop consonants in a speech database, manually labeling is a feasible but time-consuming and tedious task. This paper proposed a fully-automatic VOT estimation method to alleviate this burden. The method relies on an HMM-based phone recognizer and a random forest (RF) based onset detector. The phone recognizer performs a forced alignment to locate stop consonants, and the onset detector searches each aligned stop segment for the onsets of burst and voicing. Then the time interval between these onsets is the estimated VOT for that stop consonant. The merit of the proposed method lies in the RF-based onset detector, which is able to provide accurate onset detection with only a small amount of training data. The proposed method was evaluated on the testing set in TIMIT database, which includes 2,344 word-initial and 1,440 word-medial stops. The experimental results revealed that 81.2% of the estimations deviate from the reference values within 10 ms, and 95.7% within 20 ms.',\n",
       " '53e997dcb7602d9701fd14f6': 'This paper presents pipelined implementation of a real time programmable irregular Low Density Parity Check (LDPC) Encoder as specified in the IEEE P802.16E/D7 standard. The encoder is programmable for frame sizes from 576 to 2304 and for five different code rates. H matrix is efficiently generated and stored for a particular frame size and code rate. The encoder is implemented on Reconfigurable Instruction Cell Architecture which has recently emerged as an ultra low power, high performance, ANSI-C programmable embedded core. Different general and architecture specific optimization techniques are applied to enhance the throughput. With the architecture, a throughput from 10 to 19 Mbps has been achieved. The maximum throughput achieved with pipelining/ multi-core is 78 Mbps.',\n",
       " '53e997dcb7602d9701fd14f9': ',,Abstract  Expert systems(ES) and database systems(DBS) are major components,of information,systems and important assets to companies. The development,ofthese systems represent users’ knowledge,in the application systems. As computer technologies evolve, and as users requirements change, there is a need to upgrade these system to meet the new application requirements. To preserve the knowledge ofthe existing  information  systems,  a  methodology  for  integrating  ES  and  DBS  into  an  expert  database system(EDS) is proposed. The integrated EDS is a knowledge,based system(KBS) which derives and stores knowledge in a frame model consisting of a class header, attributes, methods and constraints. It extracts the ES rules and DBS data for an application into coupling classes at run time only. The attributes of the coupling classes are matched,with synonyms,in a synonym,table which resolves their  naming ,and  semantic  conflicts  with  user  assistance  in  knowledge ,acquisition.  The  resultant EDS is a KBS ready for application development.   Keywords: expert systems, database systems, expert database systems, knowledge based systems, frame model. ,3',\n",
       " '53e997dcb7602d9701fd1505': 'Let be the largest integer such that every convex polygon with vertices and sides has a vertex such that the next vertices clockwise from, or the next vertices counterclockwise from, are successively farther from. We prove that/3]+1 for≥4. An example gives/3]+1, and an extension of a 1952 construction of Leo Moser for a related planar problem shows that/3]+1.',\n",
       " '53e997dcb7602d9701fd1509': \"This study investigates the impact of anonymous, computerized, synchronized team competition on students' motivation, satisfaction, and interpersonal relationships. Sixty-eight fourth-graders participated in this study. A synchronous gaming learning system was developed to have dyads compete against each other in answering multiple-choice questions set in accordance with the school curriculum in two conditions (face-to-face and anonymous). The results showed that students who were exposed to the anonymous team competition condition responded significantly more positively than those in the face-to-face condition in terms of motivation and satisfaction at the 0.050 and 0.056 levels respectively. Although further studies regarding the effects of anonymous interaction in a networked gaming learning environment are imperative, the positive effects detected in this preliminary study indicate that anonymity is a viable feature for mitigating the negative effects that competition may inflict on motivation and satisfaction as reported in traditional face-to-face environments.\",\n",
       " '53e997dcb7602d9701fd1536': 'Programmable many-core processors are poised to become a major design option for many embedded applications. In the design of power-efficient embedded many-core processors, the architecture of the on-chip network plays a central role. Many designs have relied on a 2D mesh architecture as the underlying communication fabric. With the emergence of 3D technology, new on-chip network architectures are possible. In this paper, we propose a novel layer-multiplexed (LM) 3D network architecture that takes advantage of the short interlayer wiring delays enabled in 3D technology. In particular, the LM architecture replaces the one-layer-per-hop routing in a conventional 3D mesh with simpler vertical demultiplexing and multiplexing structures. When combined with a layer load-balanced oblivious routing algorithm, it can achieve the same worst-case throughput as the best known oblivious routing algorithm on a conventional 3D mesh. However, in comparison to a conventional 3D mesh, the LM architecture consumes 27% less power, attains 14.5% higher average throughput, and achieves 33% lower worst-case hop count on a 4 times 4 times 4 topology.',\n",
       " '53e997dcb7602d9701fd1568': 'In recent years, graphical modelling and simulation en- vironments have become increasingly popular for the de- velopment of automotive control systems. However, the power and high degree of freedom offered by these tools can also lead to problems if not used properly, especially in large projects. Guidelines can help to establish a consis- tent modelling style throughout a project and thus improve readability, understandability and maintainability of the re- sulting product. This paper presents general considerations about guidelines and presents a collection of patterns for modelling control-flow in STATEFLOW.',\n",
       " '53e997dcb7602d9701fd158a': 'In this paper, we introduce the Priberam Compressive Summarization Corpus, a new multi-document summarization corpus for European Portuguese. The corpus follows the format of the summarization corpora for English in recent DUC and TAC conferences. It contains 80 manually chosen topics referring to events occurred between 2010 and 2013. Each topic contains 10 news stories from major Portuguese newspapers, radio and TV stations, along with two human generated summaries up to 100 words. Apart from the language, one important difference from the DUC/TAC setup is that the human summaries in our corpus are compressive: the annotators performed only sentence and word deletion operations, as opposed to generating summaries from scratch. We use this corpus to train and evaluate learning-based extractive and compressive summarization systems, providing an empirical comparison between these two approaches. The corpus is made freely available in order to facilitate research on automatic summarization.',\n",
       " '53e997dcb7602d9701fd15bb': 'Many forms of organizational memory must exist embedded within the organizational processes and tasks. This paper argues that \"memory-in-the small\", memory utilized in the performance of an organizational task, can serve as an effective performance support mechanism. By basing organizational memory upon organizational tasks (and basing task support upon organizational memory), organizational memory systems can provide additional and necessary support services for organizations and communities. As an example of memory-in-the-small, this paper describes a software application called the ASSIST, that combines organizational memory with task performance for a scientific community. The ASSIST utilizes and stores the collective memory of astrophysicists about data analysis and is used world-wide by astrophysicists. The paper also considers the theoretical and architectural issues involved when combining organizational memory with task performance.',\n",
       " '53e997dcb7602d9701fd160a': 'Workflow provides a promising solution for organizations to achieve their business goals by interactions and collaborations between Web services. Access control is an important security mechanism to protect the resources to be only accessed by authorized users in such collaborative environments. In this paper, we aim at developing a method for formalizing and analyzing workflow access control in Web service context. To achieve this goal, we first present WSPI, Web Service Pi calculus, to formalize Web services and workflow processes. Based on WSPI, a type system is proposed to ensure that the specified TBAC policy is respected during system reductions. By subject reduction, the well-typed system can guarantee the system security and avoid access violations in run time.',\n",
       " '53e997dcb7602d9701fd162f': 'The Base Station (BS) sleeping strategy has become a well-known technique to achieve energy savings in cellular networks by switching off redundant BSs mainly for lightly loaded networks. Besides, the exploitation of renewable energies, as additional power sources in smart grids, becomes a real challenge to network operators to reduce power costs. In this paper, we propose a method based on genetic algorithms that decreases the energy consumption of a Long-Term Evolution (LTE) cellular network by not only shutting down underutilized BSs but also by optimizing the amounts of energy procured from the smart grid without affecting the desired Quality of Service.',\n",
       " '53e997dcb7602d9701fd163d': 'This paper presents an equational calculus to reason about bidirectional transformations specified in the point-free style. In particular, it focuses on the so-called lenses as a bidirectional idiom, and shows that many standard laws characterising point-free combinators and recursion patterns are also valid in that setting. A key result is that uniqueness also holds for bidirectional folds and unfolds, thus unleashing the power of fusion as a program optimisation technique. A rewriting system for automatic lens optimisation is also presented, to prove the usefulness of the proposed calculus.',\n",
       " '53e997dcb7602d9701fd1647': 'Recently, dynamic behaviors in embedded system for ubiquitous environment and service-oriented system have needed a dynamic coupling metric to evaluate the quality of the software system more accurately. The embedded systems are designed by a component-based system and the component-based system is designed by one object-oriented. Therefore, coupling between the classes in the object-oriented model are brought into relief more clearly. But, in spite of a great deal of study and effort, most coupling metrics between classes suggest metrics that measure the coupling based on a static dependency relationship. In this paper, we propose a dynamic coupling metric in order to measure the coupling accurately between classes allowing the dynamic property in the object level. In addition, we prove the theoretical soundness of the proposed metric by the axioms of Briand et al. and suggest the accuracy of the proposed metric through a comparison with conventional metrics.',\n",
       " '53e997dcb7602d9701fd1667': 'This paper presents a fast algorithm for labeling connected components in binary images based on sequential local operations. A one-dimensional table, which memorizes label equivalences, is used for uniting equivalent labels successively during the operations in forward and backward raster directions. The proposed algorithm has a desirable characteristic: the execution time is directly proportional to the number of pixels in connected components in an image. By comparative evaluations, it has been shown that the efficiency of the proposed algorithm is superior to those of the conventional algorithms.',\n",
       " '53e997dcb7602d9701fd1694': \"The mobile RFID (Radio Frequency Identification) is a new application to use mobile phone as RFID reader with a wireless technology and provides new valuable services to user by integrating RFID and ubiquitous sensor network infrastructure with mobile communication and wireless internet. However, there are an increasing number of concerns, and even some resistances, related to consumer tracking and profiling using RFID technology. Therefore, in this paper, we describe the secure application portal service framework leveraging globally mobile RFID based on web service which complies with the Korea's mobile RFID forum standard.\",\n",
       " '53e997dcb7602d9701fd16c3': 'Eifel and F-RTO have been reported representative solutions for spurious TCP timeout. However, these algorithms do not work well in case that delay spike and packet loss occur simultaneously. In this paper, we propose ER-SRTO algorithm which detects the packet loss during delay spike and conducts an efficient recovery in the RTO procedure based on ACK sequence number of retransmission packet. Proposed scheme recovers the lost packet without invoking additional loss recovery procedure. Simulation result shows that proposed algorithm outperforms previous ones.',\n",
       " '53e997dcb7602d9701fd17b6': 'Traditional workload management methods mainly focus on the current system status while information about the interaction between queued and running transactions is largely ignored. An exception to this is the transaction reordering method, which reorders the transaction sequence submitted to the RDBMS and improves the transaction throughput by considering both the current system status and information about the interaction between queued and running transactions. The existing transaction reordering method only considers the reordering opportunities provided by analyzing the lock conflict information among multiple transactions. This significantly limits the applicability of the transaction reordering method. In this paper, we extend the existing transaction reordering method into a general transaction reordering framework that can incorporate various factors as the reordering criteria. We show that by analyzing the resource utilization information of transactions, the transaction reordering method can also improve the system throughput by increasing the resource sharing opportunities among multiple transactions. We provide a concrete example on synchronized scans and demonstrate the advantages of our method through experiments with a commercial parallel RDBMS.',\n",
       " '53e997dcb7602d9701fd17f7': 'This introduction provides an overview of the state-of-the-art technology in Applications of Natural Language to Information Systems. Specifically, we analyze the need for such technologies to successfully address the new challenges of modern information systems, in which the exploitation of the Web as a main data source on business systems becomes a key requirement. It will also discuss the reasons why Human Language Technologies themselves have shifted their focus onto new areas of interest very directly linked to the development of technology for the treatment and understanding of Web 2.0. These new technologies are expected to be future interfaces for the new information systems to come. Moreover, we will review current topics of interest to this research community, and will present the selection of manuscripts that have been chosen by the program committee of the NLDB 2011 conference as representative cornerstone research works, especially highlighting their contribution to the advancement of such technologies.',\n",
       " '53e997dcb7602d9701fd1816': 'This paper describes two new three dimensional interface components (The Flow and Circulatory system). These components utilize the depth provided by 3D computer graphics to present complex information in a natural three dimensional form for user interaction. Part of a larger research project with the objective of applying 3D computer graphics to the field of human computer interfaces, this research focuses mainly on the content of the 3D space and how users utilize and interact with that content, rather than physical device related issues. Each of the new 3D interface components is designed for a particular mainstream real world interaction task (eg. web search/browsing activities). In addition to the specific components it introduces the concept of \"active 3D interfaces\", a new style of interface that presents its data to the user rather than statically waiting for the user to interact with it. Each interface is described in terms of its design, function and performance in user trials. These trials clearly demonstrate the potential for active 3D interfaces in a range of common interaction tasks.',\n",
       " '53e997dcb7602d9701fd181d': 'In recent years, numerous studies have been attempted to exploit ontology in the area of ubiquitous computing. Especially, some kinds of ontologies written in OWL are proposed for major issues in ubiquitous computing such like context-awareness. OWL is recommended by W3C as a descriptive language for representing ontology with rich vocabularies. However, developers struggle to design ontology using OWL, because of the complex syntax of OWL. The research for OWL visualization aims to overcome this problem, but most of the existing approaches unfortunately do not provide efficient interface to visualize OWL ontology. Moreover, as the size of ontology grows bigger, each class and relation are difficult to represent on the editing window due to the small size limitation of screen. In this paper, we present OWL visualization scheme that supports class information in detail. This scheme is based on concept of social network, and we implement OWL visualization plug-in on Protégé that is the most famous ontology editor.',\n",
       " '53e997dcb7602d9701fd1957': 'Traditional rule-based classifiers training with Genetic Algorithms have their major weaknesses in the classification accuracy and training time. To resolve these drawbacks, this paper reviews Recursive Learning of Genetic Algorithm with Task Decomposition and Varied Rule Set (RLGA) and proposes its variation that features Incremental Attribute Learning (RLGA-I). Experiments show that both the proposed solutions dramatically reduce the training duration with better generalization accuracy.',\n",
       " '53e997dcb7602d9701fd1959': 'Monoidal triangular norm logic MTL is the logic of left-continuous triangular norms. In the paper we present a relational formalization of the logic MTL and then we introduce relational dual tableau that can be used for verification of validity of MTL-formulas. We prove soundness and completeness of the system.',\n",
       " '53e997dcb7602d9701fd1888': 'Stereo imaging of the optic-disc is a gold standard examination of glaucoma, and progression of glaucoma can be detected from temporal stereo images. A Java-based software system is reported here which automatically aligns the left and right stereo retinal images and presents the aligned images side by side, along with the anaglyph computed from the aligned images. Moreover, the disparity between two aligned images is computed and used as the depth cue to render the optic-disc images, which can be interactively edited, panned, zoomed, rotated, and animated, allowing one to examine the surface of the optic-nerve head from different view angles. Measurement including length, area, and volume of regions of interest can also be performed interactively.',\n",
       " '53e997dcb7602d9701fd19ea': 'The development of powerful and low-cost hardware devices allied with great advances on content editing and authoring tools have promoted the creation of computer generated images (CG) to a degree of unrivaled realism. Differentiating a photo-realistic computer generated image from a real photograph (PG) can be a difficult task to naked eyes. Digital forensics techniques can play a significant role in this task. As a matter of fact, important research has been made by the scientific community in this regard. Most of the approaches focus on single image features aiming at detecting differences between real and computer generated images. However, with the current technology advances, there is no universal image characterization technique that completely solves this problem. In our work, we (1) present a complete study of several CG versus PG approaches; (2) create a large and heterogeneous dataset to be used as a training and validation database; (3) implement representative methods of the literature; and (4) devise automatic ways for combining the best approaches. We compared the implemented methods using the same validation environment showing their pros and cons with a common benchmark protocol. We collected approximately 4850 photographs and 4850 CGs with large diversity of image content and quality. We implemented a total of 13 methods. Results show that this set of methods can achieve up to 93% of accuracy when used without any form of machine learning fusion. The same methods, when combined through the implemented fusion schemes, can achieve an accuracy rate of 97%, representing a reduction of 57% of the classification error over the best individual result.',\n",
       " '53e997dcb7602d9701fd1a56': 'We explore the price dynamics of a vertically differentiated market in which two or more sellers compete to provide an information good or service to a population of buyers. Each seller offers the good or service at a fixed level of “quality”, and attempts to set its price in such a way that it maximizes its own profit. Five different seller pricing strategies, ranging widely from ones that require perfect knowledge and unlimited computational power to ones that require very little knowledge or computational capability, are employed in two different buyer populations. The resulting collective dynamics are studied using a combination of analysis and simulation. In a population of quality-sensitive buyers, all pricing strategies lead to a price equilibrium predicted by a game-theoretic analysis. However, in a population of price-sensitive buyers, most pricing strategies lead to large-amplitude cyclical price wars. The circumstances under which cyclical price wars occur can be explained in terms of the topology of an underlying “profit landscape” [J.O. Kephart, J.E. Hanson, J. Sairamesh, Price and niche wars in a free-market economy of software agents, Artificial Life Journal, 4(1), 1998, 1–23].',\n",
       " '53e997dcb7602d9701fd1b55': 'We present several formal program refinement rules for de- signing multi-tasking programs with hard real-time constraints.',\n",
       " '53e997dcb7602d9701fd1a66': 'We define a probabilistic morphological analyzer using a data-driven approach for Syriac in order to facilitate the creation of an annotated corpus. Syriac is an under-resourced Semitic language for which there are no available language tools such as morphological analyzers. We introduce novel probabilistic models for segmentation, dictionary linkage, and morphological tagging and connect them in a pipeline to create a probabilistic morphological analyzer requiring only labeled data. We explore the performance of models with varying amounts of training data and find that with about 34,500 labeled tokens, we can outperform a reasonable baseline trained on over 99,000 tokens and achieve an accuracy of just over 80%. When trained on all available training data, our joint model achieves 86.47% accuracy, a 29.7% reduction in error rate over the baseline.',\n",
       " '53e997dcb7602d9701fd1a7c': 'Although many outdoor location techniques have been proposed, there is no cost-effective indoor location technique. This paper presents a practical indoor location technique applicable to all 2G and 3G cellular/PCS indoor systems. Two typical indoor cellular/PCS architectures are considered. Recommendation for how to deploy the indoor location system with respect to each architecture is given. Simulation has been performed to validate the proposed concept. Also discussed is how to design a cost-effective indoor base station considering location service as one of the critical requirements.',\n",
       " '53e997dcb7602d9701fd1a82': 'The expanding mobile market has boosted the significance of handover management techniques. As cell sizes tend to decrease, the influx of handovers on each cell increases; bringing handover management techniques into the limelight. Consequently, handovers become one of the crucial assessing factors for the performance evaluation of cellular mobile communication systems. In order to evaluate the performance of handover management techniques, two different queuing models were designed: First Come First Serve (FCFS) and Signal Strength Priority (SSP) queues. In case of FCFS, the handovers are served in the sequence of their arrival, whereas in SSP they are served in accordance to their signal strength. Computer simulations were developed to analyze the behavior of the two models. In order to validate the results, Markov analytical models were designed and programmed. This paper presents an in-depth comparative study of the queuing models using different Quality of Service (QoS) parameters.',\n",
       " '53e997dcb7602d9701fd1be6': 'We propose community structure based node scores for network immunization. Since epidemics (e.g, virus) are propagated among groups of nodes (communities) in a network, network immunization has often been conducted by removing nodes with large score (e.g., centrality) so that the major part of the network can be protected from the contamination. Since communities are often interwoven through intermediating nodes, we propose to identify such nodes based on the community structure of a network. By regarding the community structure in terms of nodes, we construct a vector representation of each node based on a quality measure of communities for node partitioning. Two types of node score are proposed based on the direction and the norm of the constructed node vectors.',\n",
       " '53e997dcb7602d9701fd1c36': 'This paper presents a portable Multi-Angle Observation System (MAOS) to quickly collect bi-directional reflectance factor (BRF) and directional thermal radiance of land surface along with the spectroradiometer and thermal radiometer. The new system is able to make more than 13 zenith measurements in six minutes at an arbitrary azimuth direction, with the angle-controlling accuracy better than 2°. More observations are sampled in the hot-spot direction. All operations of the MAOS and data-processing are automatically controlled by the computer. Field campaign of winter wheat canopy shows that the MAOS had captured the angular variations of the BRF.',\n",
       " '53e997dcb7602d9701fd1b38': 'High speed communication systems play an important role in the development of advanced networks. The optimum design of these communication links under bandwidth and power constraints needs the use of efficient approaches. In this paper, a novel power efficient modulation scheme is introduced. For this purpose, the modulator at the transmitter uses simple and effective pulse shaping functions to change the amplitude and width of the transmitter pulse. Using this simple method, a high power efficient transceiver structure, with the same performance as typical NRZ signaling, can be designed. Our results show that not only we can reduce the power consumption by more than 10% without loosing the performance but also the probability of error is improved in the presence of jitter and interference.',\n",
       " '53e997dcb7602d9701fd1c43': 'Model Driven Software Development (MDSD) has matured over the last few years and is now becoming an established technology. Models are used in various contexts, where the possibility to perform different kinds of analyses based on the modelled applications is one of these potentials. In different use cases during these analyses it is necessary to detect patterns within large models. A general analysis technique that deals with lots of data is pattern mining. Different algorithms for different purposes have been developed over time. However, current approaches were not designed to operate on models. With employing QVT for matching and transforming patterns we present an approach that deals with this problem. Furthermore, we present an idea to use our pattern mining approach to estimate the maintainability of modelled artifacts.',\n",
       " '53e997dcb7602d9701fd1c4c': 'The present paper aims to establish an approach to roughness with mereological relations between information granules rather than common set theoretic relations. With Atomic Granule, the minimum information unit of an information system is encapsulated, which is a triple encoding the semantics \"an entity has the relevant attribute of specified value\". Based on it, a granular calculus is built up. Using the granular calculus, according to underlying mereological relations, lower and upper granule approximation is defined to achieve roughness over granular representation of Information System.',\n",
       " '53e997dcb7602d9701fd1c55': 'We propose abc-boost (adaptive base class boost) for multi-class classification and present abc-mart, an implementation of abc-boost, based on the multinomial logit model. The key idea is that, at each boosting iteration, we adaptively and greedily choose a base class. Our experiments on public datasets demonstrate the improvement of abc-mart over the original mart algorithm.',\n",
       " '53e997dcb7602d9701fd1bb2': 'Varying illumination and partial occlusion are two main difficulties in visual tracking. Existing methods based on appearance information cannot solve these problems effectively since appearance is sensitive to lighting and the appearances under occlusions are quite different. In this paper, we propose a descriptor-based dynamic tracking approach that can track objects under partial occlusions and varying illumination. Instead of global appearance, an object is represented by a set of invariant feature descriptors that are generated from local regions around some salient points. By integrating the local descriptor information into the observation model, our method is effective under varying illumination and partial occlusions.',\n",
       " '53e997ddb7602d9701fd1f85': 'In addition to robustness and fragility, security is quite an important issue in media authentication systems. This paper first examines the insecurity of several block-based authentication methods under counterfeit attacks. Then, we prove that the proposed digital signature, which is composed of structural information, is content-dependent and provides security against forgery attacks. Experimental results demonstrate the benefits of exploiting structural information in a media authentication system.',\n",
       " '53e997ddb7602d9701fd1faf': 'This paper presents a model, called the StoryToCode, which allows the specication of IDTV programs with focus on the use of software components. First, the StoryToCode allows the transformation of a storyboard in an abstract description of an element set that make up the storyboard. After this, the StoryToCode transform these elements in a specific programing language source code. In StoryToCode a software component is treated as a special element that can be reused in other contexts (web, mobile and etc). The StoryToCode is based on MDA (Model Driven Architecture) and allows design and implement of an application, independent of context, taking into account the particularities of an IDTV program.',\n",
       " '53e997ddb7602d9701fd1fba': 'A real-time operating system RTOS provides a platform for the design and implementation of a wide range of applications in real-time systems, embedded systems, and mission-critical systems. This paper presents a formal design model for a general RTOS known as RTOS+ that enables a specific target RTOS to be rigorously and efficiently derived in real-world applications. The methodology of a denotational mathematics, Real-Time Process Algebra RTPA, is described for formally modeling and refining architectures, static behaviors, and dynamic behaviors of RTOS+. The conceptual model of the RTOS+ system is introduced as the initial requirements for the system. The architectural model of RTOS+ is created using RTPA architectural modeling methodologies and refined by a set of Unified Data Models UDMs. The static behaviors of RTOS+ are specified and refined by a set of Unified Process Models UPMs. The dynamic behaviors of the RTOS+ system are specified and refined by the real-time process scheduler and system dispatcher. This work is presented in two papers in serial due to its excessive length. The static and dynamic behavioral models of RTOS+ is described in this paper; while the conceptual and architectural models of RTOS+ has been published in IJSSCI 22.',\n",
       " '53e997ddb7602d9701fd1fca': 'The flying mechanism of birds and big insects, especially the rules of wings motion in flight, are investigated, and some details of mechanical frame are also considered. The entire dynamic model of flight attitude of flapping wing micro aerial vehicle (FWMAV) is developed. The design of attitude controller is challenging due to the complexity of the flight process, and the heavy difficulty includes the system uncertainty, nonlinearity, multi-variable coupled parameters, and all kinds of disturbances. To control the attitude movement effectively, an adaptive robust control scheme is proposed to decompose the system into nominal model, structured uncertainties, and unstructured uncertainty. For them, direct feedback controller, adaptive controller, and robust controller are constructed respectively. Simulation results are presented to verify the validity of the dynamic model and the control strategy.',\n",
       " '53e997ddb7602d9701fd1fce': 'Energy-efficient realization of soft-output signal detection is of great importance in emerging high-speed multiple-input multiple-output (MIMO) wireless communication systems. This paper presents three algorithm-level complexity-reduction techniques for soft-output detector design to achieve significant energy savings. To demonstrate their effectiveness, we designed a soft-output detector for 4-4 MIMO with 64-QAM using 65nm CMOS technology. While achieving near-optimum detection performance, the detector can support over 100Mbps throughput with only 0.24mm2 silicon area and 11mw power, leading to a ×10 improvement over the state of the art.',\n",
       " '53e997ddb7602d9701fd1ff4': 'We present efficient deterministic parallel algorithmic techniques for solving geometric problems in BSP like coarse-grain network models. Our coarse-grain network techniques seek to achieve scalability and minimization of both the communication time and local computation time. These techniques enable us to solve a number of geometric problems in the plane, such as computing the visibility of non-intersecting line segments, computing the convex hull, visibility, and dominating maxima of a simple polygon, two-variable linear programming, determination of the monotonicity of a simple polygon, computing the kernel of a simple polygon, etc. Our coarse-grain algorithms represent theoretical improvement over previously known results, and take into consideration additional practical features of coarse-grain network computation.',\n",
       " '53e997dcb7602d9701fd1ec7': 'One of the Agile principles is that software development teams should regularly reflect on how to improve their practices to become more effective. Some systematic approaches have been proposed on how to conduct such a self-reflection process, but little empirical evidence yet exits. In this paper, the empirical results are reported from two XP (Extreme Programming) projects where the project teams conducted \"post-iteration workshops\" after all process iterations in order to improve and optimize working methods. Both qualitative and quantitative data from the total of eight post-iteration workshops is presented in order to evaluate and compare the findings of the two projects. The results show the decline of both positive and negative findings, as well as the narrower variation of negative findings and process improvement actions towards the end of both projects. In both projects, the data from post-iteration workshops indicate increased satisfaction and learning of project teams.',\n",
       " '53e997ddb7602d9701fd200b': 'Multi-user multiple-input multiple-output theory predicts manyfold capacity gains by leveraging many antennas on wireless base stations to serve multiple clients simultaneously through multi-user beamforming (MUBF). However, realizing a base station with a large number antennas is non-trivial, and has yet to be achieved in the real-world. We present the design, realization, and evaluation of Argos, the first reported base station architecture that is capable of serving many terminals simultaneously through MUBF with a large number of antennas (M >> 10). Designed for extreme flexibility and scalability, Argos exploits hierarchical and modular design principles, properly partitions baseband processing, and holistically considers real-time requirements of MUBF. Argos employs a novel, completely distributed, beamforming technique, as well as an internal calibration procedure to enable implicit beamforming with channel estimation cost independent of the number of base station antennas. We report an Argos prototype with 64 antennas and capable of serving 15 clients simultaneously. We experimentally demonstrate that by scaling from 1 to 64 antennas the prototype can achieve up to 6.7 fold capacity gains while using a mere 1/64th of the transmission power.',\n",
       " '53e997ddb7602d9701fd1efa': 'Recommender systems are an important component of many websites. Two of the most popular approaches are based on matrix factorization (MF) and Markov chains (MC). MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences. On the other hand, MC methods model sequential behavior by learning a transition graph over items that is used to predict the next action based on the recent actions of a user. In this paper, we present a method bringing both approaches together. Our method is based on personalized transition graphs over underlying Markov chains. That means for each user an own transition matrix is learned - thus in total the method uses a transition cube. As the observations for estimating the transitions are usually very limited, our method factorizes the transition cube with a pairwise interaction model which is a special case of the Tucker Decomposition. We show that our factorized personalized MC (FPMC) model subsumes both a common Markov chain and the normal matrix factorization model. For learning the model parameters, we introduce an adaption of the Bayesian Personalized Ranking (BPR) framework for sequential basket data. Empirically, we show that our FPMC model outperforms both the common matrix factorization and the unpersonalized MC model both learned with and without factorization.',\n",
       " '53e997ddb7602d9701fd2015': 'A growing number of applications require support for processing data that is in the form of continuous stream rather than finite stored data. For instance, network and traffic management, medical monitoring are some of the new applications that continuously examine a sensor stream in order to detect any undesirable behavior of the monitored system that requires further inspection. In this paper we present a new algorithm to detect undesirable system behaviors that are represented by some complex temporal patterns over data streams. Our algorithm efficiently scans the data stream with a sliding window, and checks the data inside the window from right-to-left to see if they satisfy the pattern predicates. By first preprocessing the complex temporal patterns at compile time, it can exploit the interdependencies between the pattern predicates, and skip unnecessary checks with efficient window slides at run time. It resembles the sliding window process of the Boyer--Moore algorithm, although allowing complex predicates that are beyond the scope of this traditional string search algorithm. Implementation and evaluation of the proposed algorithm shows its efficiency when compared to previously proposed approaches.',\n",
       " '53e997ddb7602d9701fd1f05': 'This paper proposes an efficient system which integrates multiple vision models for robust multiperson detection and tracking for mobile service and social robots in public environments. The core technique is a novel maximum likelihood (ML)-based algorithm which combines the multimodel detections in mean-shift tracking. First, a likelihood probability which integrates detections and similarity to local appearance is defined. Then, an expectation-maximization (EM)-like mean-shift algorithm is derived under the ML framework. In each iteration, the E-step estimates the associations to the detections, and the M-step locates the new position according to the ML criterion. To be robust to the complex crowded scenarios for multiperson tracking, an improved sequential strategy to perform the mean-shift tracking is proposed. Under this strategy, human objects are tracked sequentially according to their priority order. To balance the efficiency and robustness for real-time performance, at each stage, the first two objects from the list of the priority order are tested, and the one with the higher score is selected. The proposed method has been successfully implemented on real-world service and social robots. The vision system integrates stereo-based and histograms-of-oriented-gradients-based human detections, occlusion reasoning, and sequential mean-shift tracking. Various examples to show the advantages and robustness of the proposed system for multiperson tracking from mobile robots are presented. Quantitative evaluations on the performance of multiperson tracking are also performed. Experimental results indicate that significant improvements have been achieved by using the proposed method.',\n",
       " '53e997dcb7602d9701fd1d90': 'The performance of SIMD processors is often limited by the time it takes to transfer data between the central- ized control unit and the parallel processor array. This is especially true of hybrid SIMD models, such as associa- tive computing, that make extensive use of global search operations. Pipelining instruction broadcast can help, but is not enough to solve the problem, especially for mas- sively parallel processors with thousands of processing elements. In this paper, we describe a SIMD processor architecture that combines a fully pipelined broad- cast/reduction network with hardware multithreading to reduce performance degradation as the number of proc- essors is scaled up.',\n",
       " '53e997dcb7602d9701fd1d91': \"The problem of missing values in software measurement data used in empirical analysis has led to the proposal of numerous potential solutions. Imputation procedures, for example, have been proposed to `fill-in' the missing values with plausible alternatives. We present a comprehensive study of imputation techniques using real-world software measurement datasets. Two different datasets with dramatically different properties were utilized in this study, with the injection of missing values according to three different missingness mechanisms (MCAR, MAR, and NI). We consider the occurrence of missing values in multiple attributes, and compare three procedures, Bayesian multiple imputation, k Nearest Neighbor imputation, and Mean imputation. We also examine the relationship between noise in the dataset and the performance of the imputation techniques, which has not been addressed previously. Our comprehensive experiments demonstrate conclusively that Bayesian multiple imputation is an extremely effective imputation technique.\",\n",
       " '53e997dcb7602d9701fd1d93': 'Multimedia and sensor security emerged as an important area of research largely due to growing accessibility of tools and applications through different media and Web services. There are many aspects of multimedia and sensor security ranging from trust in multimedia and sensor networks; security in ubiquitous databases, middleware, and multi-domain systems; privacy in ubiquitous environment and sensor networks; and radio frequency identification security. Particularly interesting are multimedia information security, forensics, image watermarking, and steganography. There are different reasons for using security in multimedia and sensors that depend on the method of access, the type of accessed information, the information placement and the availability of copying, sensitivity of protected information, and potential ways for corruption of data and their transmission. Protection of image and data includes watermarking, steganography, and forensics. Multi-domain systems, middleware, and databases are used for both local and distributed protections. Sensor networks often use radio frequency identification tags for identification. Both trusted hardware and software are needed to maintain multimedia and sensor security. Different applications require various levels of security often embedded in the multimedia image. In addition, a certain security level is needed for transmission of the application. Sensors use different security for data gathering depending on the type of data collected. Secure transmission is needed for sensor data. We present a comprehensive approach to multimedia and sensor security from application, system, network, and information processing perspective. Trusted software and hardware are introduced as support base for multimedia and sensor security.',\n",
       " '53e997ddb7602d9701fd202d': 'Let P be a rectilinear simple polygon. The stabbing number of a partition of P into rectangles is the maximum number of rectangles stabbed by any axis-parallel line segment inside P. We present a 3-approximation algorithm for the problem of finding a partition with minimum stabbing number. It is based on an algorithm that finds an optimal partition for histograms. We also study Steiner triangulations of a simple (non-rectilinear) polygon P. Here the stabbing number is defined as the maximum number of triangles that can be stabbed by any line segment inside P. We give an O(1)-approximation algorithm for the problem of computing a Steiner triangulation with minimum stabbing number.',\n",
       " '53e997ddb7602d9701fd2039': \"In this paper, we present an approach for tackling the problem of automatically detecting and tracking a varying number of people in complex scenes. We follow a robust and fast framework to handle unreliable detections from each camera by extensively making use of multi-camera systems to handle occlusions and ambiguities. Instead of using the detections of each frame directly for tracking, we associate and combine the detections to form so called tracklets. From the triangulation relationship between two views, the 3D trajectory is estimated and back-projected to provide valuable cues for particle filter tracking. Most importantly, a novel motion model considering different velocity cues is proposed for particle filter tracking. Experiments are done on the challenging dataset PETS'09 to show the benefits of our approach and the integrated multi-camera extensions.\",\n",
       " '53e997ddb7602d9701fd2041': 'A great deal of research has been made to model the vagueness and uncertainty in information retrieval. One such research is fuzzy ranking models, which have been showing their superior performance in handling the uncertainty involved in the retrieval process. However, these conventional fuzzy ranking models have a limited ability to incorporate the user preference when calculating the rank of documents. To address this issue, in this study we develop a new fuzzy ranking model based on the user preference. Through the experiments on the TREC-2 collection of Wall Street Journal documents, we show that the proposed method outperforms the conventional fuzzy ranking models.',\n",
       " '53e997ddb7602d9701fd1f56': 'The Array redistribution problem is the heart of a number of applications in parallel computing. This paper presents a message combining approach for scheduling runtime array redistribution of one-dimensional arrays. The important contribution of the proposed scheme is that it eliminates the need for local data reorganization, as noted by Sundar in 2001; the blocks destined for each processor are combined in a series of messages exchanged between neighbouring nodes, so that the receiving processors do not need to reorganize the incoming data blocks before storing them to memory locations. Local data reorganization is of great importance, especially in networks where there is no direct communication between all nodes (like tori, meshes, and trees). Thus, a block must travel through a number of relays before reaching the target processor. This requires a higher number of messages generated, therefore, a higher number of data permutations within the memory of each target processor should be made to assure correct data order. The strategy is based on a relation between groups of communicating processor pairs called superclasses.',\n",
       " '53e997ddb7602d9701fd1f64': 'From the theory of graph minors we know that the class of planar graphs is the only critical class with respect to tree-width. In the present paper, we reveal a critical class with respect to clique-width, a notion generalizing tree-width. This class is known in the literature under different names, such as unit interval, proper interval or indifference graphs, and has important applications in various fields, including molecular biology. We prove that the unit interval graphs constitute a minimal hereditary class of unbounded clique-width. As an application, we show that list coloring is fixed parameter tractable in the class of unit interval graphs.',\n",
       " '53e997ddb7602d9701fd1f6b': 'We present a new model of parallel computation called the “array processing machine” or APM (for short). The APM was designed to closely model the architecture of existing vector- and array processors, and to provide a suitable unifying framework for the complexity theory of parallel combinatorial and numerical algorithms. It is shown that every problem that is solvable in polynomial space on an ordinary, sequential random access machine can be solved in parallel polynomial time on an APM (and vice versa). The relationship to other models of parallel computation is discussed.',\n",
       " '53e997ddb7602d9701fd1f70': 'The Java programming language supports concurrency. Concurrent programs are hard to test due to their inherent non-determinism. This paper presents a classification of concurrency failures that is based on a model of Java concurrency. The model and failure classification is used to justify coverage of synchronization primitives of concurrent components. This is achieved by constructing concurrency flow graphs for each method call. A producer-consumer monitor is used to demonstrate how the approach can be used to measure coverage of concurrency primitives and thereby assist in determining test sequencesfor deterministic execution.',\n",
       " '53e997ddb7602d9701fd21cb': 'Abstract: This paper describes the recent changes to the RTI-Kit and changes being studied to develop new paradigms for real-time distributed simulation execution. We discuss design options for hard real-time (HRT) extensions to the High-Level Architecture and current efforts to adapt the high-performance RTI-Kit modules to a HRT RTI. We give some preliminary results supporting the efficacy of a hard real-time RTI implementation and discuss the impact of its availability.\\n\\n',\n",
       " '53e997ddb7602d9701fd2070': \"In this paper we consider a motion planning problem that occurs in tasks such as spot welding, car painting, inspection, and measurement, where the end-effector of a robotic arm must reach successive goal placements given as inputs. The problem is to compute a nearoptimal path of the arm so that the end-effector visits each goal once. It combines two notoriously hard subproblems: the collisionfree shortest-path and the traveling-salesman problems. It is further complicated by the fact that each goal placement of the end-effector may be achieved by several configurations of the arm (distinct solutions of the arm's inverse kinematics). This leads to considering a set of goal configurations of the robot that are partitioned into groups. The planner must compute a robot path that visits one configuration in each group and is near optimal over all configurations in every goal group and over all group orderings. The algorithm described in this paper operates under the assumption that finding a good tour in a graph with edges of given costs takes much less time than computing good paths between all pairs of goal configurations from different groups. So, the algorithm balances the time spent in computing paths between goal configurations and the time spent in computing tours. Although the algorithm still computes a quadratic number of such paths in the worst case, experimental results show that it is much faster in practice.\",\n",
       " '53e997ddb7602d9701fd21dd': 'In this paper, a de-interlacing algorithm to find the optimal deinterlaced results given accuracy-limited motion information is proposed. The de-interlacing process is formulated as a Maximum A Posterior (MAP) - Markov Random Field (MRF) problem. The MAP solution is the one that minimizes an energy function. The energy function imposes discontinuity adaptive smoothness constraint upon the deinterlaced frame. Simulation results show that the MAP-MRF formulation is efficient and the high frequency noise is removed in a few iterations. Index Terms— TV receiver signal processing, hidden Markov models',\n",
       " '53e997ddb7602d9701fd21e0': 'The authors formally define a distributed-memory parallel architecture called the pipelined hypercube. A coarse-grained parallel sorting algorithm that can be mapped efficiently on such an architecture is also presented. The pipelined hypercube has a more powerful communication mechanism than the traditional binary code architecture, in that it permits communication of blocks of data between processing elements (PEs) to be performed in a pipelined manner. Certain data communication problems which would probably be serialized on the binary code architecture, can be performed optimally on the pipelined hypercube. The sorting algorithm can be mapped efficiently onto a pipelined hypercube of P PEs. It sorts N data items, initially distributed among the PEs, in time O((N log N/P)+log/sup 2/ P), thereby achieving linear speedup when P is O(N/log N).',\n",
       " '53e997ddb7602d9701fd232a': \"In steganography, several different types of media have been used as carriers, such as images, audios and video streams, to hide secret data. Nevertheless, various novel media and applications have been developed due to the rapid growth of internet. In this paper, we select maze games as carrier media to conceal secret data. The original idea of embedding data in a maze is proposed by Niwayama et al. Their method has two disadvantages. One is the very small embedding capacity; the other is that the stego maze is not perfect. Here, we propose an improved algorithm for increasing the embedding capacity and preserving the ''perfect'' property.\",\n",
       " '53e997ddb7602d9701fd2394': 'Given an arbitrary partial anonymous grid (a finite grid with possibly missing vertices or edges), this paper focuses on the exploration of such a grid by a set of mobile anonymous agents (called robots). Assuming that the robots can move synchronously, but cannot communicate with each other, the aim is to design an algorithm executed by each robot that allows, as many robots as possible (let k be this maximal number), to visit infinitely often all the vertices of the grid, in such a way that no vertex hosts more than one robot at a time, and each edge is traversed by at most one robot at a time. The paper addresses this problem by considering a central parameter, denoted ρ , that captures the view of each robot. More precisely, it is assumed that each robot sees the part of the grid (and its current occupation by other robots, if any) centered at the vertex it currently occupies and delimited by the radius ρ . Based on such a radius notion, a previous work has investigated the cases ρ = 0 and ρ = + ***, and shown that, while there is no solution for ρ = 0, k ≤ p *** q is a necessary and sufficient requirement when ρ = + ***, where p is the number of vertices of the grid, and q a parameter whose value depends on the actual topology of the partial grid. This paper completes our previous results by addressing the more difficult case, namely ρ = 1. It shows that k ≤ p *** 1 when q = 0, and k ≤ p *** q otherwise, is a necessary and sufficient requirement for solving the problem. More generally, the paper shows that this case is the borderline from which the considered problem can be solved.',\n",
       " '53e997ddb7602d9701fd244e': 'The Radial Basis Function method (RBF) provides a generic mathematical tool for various interpolation and smoothing problems in computer graphics and vision, such as surface reconstruction from scattered data, smoothing of noisy data, filling gaps and restoring missing data. As the radial function uses distances in the data set, it does not depend on the dimension of the problem. Thus, RBF can be used for 2D data processing (images, height fields), 3D data processing (surfaces, volumes), 4D data processing (time-varying data) or even higher. In this paper, we present a novel RBF based approach for reconstruction of images and height fields from highly corrupted data, which can handle large images in feasible time, while being very simple to program. Our approach uses an image partitioning via Delaunay triangulation of the dataset. The advantage of our approach is that it can be combined with fast evaluation of RBF using R-expansion [Zandifar et al. 2004] of the basis function.',\n",
       " '53e997ddb7602d9701fd246a': 'The parallel watershed transformation used in grayscale image segmentation is here reconsidered on the basis of markers. The goal is to reduce the typical over segmen- tation by decreasing the number of catchment basins produced by flooding. Assimilating the set of basins with a weighted neigh- borhood graph and computing the minimum spanning forest in which every tree is rooted at a marked vertex, all non-marked regions in each tree are incorporated in the root region of the tree. A lognN distributed message passing algorithm per- forming the above stated goal on N processors is here presented. Two merits of the parallel algorithm are worth of mentioning: first, the local detection of the catchment basins conforming to the watershed princi- ple (which strongly depends on the history of the region growth), with an extremely low communication traffic; and second, the parallel computation of the BorBvka like minimum spanning forest with the constraint that any tree contains exactly one marker. Evaluation of a Cray T3D implementation under Message Passing Interface (MPI) is herein included.',\n",
       " '53e997ddb7602d9701fd2471': 'Audio watermarking is a technique, which can be used to embed information into the digital representation of audio signals. The main challenge is to hide data representing some information without compromising the quality of the watermarked track and at the same time ensure that the embedded watermark is robust against removal attacks. Especially providing perfect audio quality combined with high robustness against a wide variety of attacks is not adequately addressed and evaluated in current watermarking systems. In this paper, we present a new phase modulation audio watermarking technique, which among other features provides evidence for high audio quality. The system combines the alteration of the phase with the spread spectrum concept and is referred to as Adaptive Spread Phase Modulation (ASPM). Extensive benchmarking provide the evidence for the inaudibility of the embedded watermark and the good robustness.',\n",
       " '53e997ddb7602d9701fd24b5': 'We compare space-time coding (transmit diversity) and random &#34;opportunistic&#34; beamforming in a space-division multiple access/time-division multiple access single-cell downlink system with random packet arrivals, correlated block-fading channels, and non-perfect channel state information at the transmitter due to a feedback delay. Our comparison is based on system stability. The ability of accurate...',\n",
       " '53e997ddb7602d9701fd224f': 'Many different models of genetic regulatory networks (GRN) exist, but most of them are focused on off-line processing, so that important features of real networks, like adaptive and non-stationary character are missed. Interdisciplinary insight into the area of self-organization within the living organisms has caused some interesting new thoughts, and the suggested model is among them. Based on reinforcement learning of the Boolean network with random initial structure, the model is searching for a specialized network, that agrees with experimentally obtained data from the real GRN. With some experiments of real biological networks we investigate its behaviour.',\n",
       " '53e997ddb7602d9701fd2263': 'There are three important stages of path-based algorithms (PBAs) for solving the static user equilibrium traffic assignment problem (STA): finding shortest paths between various origins and destinations based on the present flow conditions to update the path set, updating path flows based on the move direction of the PBA, and updating the link flows and costs. This article proposes strategies to improve the computational efficiency of these three stages. The first strategy provides a simple method to preclude the through-routing via the zone centroid and helps to avoid unrealistic flow without affecting the flow update process of a PBA. The second strategy seeks to improve the efficiency of the path flow update process by circumventing unnecessary computation. The third strategy proposes faster link flow and link cost update processes along with a link data structure to support it. The computational experiments using two recently developed PBAs validate the effectiveness of these strategies and help to understand their rationale. The strategies are significant from both theoretical and practical perspectives. From a theoretical viewpoint, they help in designing an efficient execution process for PBAs and provide an improved common platform for comparing their performances. For practice, they can reduce the computational cost in finding the solution of the STA without increasing the complexity of the execution of the algorithm.',\n",
       " '53e997ddb7602d9701fd251e': 'We define a general model capturing the behavior of a population of anonymous agents that interact in pairs. This model captures some of the main features of opportunistic networks, in which nodes (such as the ones of a mobile ad hoc networks) meet sporadically. For its reminiscence to Population Protocol, we call our model Large-Population Protocol, or LPP. We are interested in the design of LPPs enforcing, for every ν∈[0,1], a proportion ν of the agents to be in a specific subset of marked states, when the size of the population grows to infinity; In which case, we say that the protocol computesν. We prove that, for every ν∈[0,1], ν is computable by a LPP if and only if ν is algebraic. Our positive result is constructive. That is, we show how to construct, for every algebraic number ν∈[0,1], a protocol which computes ν.',\n",
       " '53e997ddb7602d9701fd242f': 'Stochastic models of images are commonly represented in terms of three random processes (random fields) defined on the region of support of the image. The observed image process G is considered as a composite of two random process: a high level process Gh , which represents the regions (or classes) that form the observed image; and a low level process Gl , which describes the statistical characteristics of each region (or class). The representation G = (Gh , Gl ) has been widely used in the image processing literature in the past two decades. In this paper, we consider the low level process Gl as mixture of normal distributions, and we use the Expectation- Maximization (EM) algorithm to estimate the mean, the variance, and proportion for each distribution. A popular model for the high level process Gh has been the Gibbs-Markov random field (GMRF) model. We introduce a novel unsupervised approach to estimate the parameters of a GMRF model. In this approach, we estimate the model parameters that maximize the posteriori probability of each pixel in a given image. The MAP estimate is obtained using a combination of genetic search and deterministic optimization using the iterated conditional mode (ICM) approach of Besag. The desired estimate of the GMRF parameters is the one corresponding to the MAP estimate. The approach has been applied on real images (Spiral CT slices) and provides satisfactory results.',\n",
       " '53e997ddb7602d9701fd26e7': 'Considers the retrieval of variable bit-rate (VBR) video data from a storage server subject to the constraint that the server\\'s buffer between its disk and network subsystem neither empties nor overflows. Our approach is based on the realization that there are two separate decisions to be made: firstly, to determine which data to access by what deadline, and secondly, how to schedule the data access. These decisions are the responsibilities of the data access and data scheduling policies. In the context of data access policies, we examine both the constant time length (CTL) and constant data length (CDL) policies. We find that increasing either the round length (for CTL access) or the fixed data size (for CDL access) allows data retrieval to be \"smoothed out\" over time. We then examine round-based and deadline-based data scheduling in conjunction with CTL and CDL data access policies. We find that two deadline-based scheduling policies that we propose (\"lazy EDF\" and \"workahead CTL-round\") allow a significant additional amount of smoothing to be achieved over traditional round-based data scheduling.',\n",
       " '53e997ddb7602d9701fd271f': 'This paper discusses advantages and shortcomings of the S environment for multivariable geostatistics, in particular when extended with the gstat package, an extension package for the S environments (R, S-Plus). The gstat S package provides multivariable geostatistical modelling, prediction and simulation, as well as several visualisation functions. In particular, it makes the calculation, simultaneous fitting, and visualisation of a large number of direct and cross (residual) variograms very easy. Gstat was started 10 years ago and was released under the GPL in 1996; gstat.org was started in 1998. Gstat was not initially written for teaching purposes, but for research purposes, emphasising flexibility, scalability and portability. It can deal with a large number of practical issues in geostatistics, including change of support (block kriging), simple/ordinary/universal (co)kriging, fast local neighbourhood selection, flexible trend modelling, variables with different sampling configurations, and efficient simulation of large spatially correlated random fields, indicator kriging and simulation, and (directional) variogram and cross variogram modelling. The formula/models interface of the S language is used to define multivariable geostatistical models. This paper introduces the gstat S package, and discusses a number of design and implementation issues. It also draws attention to a number of papers on integration of spatial statistics software, GIS and the S environment that were presented on the spatial statistics workshop and sessions during the conference Distributed Statistical Computing 2003.',\n",
       " '53e997ddb7602d9701fd2720': \"The limits of a recently proposed Computer method for finding all distinct substructures of a chemical structure are systematically explored within comprehensive graph samples which serve as supersets of the graphs corresponding to saturated hydrocarbons. both acyclic (up to n=20) and (poly)cyclic (up to n=10). Several pairs of smallest graphs and compounds are identified that cannot be distinguished using selected combinations of invariants such as combinations of Balaban's index J and graph matrix eigenvalues. As the most important result. it can now be stated that the computer program NIMSG, using J and distance eigenvalues. is safe within the domain of mono- through tetracyclic saturated hydrocarbon substructures up to n=10 (oligiocyclic decanes) and of all acyclic alkane substructures up to n=19 (nonadecanes), i.e., it will not miss any of these Substructures. For the regions surrounding this safe domain, upper limits are found for the numbers of substructures that may be lost in the worst case. and these are low. This taken together means that the computer program can be reasonably employed in chemistry whenever one is interested in finding the saturated hydrocarbon substructures. As to unsaturated and heteroatom containing substructures. there are reasons to conjecture that the method's resolving power for them is similar.\",\n",
       " '53e997ddb7602d9701fd2941': \"The daily experiences of professionals who work chiefly at desks and tables have become the basis for modeling human-computer interaction. The scientific visualization process particularly benefits from special human-computer interfaces that support such things as walk-through experiences and virtual object manipulation. In a departure from systems that focus on immersing the user in a virtual environment, the Responsive Workbench has a user-centered, task-driven point of view and--with its computer-connected sensors and reaction devices--is a responsive environment. While specifically examining the working environments of physicians, automotive engineers, and architects, the authors explain the workbench design, hardware and software tool implementations, and some application experiences. Applications determined to be particularly amenable to Responsive Workbench development included medical education and body process representation; dynamic simulations for automotive design, specifically the virtual windtunnel and a mixing process; and landscape and environmental planning. The authors describe results and how they adapted the workbench to each discipline. In all applications, the most important and natural manipulation tool for virtual environments is the user's hand. The Responsive Workbench environment depends on the actual hand, not a computer-generated representation. The user wears a glove device with a Polhemus sensor mounted on the back. The authors list common problems that the workbench shares with other virtual environments and identify sample solutions. They also define areas for future Responsive Workbench development.\",\n",
       " '53e997ddb7602d9701fd29a6': 'This paper describes a new methodology for automatic location of the optic disc in retinal images, based on the combination of information taken from the blood vessel network with intensity data. The distribution of vessel orientations around an image point is quantified using the new concept of entropy of vascular directions. The robustness of the method for OD localization is improved by constraining the search for maximal values of entropy to image areas with high intensities. The method was able to obtain a valid location for the optic disc in 1357 out of the 1361 images of the four datasets.',\n",
       " '53e997ddb7602d9701fd2a0e': \"This study (a nationally representative telephone survey; n = 406) examines how attention to accident and crime stories among adolescents predicts judgments regarding alcohol-related risks, and how effects of 2 relevant individual difference variables-sensation seeking and negative 1st- or 2nd-hand personal experiences with alcohol risks-are mediated by attention. Results indicate, after controlling for a variety of demographic and behavioral variables, that attention to accident and crime news predicts adolescent risk judgments and mediates the influence of sensation seeking and negative experience on such risk judgments. Moreover, exposure to national TV news moderates these indirect effects, and sensation seeking and negative experiences also in turn moderate each others' indirect effects. Thus, the best characterization of the complex relation between individual differences, media, and risk judgments is one of moderated mediation.\",\n",
       " '53e997ddb7602d9701fd29c5': 'Online mechanism design considers the problem of sequential decision making in a multi-agent system with self-interested agents. The agent population is dynamic and each agent has private information about its value for a sequence of decisions. We introduce a method (\"ironing\") to transform an algorithm for online stochastic optimization into one that is incentive-compatible. Ironing achieves this by canceling decisions that violate a form of monotonicity. The approach is applied to the CONSENSUS algorithm and experimental results in a resource allocation domain show that not many decisions need to be canceled and that the overhead of ironing is manageable.',\n",
       " '53e997ddb7602d9701fd29dd': 'The topology of the Internet has been extensively studied in recent years, driving a need for increasingly complex measurement infrastructures. These measurements have produced detailed topologies with steadily increasing temporal resolution, but concerns exist about the ability of active measurements to measure the true Internet topology. Difficulties in ensuring the accuracy of every individual measurement when millions of measurements are made daily, and concerns about the bias that might result from measurements along the tree of routes from each vantage point to the wider reaches of the Internet must be addressed. However, early discussions of these concerns were based mostly on synthetic data, oversimplified models or data with limited or biased observer distributions. In this paper, we show the importance that extensive sampling from a broad and well spread set of vantage points has on the resulting topology and bias. The majority of this paper is devoted to a first look at the importance of the distribution quality. We show that diversity in the locations and types of vantage points is required for obtaining an unbiased topology. We analyze the effect that broad distribution has over the convergence of various autonomous systems topology characteristics, and show that although diverse and broad distribution is not required for all inspected properties, it is required for some. Finally, claims against bias in active traceroute sampling are revisited, and we empirically show that diverse and broad distribution can question their conclusions.',\n",
       " '53e997ddb7602d9701fd2a25': 'We present a study of tail-bias-current 1/f noise upconversion into 1/f3 phase noise for both CMOS Colpitts and differential-pair LC oscillators. We focus on the incremental Groszkowski effect, i.e., the modulation of the shift in oscillation frequency induced by the higher current harmonics flowing into the LC tank of a harmonic oscillator induced by bias instabilities, as we show that there is n...',\n",
       " '53e997ddb7602d9701fd2a7b': 'We present a method to render and control stylized highlights using projective textures for real-time applications. For each vertex of a given 3D model, we compute principal directions and their corresponding principal curvatures in the preprocessing step. We find the maximum specular intensity point on the surface of the model and compute the principal directions and curvatures at the point by bilinear interpolation. The position, orientation, and projection frustum of a texture projector are determined by the principal directions and curvatures at the point. Using a simple culling method, we reduce the number of triangles to compute the principal directions and curvatures. We can control the shape, size, position, and orientation of a stylized highlight in an easy, fast, and intuitive manner. We discuss the limitations of our method and also give approximate solutions for some limitations.',\n",
       " '53e997ddb7602d9701fd2a5d': 'An algorithm is presented that finds a min-cut linear arrangement of a tree in O(n log n) time. An extension of the algorithm determines the number of pebbles needed to play the black and white pebble game on a tree.',\n",
       " '53e997ddb7602d9701fd2a99': 'This paper describes and analyzes the results of our experiments in Geographical Information Retrieval (GIR) in the context of our participation in the CLEF 2007 GeoCLEF Monolingual English task.Our system uses Linguistic and Geographical Analysis to process topics and document collections. Geographical Document Retrieval is performed with Terrier and Geographical Knowledge Bases.Our experiments show that Geographical Knowledge Bases can be used to improve the retrieval results of the Terrier state-of-the-art IR system by filtering out non geographically relevant documents.',\n",
       " '53e997ddb7602d9701fd2adf': 'This paper presents results of new experiments with the Global Optimising Resource Broker and Allocator GORBA for grid systems. The scheduling algorithm is based on the Evolutionary Algorithm GLEAM (General Learning Evolutionary Algorithm and Method) and several heuristics. The task of planning grid resource allocation is compared to pure NP-complete job shop scheduling and it is shown in which way it is of greater complexity. Two different gene models and two repair methods are described in detail and assessed by the experimental results. Based on the analysis of the experimental results, directions of further work and improvements will be outlined.',\n",
       " '53e997ddb7602d9701fd2aa7': 'Infohabitants of the connected information systems include individuals, organizations, smart appliances, smart buildings, and other smart systems, as well as virtual entities acting on their behalf. They can best be represented by software agents. Hence, realistic cognitive abilities of software agents such as influence of personality to decision making and problem solving is of practical computational importance. In this article, two characteristics are added to software agents with personality: dynamic personality and the relationships of personality trait openness with both problem solving ability and cognitive complexity. The last characteristic of openness leads to its impact to dynamic modification of problem solving ability. In this article, an implementation of a fuzzy agent with personality is realized in Java environment to show personality descriptors, personality factors, personality style, and problem solving success consequently. Furthermore, a prototype system is presented to update personality facets and respective personality trait openness which can affect problem solving ability.',\n",
       " '53e997ddb7602d9701fd2ab3': 'In this paper, we present a novel, frequency-domain stereo to mono downmixing, which preserves the energy of spectral components and avoids setting the left or right channel as a phase reference. Based on this downmixing technique, a parametric stereo analysis-synthesis model is described in which subband stereo parameters consist of interchannel level differences and phase differences between the mono signal and one of the stereo channels (left or right). This model is applied to the stereo extension of ITU-T G.722 at 56+8 and 64+16 kbit/s with a frame length of 5 ms. AB test results are provided to assess the quality of the proposed downmixing technique. In addition, the quality of the proposed G.722-based stereo coder is compared against reference coders (G.722.1 at 24 and 32 kbit/s dual mono and G.722 at 64 kbit/s dual mono) for clean speech, noisy speech and music.',\n",
       " '53e997ddb7602d9701fd2ac2': 'The amount of electronic car functions increased rapidly in the last decade. Audi uses Hardware-in- the-Loop systems in combination with the model based test automation system EXAM for safeguarding of the electronic functions. The complexity of the test cases is comparable to real software programs. Therefore, a quality assurance for the test cases is necessary. A verification approach based on an action logic and Petri nets is presented to verify the causal ordering of the test steps.',\n",
       " '53e997ddb7602d9701fd2b99': 'This paper presents the two new ITU-T Recommendations G.722 Annex D and G.711.1 Annex F, which are stereo extensions of the wideband codecs ITU-T G.722 and G.711.1 and their superwideband extensions (G.722 Annex B and G.711.1 Annex D). An embedded scalable structure is used to add stereo extension layers on top of the wideband or superwideband core coding. Wideband stereo modes are supported at the bit rates of 64/80 and 96/128 kbit/s for G.722 and G.711.1 (respectively), while superwideband stereo modes are supported at 80/96/112/128 and 112/128/144/160 kbit/s. The parametric stereo coding model is based on a frequency domain downmix, wideband inter-channel differences estimation, quantization and synthesis, low complexity coherence analysis and synthesis, stereo transient detection and stereo post-processing. An overview of formal ITU-T characterization listening tests illustrates the performance of these codecs. © 2013 IEEE.',\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_dict = df['abstract'].to_dict()\n",
    "abstract_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ea87fa8-662b-440c-a7cd-52490719f202",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m keywords_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(data_home \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph/keywords.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m abstract_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(data_home \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph/abstract.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m fos_list \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgraph/fos.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/numpy/lib/npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/numpy/lib/format.py:795\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 795\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be loaded when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "title_list = np.load(data_home + \"graph/title.npy\")\n",
    "keywords_list = np.load(data_home + \"graph/keywords.npy\")\n",
    "abstract_list = np.load(data_home + \"graph/abstract.npy\")\n",
    "fos_list = np.load(data_home + \"graph/fos.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806ac75-def4-49c8-8ce1-aa79d4cdc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model = fasttext.load_model('./data/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a202bad-417a-4456-be04-e0ecb92363c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(model, keywords_list):\n",
    "    embeddings = [ np.array(list(map(model.get_word_vector, keywords))) for keywords in keywords_list ]\n",
    "    return embeddings\n",
    "    \n",
    "def word_list_embedding(model, keywords_list):\n",
    "    embeddings = word_embedding(model, keywords_list)\n",
    "    # print(len(embeddings))\n",
    "    # return np.array([ np.mean(keywords_embedding, axis=0) for keywords_embedding in embeddings])\n",
    "    return [ np.mean(keywords_embedding, axis=0) for keywords_embedding in embeddings]\n",
    "    \n",
    "def sentence_embedding(model, sentence_list):\n",
    "    return np.array([ model.get_sentence_vector(' '.join(str(sentence).split())) for sentence in sentence_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd037f0-3deb-4d8c-be47-a103951a07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_embedding_list = word_list_embedding(fast_model, keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e611f8-0b4f-44eb-80c8-ae58198f54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embedding_list = sentence_embedding(fast_model, title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab5c2aab-58eb-4d53-816c-4b276625d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_from_abstract = []\n",
    "threshold = 0.3\n",
    "keywords_limit = 2\n",
    "# bert_model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "bert_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "214b4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Load DistilBERT model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put the model on GPU\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "\n",
    "def extract_keywords(text, top_n=5):\n",
    "    \"\"\"\n",
    "    Extract top_n keywords from a text using DistilBERT embeddings.\n",
    "    \"\"\"\n",
    "    # Tokenize input and get outputs from DistilBERT\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    tokenized_words = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {key: val.to('cuda') for key, val in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the embeddings of the tokens\n",
    "    embeddings = outputs.last_hidden_state[0]\n",
    "    \n",
    "    # Compute norms of the embeddings\n",
    "    norms = torch.norm(embeddings, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Filter out \"unused\" tokens and rank remaining tokens\n",
    "    filtered_tokens = [(word, norm) for word, norm in zip(tokenized_words, norms) if \"unused\" not in word]\n",
    "    sorted_tokens = sorted(filtered_tokens, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the top_n keywords\n",
    "    keywords = [token[0] for token in sorted_tokens[:top_n]]\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d794f96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The eXtensible Markup Language 驴 XML 驴 is not only a language for communication between humans and the web, it is also a language for communication between programs. Rather than passing parameters, programs can pass documents from one to another, containing not only pure data, but control information as well. Even legacy programs written in ancient languages such as COBOL and PL/I can be adapted by means ofinterface reengineering to process and to generate XML documents.',\n",
       "       'Resource allocation for multi-tier web applications in virtualization environments is one of the most important problems in autonomous computing. On one hand, the more resources that are provisioned to a multitier web application, the easier it is to meet service level objectives (SLO). On the other hand, the virtual machine which hosts the multi-tier web application needs to be consolidated as much as possible in order to maintain high resource utilization. This paper presents an adaptive resource controller which consists of a feedback utilization controller and an auto-regressive and moving average model (ARMA)-based model estimator. It can meet application-level quality of service (QoS) goals while achieving high resource utilization. To evaluate the proposed controllers, simulations are performed on a testbed simulating a virtual data center using Xen virtual machines. Experimental results indicate that the controllers can improve CPU utilization and make the best tradeoff between resource utilization and performance for multi-tier web applications.',\n",
       "       'As process variations become a significant problem in deep sub-micron technology, a shift from deterministic static timing analysis to statistical static timing analysis for high-performance circuit designs could reduce the excessive conservatism that is built into current timing design methods. We address the timing yield problem for sequential circuits and propose a statistical approach to handle it. We consider the spatial and path reconvergence correlations between path delays, set-up time and hold time constraints, and clock skew due to process variations. We propose a method to get the timing yield based on the delay distributions of register-to-register paths in the circuit On average, the timing yield results obtained by our approach have average errors of less than 1.0% in comparison with Monte Carlo simulation. Experimental results show that shortest path variations and clock skew due to process variations have considerable impact on circuit timing, which could bias the timing yield results. In addition, the correlation between longest and shortest path delays is not significant.',\n",
       "       ...,\n",
       "       'In many applications, such as Eddy-Current Testing (ECT), we are often interested in the joint model choice and parameter estimation. Nested Sampling (NS) is one of the possible methods. The key step that reflects the efficiency of the NS algorithm is how to get samples with hard constraint on the likelihood value. This contribution is based on the classical idea where the new sample is drawn within a hyper-ellipsoid, the latter being located from Gaussian approximation. This sampling strategy can automatically guarantee the hard constraint on the likelihood. Meanwhile, it shows the best sampling efficiency for models which have Gaussian-like likelihood distributions. We apply this method in ECT. The simulation results show that this method has high model choice ability and good parameter estimation accuracy, and low computational cost meanwhile.',\n",
       "       \"Changes in software development come in many forms. Some changes are frequent, idiomatic, or repetitive (e.g. adding checks for nulls or logging important values) while others are unique. We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways; they may require more expertise or represent code that is more complex or prone to mistakes. As such, these unique changes are worthy of study. In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two of those applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes.\\n\\n\",\n",
       "       'Scientists and engineers from different backgrounds share the need for measurement of electrical impedance and related quantities. Equivalent lumped resistance, capacitance, or inductance are common parameters in the characterization, specification, and design of electrical and electronic components. Electric and magnetic material properties such as resistivity, permittivity, and permeability are derived from impedance and from geometrical measurements. Many sensors of physical quantities have impedance as their output quantity. The basis of electrochemical impedance spectroscopy (EIS) and electrical impedance tomography (EIT) is impedance measurement. As in all measurements, the basis of accurate impedance measurements is: ·a proper definition of the measurand; and · a traceability to the impedance units of the International System (SI), achieved through the use of calibrated impedance meters and impedance standards.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstract'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4b238ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('xml language', 0.749),\n",
       "  ('extensible markup', 0.4908),\n",
       "  ('ofinterface reengineering', 0.3282),\n",
       "  ('communication humans', 0.2729),\n",
       "  ('data control', 0.2657),\n",
       "  ('legacy programs', 0.2539),\n",
       "  ('passing parameters', 0.1818),\n",
       "  ('process generate', 0.1781),\n",
       "  ('written ancient', 0.1736),\n",
       "  ('cobol pl', 0.1389)],\n",
       " [('applications virtualization', 0.5221),\n",
       "  ('adaptive resource', 0.4638),\n",
       "  ('service qos', 0.3697),\n",
       "  ('data center', 0.2667),\n",
       "  ('improve cpu', 0.2563),\n",
       "  ('tier', 0.2285),\n",
       "  ('xen', 0.2178),\n",
       "  ('controller consists', 0.1567),\n",
       "  ('slo hand', 0.1181),\n",
       "  ('average', 0.1079)],\n",
       " [('circuit timing', 0.6101),\n",
       "  ('carlo simulation', 0.2605),\n",
       "  ('register paths', 0.237),\n",
       "  ('sub micron', 0.2264),\n",
       "  ('shift deterministic', 0.2097),\n",
       "  ('reconvergence correlations', 0.1641),\n",
       "  ('built current', 0.1575),\n",
       "  ('yield results', 0.1195),\n",
       "  ('design methods', 0.0978),\n",
       "  ('average errors', 0.0721)]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = bert_model.extract_keywords(df['abstract'].to_numpy()[:3], top_n=10, keyphrase_ngram_range=(1, 2), use_mmr=True, diversity=0.6, use_maxsum=False)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5981f658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53e99784b7602d9701f3f411 The eXtensible Markup Language 驴 XML 驴 is not only a language for communication between humans and the web, it is also a language for communication between programs. Rather than passing parameters, programs can pass documents from one to another, containing not only pure data, but control information as well. Even legacy programs written in ancient languages such as COBOL and PL/I can be adapted by means ofinterface reengineering to process and to generate XML documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('xml language', 0.749), ('extensible markup', 0.4908), ('ofinterface reengineering', 0.3282), ('communication humans', 0.2729), ('data control', 0.2657), ('legacy programs', 0.2539), ('passing parameters', 0.1818), ('process generate', 0.1781), ('written ancient', 0.1736), ('cobol pl', 0.1389)]\n",
      "0 is done => shape: 3\n",
      "53e99784b7602d9701f3f5fe Resource allocation for multi-tier web applications in virtualization environments is one of the most important problems in autonomous computing. On one hand, the more resources that are provisioned to a multitier web application, the easier it is to meet service level objectives (SLO). On the other hand, the virtual machine which hosts the multi-tier web application needs to be consolidated as much as possible in order to maintain high resource utilization. This paper presents an adaptive resource controller which consists of a feedback utilization controller and an auto-regressive and moving average model (ARMA)-based model estimator. It can meet application-level quality of service (QoS) goals while achieving high resource utilization. To evaluate the proposed controllers, simulations are performed on a testbed simulating a virtual data center using Xen virtual machines. Experimental results indicate that the controllers can improve CPU utilization and make the best tradeoff between resource utilization and performance for multi-tier web applications.\n",
      "[('applications virtualization', 0.5221), ('adaptive resource', 0.4638), ('service qos', 0.3697), ('data center', 0.2667), ('improve cpu', 0.2563), ('tier', 0.2285), ('xen', 0.2178), ('controller consists', 0.1567), ('slo hand', 0.1181), ('average', 0.1079)]\n",
      "53e99784b7602d9701f3e15d As process variations become a significant problem in deep sub-micron technology, a shift from deterministic static timing analysis to statistical static timing analysis for high-performance circuit designs could reduce the excessive conservatism that is built into current timing design methods. We address the timing yield problem for sequential circuits and propose a statistical approach to handle it. We consider the spatial and path reconvergence correlations between path delays, set-up time and hold time constraints, and clock skew due to process variations. We propose a method to get the timing yield based on the delay distributions of register-to-register paths in the circuit On average, the timing yield results obtained by our approach have average errors of less than 1.0% in comparison with Monte Carlo simulation. Experimental results show that shortest path variations and clock skew due to process variations have considerable impact on circuit timing, which could bias the timing yield results. In addition, the correlation between longest and shortest path delays is not significant.\n",
      "[('circuit timing', 0.6101), ('carlo simulation', 0.2605), ('register paths', 0.237), ('sub micron', 0.2264), ('shift deterministic', 0.2097), ('reconvergence correlations', 0.1641), ('built current', 0.1575), ('yield results', 0.1195), ('design methods', 0.0978), ('average errors', 0.0721)]\n",
      "53e99784b7602d9701f3f95d Mobile online analytical processing (mOLAP) encompasses all necessary technologies for information systems that enable OLAP data access to users carrying a mobile device. This paper presents FCLOS, a complete client–server architecture explicitly designed for mOLAP. FCLOS founds on intelligent scheduling and compressed transmissions in order to become a query efficient, self-adaptive and scalable mOLAP information system. Scheduling exploits derivability between data cubes in order to group related queries and eventually reduce the necessary transmissions (broadcasts). Compression is achieved by the m-Dwarf, a novel, compressed data cube physical structure, which has no loss of semantic information and is explicitly designed for mobile applications. The superiority of FCLOS against state of the art systems is shown both experimentally and analytically.\n",
      "[('compressed data', 0.4925), ('intelligent scheduling', 0.3997), ('mobile applications', 0.3696), ('server architecture', 0.3199), ('enable olap', 0.3114), ('molap fclos', 0.2952), ('cube', 0.26), ('related queries', 0.2124), ('online analytical', 0.2057), ('transmissions order', 0.1098)]\n",
      "53e99785b7602d9701f442cb The emergence of India as a global player in software development, IT, and call centre operations is one side of an information revolution that has also begun to impact on governance and development at a domestic level in areas such as e-governance, e-commerce and e-health. The state, private and civil sectors have invested in numerous initiatives throughout the length and breadth of India aimed at extending the benefits of the information revolution to rural and remote areas. These range from Reliance Infocom’s roll out of low-cost mobile cellular phones, to numerous civil society based initiatives aimed at establishing affordable access to information and knowledge. The state continues to invest in ICTs for development – from its support for Village Public Telephones (VPTs) to its enabling the computerisation of land records such as the Bhoomi project in Karnataka. The state’s recognition of the role played by private and civil society sectors in development marks a major and distinct change in attitude from one characterised by ‘tolerance’ at best for these sectors and belief in the self-sufficiency of a ‘dirigiste’ economy, to pragmatic accomodations with these sectors. This change is to some extent a reflection of post-SAP policies adopted by the state, best illustrated by its steady withdrawal of support from its welfare agenda.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m idx, (\u001b[39mid\u001b[39m, abstract) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(abstract_dict\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mid\u001b[39m, abstract)\n\u001b[0;32m----> 3\u001b[0m     keywords \u001b[39m=\u001b[39m bert_model\u001b[39m.\u001b[39;49mextract_keywords(abstract, top_n\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, keyphrase_ngram_range\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m), use_mmr\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, diversity\u001b[39m=\u001b[39;49m\u001b[39m0.6\u001b[39;49m, use_maxsum\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(keywords)\n\u001b[1;32m      5\u001b[0m     filtered_keywords \u001b[39m=\u001b[39m [ keyword[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m keyword \u001b[39min\u001b[39;00m keywords \u001b[39mif\u001b[39;00m keyword[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m threshold ]\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/keybert/_model.py:178\u001b[0m, in \u001b[0;36mKeyBERT.extract_keywords\u001b[0;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords, doc_embeddings, word_embeddings)\u001b[0m\n\u001b[1;32m    176\u001b[0m     doc_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39membed(docs)\n\u001b[1;32m    177\u001b[0m \u001b[39mif\u001b[39;00m word_embeddings \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     word_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49membed(words)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m seed_keywords \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     seed_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39membed([\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(seed_keywords)])\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/keybert/backend/_sentencetransformers.py:62\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[0;34m(self, documents, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed\u001b[39m(\u001b[39mself\u001b[39m, documents: List[\u001b[39mstr\u001b[39m], verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     51\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m    matrix of embeddings\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_model\u001b[39m.\u001b[39;49mencode(documents, show_progress_bar\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    433\u001b[0m         past_key_value,\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:286\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    277\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    278\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    285\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 286\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[1;32m    288\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/msc/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, abstract in enumerate(list(abstract_dict.values())):\n",
    "    # print(id, abstract)\n",
    "    keywords = bert_model.extract_keywords(abstract, top_n=10, keyphrase_ngram_range=(1, 2), use_mmr=True, diversity=0.6, use_maxsum=False)\n",
    "    print(keywords)\n",
    "    filtered_keywords = [ keyword[0] for keyword in keywords if keyword[1] > threshold ]\n",
    "    if len(filtered_keywords) == 0:\n",
    "        filtered_keywords = [ keyword[0] for keyword in keywords[:keywords_limit] ]\n",
    "    \n",
    "    keywords_from_abstract.append(filtered_keywords)\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"{idx} is done => shape: {len(filtered_keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e08fb-31c6-4385-8e85-7f1e2c94d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, abstract in enumerate(abstract_list):\n",
    "    keywords = bert_model.extract_keywords(abstract, top_n=20, use_mmr=True)\n",
    "    \n",
    "    filtered_keywords = [ keyword[0] for keyword in keywords if keyword[1] > threshold ]\n",
    "    if len(filtered_keywords) == 0:\n",
    "        filtered_keywords = [ keyword[0] for keyword in keywords[:keywords_limit] ]\n",
    "    \n",
    "    keywords_from_abstract.append(filtered_keywords)\n",
    "    if idx % 10_000 == 0:\n",
    "        print(f\"{idx} is done => shape: {len(filtered_keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f73687-a279-408c-9f95-db1dde9465e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_embedding_list = word_list_embedding(fast_model, keywords_from_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797d9e6-2d48-4e89-8afb-72fc1bf013da",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_eleminate_list = []\n",
    "original_shape = abstract_embedding_list[0].shape\n",
    "for idx, embedding in enumerate(abstract_embedding_list):\n",
    "    if original_shape != embedding.shape:\n",
    "        to_eleminate_list.append(idx)\n",
    "        print(idx, len(abstract_list[idx]), abstract_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2d116-76c3-4149-852e-0f1cf38681cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_embedding = []\n",
    "title_embedding = []\n",
    "abstract_embedding = []\n",
    "\n",
    "filtered_fos = []\n",
    "filtered_id = []\n",
    "start = 0\n",
    "\n",
    "for end in to_eleminate_list:\n",
    "    keywords_embedding.extend(keywords_embedding_list[start:end])\n",
    "    title_embedding.extend(title_embedding_list[start:end])\n",
    "    abstract_embedding.extend(abstract_embedding_list[start:end])\n",
    "    \n",
    "    filtered_fos.extend(list(df['fos'].iloc[:][start:end]))\n",
    "    filtered_id.extend(list(df['_id'].iloc[:][start:end]))\n",
    "    start = end + 1\n",
    "    \n",
    "keywords_embedding.extend(keywords_embedding_list[start:])\n",
    "title_embedding.extend(title_embedding_list[start:])\n",
    "abstract_embedding.extend(abstract_embedding_list[start:])\n",
    "\n",
    "filtered_fos.extend(list(df['fos'].iloc[:][start:]))\n",
    "filtered_id.extend(list(df['_id'].iloc[:][start:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188119b-bcf5-4b6a-8baa-b33b21672e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_embedding = np.array(keywords_embedding)\n",
    "title_embedding = np.array(title_embedding)\n",
    "abstract_embedding = np.array(abstract_embedding)\n",
    "\n",
    "filtered_id = np.array(filtered_id)\n",
    "filtered_fos = np.array(filtered_fos, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3b906-0b38-4e2e-8428-d023e62a7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/data/jhk1c21/graph_data/title_full.npy', title_embedding)\n",
    "np.save('/data/jhk1c21/graph_data/keywords_full.npy', keywords_embedding)\n",
    "np.save('/data/jhk1c21/graph_data/abstract_full.npy', abstract_embedding)\n",
    "np.save('/data/jhk1c21/graph_data/id_full.npy', filtered_id)\n",
    "np.save('/data/jhk1c21/graph_data/fos_full.npy', filtered_fos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c23d8-6a93-47a7-8d67-485a2f420fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/data/jhk1c21/graph_data/title_full.npy', np.array(title_embedding_list))\n",
    "np.save('/data/jhk1c21/graph_data/keywords_full.npy', np.array(keywords_embedding_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
